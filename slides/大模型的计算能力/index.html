<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>大模型的计算能力 | 拾柒读库</title>
<meta name=keywords content="LLM"><meta name=description content="大语言模型的计算能力 大模型领域的几个核心数学问题 N-GRAM 的计算能力问题 过参数化模型的统计学习问题 非凸的数值优化问题 对深度神经网络的数学理解 Transformer 算子的"><meta name=author content="癸老师"><link rel=canonical href=https://blog.uglyboy.cn/slides/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/><link crossorigin=anonymous href=/assets/css/stylesheet.0c4fd3725171366f335155acde6fb3c7b7042cd2fd075bebf5023d9b28a701b2.css integrity="sha256-DE/TclFxNm8zUVWs3m+zx7cELNL9B1vr9QI9myinAbI=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.uglyboy.cn/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.uglyboy.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.uglyboy.cn/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.uglyboy.cn/apple-touch-icon.png><link rel=mask-icon href=https://blog.uglyboy.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/reveal.min.css integrity="sha512-8849oiyeHKp8wdWclFreBguH6cXFoZfHhVBcoxdm+2+JcXagu1Poobm5vzBqEZqL9iJbTYK1bXbx3SWfNA3kRQ==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/highlight/zenburn.min.css integrity="sha512-JPxjD2t82edI35nXydY/erE9jVPpqxEJ++6nYEoZEpX2TRsmp2FpZuQqZa+wBCen5U16QZOkMadGXHCfp+tUdg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/theme/serif.min.css integrity="sha512-oS1nbkOWsc5Ko9pLGsFZGIt0FBWp1ZmTD7LplvylADPQuxHfztC8Fd24/5CZGNDoLORrtkESGshNdgTbrXPBoQ==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/reveal.min.js integrity="sha512-w7pY1wL5qhcH40XK9PIRYZc/qeog8TdyxRqEA9eHKBz+/tnhAOyIFHivbmJd/ezr8xbB7iVAh4tBNiJakCTw2g==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/markdown/markdown.min.js integrity="sha512-4exkEeyVuaWUFKozXl6L3UCugl6ai1cKnrVFkWUstdrNB2sDxxmPEaHBzTlYm9wX78EjPzEBG0s8k37oPeUFIw==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/highlight/highlight.min.js integrity="sha512-OE5v8UPoWQBlcTYkiOyXhC/+7WBrdCXC0Wg/QrrvDxtcEd7TzTLhHkx5WvTue9aJDORb15vhajbJlCSSZgifgg==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/math/math.min.js integrity="sha512-skPZpuRwuUAnF9iEEFBXc4zJaucKcHUDgY1wDBTv0ILy82C2gn8MJsbcinzj2u8r/iZjD/78HRgw2/n//poOhQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>document.addEventListener("DOMContentLoaded",function(){Reveal.initialize({transition:"fade",progress:!0,embedded:!0,minScale:.2,plugins:[RevealMarkdown,RevealHighlight,RevealMath.MathJax3]})})</script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/lxgw-wenkai-webfont/1.6.0/style.min.css><style>body,code,section{font-family:lxgw wenkai,sans-serif}</style><meta property="og:title" content="大模型的计算能力"><meta property="og:description" content="大语言模型的计算能力 大模型领域的几个核心数学问题 N-GRAM 的计算能力问题 过参数化模型的统计学习问题 非凸的数值优化问题 对深度神经网络的数学理解 Transformer 算子的"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.uglyboy.cn/slides/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/"><meta property="article:section" content="slides"><meta property="article:published_time" content="2023-11-21T01:45:37+08:00"><meta property="article:modified_time" content="2023-11-21T02:52:17+08:00"><meta property="og:site_name" content="拾柒读库"><meta name=twitter:card content="summary"><meta name=twitter:title content="大模型的计算能力"><meta name=twitter:description content="大语言模型的计算能力 大模型领域的几个核心数学问题 N-GRAM 的计算能力问题 过参数化模型的统计学习问题 非凸的数值优化问题 对深度神经网络的数学理解 Transformer 算子的"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"演示","item":"https://blog.uglyboy.cn/slides/"},{"@type":"ListItem","position":2,"name":"大模型的计算能力","item":"https://blog.uglyboy.cn/slides/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"大模型的计算能力","name":"大模型的计算能力","description":"大语言模型的计算能力 大模型领域的几个核心数学问题 N-GRAM 的计算能力问题 过参数化模型的统计学习问题 非凸的数值优化问题 对深度神经网络的数学理解 Transformer 算子的","keywords":["LLM"],"articleBody":"大语言模型的计算能力 大模型领域的几个核心数学问题 N-GRAM 的计算能力问题 过参数化模型的统计学习问题 非凸的数值优化问题 对深度神经网络的数学理解 Transformer 算子的含义 fine-tuning 的数学含义 N-GRAM 计算能力问题的描述 大语言模型的基本范式：\n假设 $w_1, w_2,\\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：\n$$ p(w_{1},w_{2},\\dots,w_{N})=\\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\\dots,w_{i}) $$\n该模型是一个 $N-1$ 阶的马尔可夫链，称为 N-GRAM 模型。 该模型的计算能力是有限的，因为它的是个有穷自动机。 非确定型有穷自动机(NFA) 是一个 5 元数组 $Q,\\Sigma,\\delta,q_0,F$，其中 $Q$ 是一个有穷集合，称为状态集。 $\\Sigma$ 是一个有穷集合，称为字母表。 $\\delta:Q\\times\\Sigma_\\varepsilon\\rightarrow \\mathcal{P}(Q)$ 是转移函数。 $q_0\\in Q$ 是起始状态。 $F \\subseteq Q$ 是接受状态集。 大模型是 NFA 的证明 令 $q_0 =\\varepsilon$ 为初始状态，大语言模型的预测函数记为 $$ \\phi:{s_0s_1s_2…s_{n-1}}\\rightarrow s_n,s_i \\in \\Sigma $$\n取 $\\delta$ 为： $$ \\begin{equation} \\delta(q,\\sigma) = \\left \\{ \\begin{array}{ll} q \\circ\\sigma \u0026 \\sigma \\neq \\varepsilon\\\\ q \\circ\\phi(q) \u0026 \\sigma = \\varepsilon \\end{array} \\right. \\end{equation} $$\n也就是将 $Q$ 设置为已经拥有的上文，持续输入或预测下一个字符。\n在 Bhattamishra et al. 2020 中也有类似的实验，实验中基于 Transformer 的大语言模型甚至只能识别弱化的正则语言。\n大模型无法判别一个 ${[0|1]^*}$ 序列中是否有奇数个 $1$。 给定 $n$ 大模型无法生成 $(aa)^n$。 在 Zhou et al. 2023 中还尝试论述了基于 Transformer 的大语言模型也无法实现加法运算之类的复杂运算。\n以及，有穷自动机无法判定 $\\{0^n\\#1^n\\}$ 形式的序列；也无法进行基础的四则运算（无法处理括号的闭合和乘法的优先顺序）。\n大模型的计算能力是有限的 大模型不具备推理能力，这件事与模型的规模无关。 大模型是通过语言概率分布的泛化来模拟出推理能力的假象。 例如：大模型不会数数。他只是把看到过的数数类型的答案都记住了，然后用类比的方式去回答新的问题；如果答案在记忆中，就会回答正确。 更大的模型可以记住更多的答案，但是这并不是我们想要的答案。 我们需要新的范式来解决 AGI 问题。 Agent + LLM 可以成为完备图灵机 Agent 的基本范式是一个 While 程序。 例如下面几种常见的 Agent： ReAct 获得反思推理能力 BabyAGI 基础的计划任务 Agent Reflexion 长期记忆和短期记忆（短期记忆就符合上述流程） AutoGPT 第一个全能 Agent 代码示例： @dataclass class ReAct(ABC): thought: Optional[str] = None action: Optional[str] = None def __post_init__(self): self.obs = self.run() # 获取执行 Action 的结果 act = ReAct() acts = [] while not act.done: acts.append(act) prompt = get_prompt(acts) act: ReAct = ReAct.parse(llm.call(prompt)) # 调用大模型 编程语言 WHILE 语义 (Semnatik): 一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\\mathbb{N}^k\\rightarrow\\mathbb{N}$ 其他未定义的参数可以在程序里被调用,但是必须设定为 $0$ WHILE 程序的结果在结束结束后通过 $x_0$ 传达 对于一个 WHILE 程序,有三种运行模式: 变量赋值: $x_i=x_j+c,c\\in{0,1,−1}$ $P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$ WHILE $x_i \\neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0 定理：编程语言 WHILE 是图灵完备的\n证明: 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。\n证明细节参看：while循环 ，源自 Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen\n实现图灵完备的方法不是唯一的。例如，Schuurmans 2023 就证明了，当可以使用外部存储时，大模型是图灵完备的。\n但是，这并不意味着 Agent + LLM 就是 AGI，毕竟我们现在的电脑本身就是图灵完备的，但是我们并不认为电脑就是 AGI。\n我们需要的是一个可以自适应的 Agent，而不是一个固定的 Agent。也就是可以通过学习来改变自己的行为，而不是通过人工的方式来改变自己的行为。\n以整数加法为例 我们希望 Agent 学会整数加法。但学会的方式不是我们给了它加法的程序让它执行，而是通过加法的真值，“推导”出加法的程序。模拟人做加法的方式，我们尝试让机器训练出加法程序\n已知： 十以内的加减法（这部分的结果是背下来的）； 用字符串来保存十进制的数字； 有额外的存储可以利用（保存进位时的信息）； 目标： 懂得如何利用已知的十以内的加减法的结果，进行多位的加减法运算； 计算时需要从后向前计算； 需要保留进位的信息； 确保计算结果一定是准确的； 这应该是一个强化学习的过程——只有计算正确时，才会获得奖励。其中，尝试探索的动作空间是：\n如何利用十以内的加减 通过怎样的计算顺序 如何利用额外的存储 最终以此来生成一个完整的加法运算流程。\n使用 RL 训练 Agent Agent 的 While 循环模式恰好符合 Bellman 方程的形式。\n通过强化学习训练 Agent，直观上，状态空间和奖励都不难定义，难的是究竟如何定义 Action 空间。\n例如十以内的加减法为什么可以成为加法的 Action？ 进一步，当我们尝试训练乘法时，九九乘法表一定在我们的 Action 空间中，那如何凭空让机器找到这个“九九乘法表”呢？ 甚至计算乘法时，加法也需要在 Action 空间中。这意味着，强化学习能学会内容是有序的，而且 Action 空间的生成也是依赖之前的训练结果的。 这个结果并不意外，《技术的本质》一书中就提到过人类技术的发展是渐进的，而不是突变的。 牛顿无法简单的通过阅读代数和几何来发明微积分。问题是，要生成全新的想法，还缺什么？\n有可能达成 AGI 的新范式应当需要 Agent + LLM 的联合训练，而不是各自的单独训练。\nThanks! ","wordCount":"1952","inLanguage":"zh-cn","datePublished":"2023-11-21T01:45:37+08:00","dateModified":"2023-11-21T02:52:17+08:00","author":{"@type":"Person","name":"癸老师"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.uglyboy.cn/slides/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/"},"publisher":{"@type":"Organization","name":"拾柒读库","logo":{"@type":"ImageObject","url":"https://blog.uglyboy.cn/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.uglyboy.cn/ accesskey=h title="拾柒读库 (Alt + H)">拾柒读库</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.uglyboy.cn/ title=主页><span>主页</span></a></li><li><a href=https://blog.uglyboy.cn/slides/ title=演示><span>演示</span></a></li><li><a href=https://blog.uglyboy.cn/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://blog.uglyboy.cn/tags/ title=标签><span>标签</span></a></li><li><a href=https://blog.uglyboy.cn/archives/ title=时间轴><span>时间轴</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.uglyboy.cn/>主页</a>&nbsp;»&nbsp;<a href=https://blog.uglyboy.cn/slides/>演示</a></div><h1 class=post-title>大模型的计算能力</h1><div class=post-meta><span title='2023-11-21 01:45:37 +0800 CST'>2023-11-21</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;癸老师</div></header><div class=reveal style=height:50vh><div class=slides><section data-markdown><textarea data-template>
        ## 大语言模型的计算能力

---
## 大模型领域的几个核心数学问题

1. N-GRAM 的计算能力问题
2. 过参数化模型的统计学习问题
3. 非凸的数值优化问题
4. 对深度神经网络的数学理解
5. Transformer 算子的含义
6. fine-tuning 的数学含义

---
## N-GRAM 计算能力问题的描述

---

- **大语言模型的基本范式：**

- 假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：

$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$

- 该模型是一个 $N-1$ 阶的马尔可夫链，称为 `N-GRAM` 模型。
- 该模型的计算能力是有限的，因为它的是个有穷自动机。

---

- `非确定型有穷自动机`(`NFA`) 是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中
  - $Q$ 是一个有穷集合，称为**状态集**。
  - $\Sigma$ 是一个有穷集合，称为**字母表**。
  - $\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是**转移函数**。
  - $q_0\in Q$ 是**起始状态**。
  - $F \subseteq Q$ 是**接受状态集**。

---
## 大模型是 NFA 的证明
- 令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为
$$
\phi:\{s_0s_1s_2...s_{n-1}\}\rightarrow s_n,s_i \in \Sigma
$$
- 取 $\delta$ 为：
$$
\begin{equation}
\delta(q,\sigma) = 
\left \\\{
\begin{array}{ll}
q \circ\sigma &amp; \sigma \neq \varepsilon\\\\
q \circ\phi(q) &amp; \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$

- 也就是将 $Q$ 设置为已经拥有的上文，持续输入或预测下一个字符。

---

- 在 [Bhattamishra et al. 2020](https://arxiv.org/abs/2009.11264) 中也有类似的实验，实验中基于 Transformer 的大语言模型甚至只能识别弱化的正则语言。
  - 大模型无法判别一个 $\{[0|1]^*\}$ 序列中是否有奇数个 $1$。
  - 给定 $n$ 大模型无法生成 $(aa)^n$。

- 在 [Zhou et al. 2023](http://arxiv.org/abs/2310.16028) 中还尝试论述了基于 Transformer 的大语言模型也无法实现加法运算之类的复杂运算。

- 以及，有穷自动机无法判定 $\\\{0^n\\\#1^n\\\}$ 形式的序列；也无法进行基础的四则运算（无法处理括号的闭合和乘法的优先顺序）。

---
## 大模型的计算能力是有限的

- 大模型不具备推理能力，这件事与模型的规模无关。
- 大模型是通过语言概率分布的泛化来模拟出推理能力的假象。
  - 例如：大模型不会数数。他只是把看到过的数数类型的答案都记住了，然后用类比的方式去回答新的问题；如果答案在记忆中，就会回答正确。
  - 更大的模型可以记住更多的答案，但是这并不是我们想要的答案。
- 我们需要新的范式来解决 AGI 问题。

---
## Agent &#43; LLM 可以成为完备图灵机
---
- Agent 的基本范式是一个 While 程序。
- 例如下面几种常见的 Agent：
  1. [ReAct](http://arxiv.org/abs/2210.03629) 获得反思推理能力
  2. [BabyAGI](https://github.com/yoheinakajima/babyagi) 基础的计划任务 Agent
  3. [Reflexion](http://arxiv.org/abs/2303.11366) 长期记忆和短期记忆（短期记忆就符合上述流程）
  4. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 第一个全能 Agent

---
- 代码示例：
```python
@dataclass
class ReAct(ABC):
    thought: Optional[str] = None
    action: Optional[str] = None

    def __post_init__(self):
        self.obs = self.run() # 获取执行 Action 的结果

act = ReAct()
acts = []
while not act.done:
    acts.append(act)
    prompt = get_prompt(acts)
    act: ReAct = ReAct.parse(llm.call(prompt)) # 调用大模型
```

---

### 编程语言 WHILE 语义 (Semnatik):

- 一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$
- 其他未定义的参数可以在程序里被调用,但是必须设定为 $0$
- WHILE 程序的结果在结束结束后通过 $x_0$ 传达
- 对于一个 WHILE 程序,有三种运行模式:
  - 变量赋值: $x_i=x_j&#43;c,c\in\{0,1,−1\}$
  - $P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$
  - WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0
---
- **定理：编程语言 WHILE 是图灵完备的**

- **证明:** 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。

- 证明细节参看：[while循环](https://zhuanlan.zhihu.com/p/343107128) ，源自 [Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen](https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf)

---
- 实现图灵完备的方法不是唯一的。例如，[Schuurmans 2023](http://arxiv.org/abs/2301.04589) 就证明了，当可以使用外部存储时，大模型是图灵完备的。

- 但是，这并不意味着 Agent &#43; LLM 就是 AGI，毕竟我们现在的电脑本身就是图灵完备的，但是我们并不认为电脑就是 AGI。

- 我们需要的是一个可以自适应的 Agent，而不是一个固定的 Agent。也就是可以通过学习来改变自己的行为，而不是通过人工的方式来改变自己的行为。

---
## 以整数加法为例

&gt; 我们希望 Agent 学会整数加法。但学会的方式不是我们给了它加法的程序让它执行，而是通过加法的真值，“推导”出加法的程序。模拟人做加法的方式，我们尝试让机器训练出加法程序

---
- **已知**：
  - 十以内的加减法（这部分的结果是背下来的）；
  - 用字符串来保存十进制的数字；
  - 有额外的存储可以利用（保存进位时的信息）；
- **目标：**
  - 懂得如何利用已知的十以内的加减法的结果，进行多位的加减法运算；
  - 计算时需要从后向前计算；
  - 需要保留进位的信息；
  - 确保计算结果一定是准确的；

---
- 这应该是一个强化学习的过程——只有计算正确时，才会获得奖励。其中，尝试探索的动作空间是：
  - 如何利用十以内的加减
  - 通过怎样的计算顺序
  - 如何利用额外的存储

- 最终以此来生成一个完整的加法运算流程。

---
## 使用 RL 训练 Agent

Agent 的 While 循环模式恰好符合 Bellman 方程的形式。

---
通过强化学习训练 Agent，直观上，状态空间和奖励都不难定义，难的是究竟如何定义 Action 空间。

---
- 例如十以内的加减法为什么可以成为加法的 Action？
- 进一步，当我们尝试训练乘法时，九九乘法表一定在我们的 Action 空间中，那如何凭空让机器找到这个“九九乘法表”呢？
- 甚至计算乘法时，加法也需要在 Action 空间中。这意味着，强化学习能学会内容是有序的，而且 Action 空间的生成也是依赖之前的训练结果的。
- 这个结果并不意外，《技术的本质》一书中就提到过人类技术的发展是渐进的，而不是突变的。

---
&gt; 牛顿无法简单的通过阅读代数和几何来发明微积分。问题是，要生成全新的想法，还缺什么？

**有可能达成 AGI 的新范式应当需要 Agent &#43; LLM 的联合训练，而不是各自的单独训练。**

---

## Thanks!
        </textarea></section></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.uglyboy.cn/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://blog.uglyboy.cn/slides/2.-rag%E6%8A%80%E6%9C%AF/><span class=title>« 上一页</span><br><span>RAG 技术</span>
</a><a class=next href=https://blog.uglyboy.cn/slides/1.-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E5%88%86%E4%BA%AB/><span class=title>下一页 »</span><br><span>大语言模型原理分享</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://blog.uglyboy.cn/>拾柒读库</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制代码";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制代码"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>