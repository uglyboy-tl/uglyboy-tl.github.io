<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>机器学习 on 拾柒读库</title>
    <link>https://blog.uglyboy.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 拾柒读库</description>
    <generator>Hugo -- 0.134.3</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 18 Sep 2024 13:23:10 +0800</lastBuildDate>
    <atom:link href="https://blog.uglyboy.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>RAG 的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2024/04/07/rag-%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3/</link>
      <pubDate>Sun, 07 Apr 2024 10:24:34 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/04/07/rag-%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3/</guid>
      <description>&lt;h2 id=&#34;问题的定义&#34;&gt;问题的定义&lt;/h2&gt;
&lt;p&gt;探讨 RAG 之前，我们需要对我们要解决的问题做一个重新的理解。传统的 LLM 是一个语言的概率预测模型，它描述的是语言的自然分布概率，所以对于这样的模型，没有回答的答案哪个更好的说法，只有回答的答案哪个概率更高的描述。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="问题的定义">问题的定义</h2>
<p>探讨 RAG 之前，我们需要对我们要解决的问题做一个重新的理解。传统的 LLM 是一个语言的概率预测模型，它描述的是语言的自然分布概率，所以对于这样的模型，没有回答的答案哪个更好的说法，只有回答的答案哪个概率更高的描述。</p>
<p>但对于 LLM 的真实场景应用来说，我们不仅仅希望 LLM 给出像人一样的回答，而且对回答的答案是有着有别于语言概率分布的预期的。所以对于实际应用场景下的 LLM 的评估，并不是评估 LLM 原本的能力（泛化性），而是评估在很多具体问题下的回答距离某个我们认为合理的回答是否更接近。</p>
<h3 id="更具体的阐述">更具体的阐述</h3>
<p>什么叫做好的 <code>LLM</code>？是回答问题更加准确的模型吗？从 <code>LLM</code> 的训练过程的输入输出（优化目标）来看，非也。</p>
<p><code>LLM</code> 的训练优化的核心目标是泛化性。这个能力其实和人类的智力测试所考察的能力是很类似的——是希望 <code>LLM</code> 具有更强的举一反三，寻找规律的能力，甚至是记忆力（长上下文的大海捞针），但是并不需要考察它已经背下了多少信息。</p>
<blockquote>
<p>很多时候我们对模型的考核是综合的，某种意义上也就混淆了“模型更聪明”和“知道的更多”之间的界限。
所以一般我们认为更加精心挑选的训练样本可以提升模型的能力，以及更小的模型可以比参数更多的模型能力强，基本上都是过多的考察了“知道的更多”这部分的能力；但回归到我们更希望模型具有的“聪明”的能力上，例如“结构化输出”、“Function Cal”、一些简单的推理等等，模型规模带来的界限就异常的坚固了。</p>
</blockquote>
<p>对于 <code>LLM</code> 我们更看重它的“聪明”还是“知道的更多”呢？很遗憾，这两种能力不是完全兼容的。</p>
<p>例如：一个具有良好泛化性能力的模型，是可以兼容各种粗别字、拼写错误、语法错误的语言模型。但如果一个模型具有这种能力，就需要其训练样本中存在含有错别字、拼写错误、语法错误的训练样本。这样的样本训练出来的结果，自然也会有概率输出错误。但这样的模型是更聪明了还是更笨了呢？</p>
<p>类似的，如果希望模型能够充满创造力，那模型就需要拥有想像的能力，需要看到过各种神奇的想像。于是幻觉也正是创造力无法分割的一部分。那有幻觉的模型是更聪明的模型还是更笨的模型呢？</p>
<p>所以这也带来了另一个问题：对于 <code>LLM</code> 现有的评测集究竟考察了哪种 <code>LLM</code> 的能力？是考察了它背住的正确答案的多少？还是在考察 <code>LLM</code> 的泛化性？</p>
<p>因为从 <code>LLM</code> 的优化目标上来看，我们已经有了关于聪明的描述（就是泛化性，虽然我们当前的很多测试集存在问题）。但还有很多的使用场景下，我们更关注的不是模型的“聪明”程度，而是希望模型“知道的更多”，所以本文需要给这种能力重新做一个相对清晰的定义，然后才能进一步分析如何提升这种能力。</p>
<blockquote>
<p>其实这也是 RAG 对这类真实问题能极大提升效果的原因——提升“知道的更多”这个问题的精度的最重要的办法就是背更多的答案。利用了 RAG 做答案检索可以更加精准的给出结果，自然就更容易提升效果。这个后文中会有更多的论述和证明。</p>
</blockquote>
<h3 id="llm-能力的重新定义">LLM 能力的重新定义</h3>
<p>$question$ 表示输入的语言序列，$answer$ 表示输出的语言序列，$score^{question}(answer)$ 表示在 $question$ 这个问题下，$answer$ 这个回答的评分，后面简记为：$score(answer)$。</p>
<p>一个语言模型 <code>LLM</code> 是指一组概率分布：</p>
<p>$$
p^{LLM}(answer|question)
$$</p>
<p>表示在 <code>LLM</code> 这个语言模型中，通过 $question$ 这个输入生成 $answer$ 的概率。</p>
<p>从而我们得到了语言模型在<strong>回答真实问题中</strong>的能力定义：</p>
<p>$$
Score^{LLM} = \sum p^{LLM}(answer|question)\cdot score(answer)
$$</p>
<p>通常，我们无法遍历得到全部的 $question$，同时也无法对全部的 $question$ 进行打分，所以模型实际的分数往往是通过采样的方式得到的估计值。</p>
<h3 id="rag-的定义">RAG 的定义</h3>
<blockquote>
<p>这里给出的定义不是从 RAG 的名词含义出发得到的定义，而是从 RAG 这种手段的本质出发，得到的定义，即——<strong>如何改写 prompt 以获得更好的结果</strong>。</p>
</blockquote>
<p>对于一个已知的语言模型 <code>LLM</code>，我们希望找到最好的一组变换 $f:X\to X,X是语言序列$ 使得：</p>
<p>$$
Score^* = \max_{f} \sum p^{LLM}(answer|f(question))\cdot score(answer)
$$</p>
<p>并将最好的这组变换记作 $f^*$:</p>
<p>$$
f^* = \arg\max_{f} \sum p(answer|f(question))\cdot score(answer)
$$</p>
<p>当取 $f(x)=x$ 时，$Score_{f} = Score_{LLM}$，所以至少有：</p>
<p>$$
Score^* \geq Score_{LLM}
$$</p>
<p>即，一个合理的 RAG 手段至少不会让结果变得更差。</p>
<h3 id="rag-进一步解析">RAG 进一步解析</h3>
<blockquote>
<p>这一节要解答：是不是使用 <code>RAG</code> 总能带来收益呢？如果原本的 <code>LLM</code> 已经很好了，是不是就不需要 <code>RAG</code> 了呢？</p>
</blockquote>
<p>从定义出发，很容易知道，<code>RAG</code> 一定是提效的。上一节也给出了一个简单的理论证明。但更加实践的角度来看，怎么保证 <code>RAG</code> 能带来更好的提升呢？</p>
<h4 id="prompt-engineering">Prompt Engineering</h4>
<p>对于大语言模型，大家都知道神奇的 “Let&rsquo;s think step by step”，对于大部分的 $question$，对其进行改写，加入 <code>Let's think step by step</code>，就能提升回答的效果。</p>
<p>类似的，现在也有诸多的手段进行 <strong>Prompt Engineering</strong>，都是可以提升原始 LLM 的回答结果的。</p>
<p>当然，这些手段是符合我们的 RAG 定义的，但是往往因为其没有更加充分的利用 $question$ 和 $answer$ 的有效信息，不会被归类到通常意义下的 RAG 当中。这部分内容也不放在我们讨论的重点。</p>
<p>但这里有一个相关的技巧，是利用了通常意义下的 RAG 技术，所以值得拿出来单独说明一下的。</p>
<h5 id="cot">CoT</h5>
<p><code>CoT</code> 可以提升 <code>LLM</code> 的效果这一点也是被大家认同的结论。通常意义上来说，这也属于 <code>Prompt Engineering</code> 的一种技巧。但是 <code>CoT</code> 有别于 <code>Let's think step by step</code>，它需要根据 $question$ 和 $answer$ 进行构造。</p>
<p>所以我们可以将已经构造好的 <code>CoT</code> Prompt 放到引擎中，即可通过检索的方式实现我们需要的 $f$，所以是可以通过 <code>RAG</code> 的方式实现 <code>CoT</code> 来优化 <code>LLM</code> 的结果的。</p>
<h4 id="当我们能直接找到-answer">当我们能直接找到 $Answer$</h4>
<p>这是另一个极端——极端乐观的情况。</p>
<p>如果我们可以通过一些手段直接获得</p>
<p>$$
answer^* = \arg\max_{answer} score(answer)
$$</p>
<p>或者一个分数很高的 $answer^*$。则我们的问题将变成：</p>
<p>找到 $f$ 使得 $f(question) = answer$。</p>
<p>通过类似构造的方法：</p>
<p>$$
f(question) = \mbox{请输出如下内容} + answer
$$</p>
<p>即可得到这样的 $f$。</p>
<h5 id="当我们能找到-n-个-answer">当我们能找到 $n$ 个 $Answer$</h5>
<p>如果我们能准确的获得 $score(answer)$ 用上节中的方法，直接利用 $answer^<em>$ 构造 $f^</em>$ 即可。但是真实场景中，我们无法精准的获得 $score(answer)$ 的，所以可以利用 <code>LLM</code> 的能力，构造如下的 $f$：</p>
<p>$$
f(question) = \mbox{根据} + question + \mbox{从下述答案中选出最好的答案} + answer_{1} + answer_{2} + \dots
$$</p>
<p>这里的假定是 <code>LLM</code> 有更大的概率可以根据 $question$ 选出更优质的答案。虽然 “<code>LLM</code> 能选出最优答案”这件事未必令人确信，但如果不利用 <code>LLM</code>，则最合理的答案是几个答案的均匀分布随机，但这样未能更好的利用 $question$ 和 $answer$ 的相关信息。</p>
<blockquote>
<p>可以看到，在这个条件下，我们所获得的 <code>RAG</code> 的映射函数 $f^*$ 已经和现实意义下的 <code>RAG</code> 很相似了。</p>
<p>这样的结果其实充分的说明了，我们通常意义下所使用的 <code>RAG</code> 方法，正是我们这个定义下的最自然的解决方案之一。也侧面说明了本文定义的合理性。</p>
</blockquote>
<h2 id="长文本与-rag">长文本与 RAG</h2>
<p>其实从本文的定义很容易看出，长文本和 <code>RAG</code> 根本上就是完全不同的两个概念，完全没有什么可以比较的意义，讨论长文本和 <code>RAG</code> 的关系这件事其实非常的业余。无奈总有人带节奏说这个话题，所以还是单独拿出一定的篇幅来针对一个很细节的应用场景，来聊一聊这部分内容。</p>
<h3 id="长文本应用的定义">长文本应用的定义</h3>
<p>大语言模型的能力（应用价值）的定义如前文：</p>
<p>$$
Score_{LLM} = \sum p^{LLM}(answer|question)\cdot score(answer)
$$</p>
<p>长文本模型的能力并不会超出这样一个定义，所以想要体现长文本模型能力的价值，需要对上面的使用场景再添加更多的限制条件：</p>
<ol>
<li>我们有一段很长的上下文信息 $context$；</li>
<li>我们的问题 $question$ 需要 $context$ 才能获得优质的答案 $answer$；</li>
<li>原本的 <code>LLM</code> 不一定有能力直接容纳 $context + question$ 这么长的输入文本；</li>
</ol>
<p>在上述限制条件下我们称之为<strong>长文本应用</strong>。</p>
<p>进而，通常意义下长文本给出的解决方案按照本文的定义，即：</p>
<p>$$
f^{Long}(question) = context + question
$$</p>
<p>当然，这样的 $f$ 是需要 <code>LLM</code> 具有长文本能力，才能实现的。</p>
<h3 id="上文条件下的-rag-解">上文条件下的 <code>RAG</code> 解</h3>
<p>从定义出发，</p>
<p>$$
f^* = \arg\max_{f} \sum p(answer|f(question))\cdot score(answer)
$$</p>
<p>所以</p>
<p>$$
f^* \geq f^{Long}
$$</p>
<p>而且因为 $f^{Long}$ 是个平凡的构造解，所以等号成立的概率几乎为零。即长文本带来的能力完全取代不了 <code>RAG</code> 带来的能力</p>
<blockquote>
<p>当然，因为我们并没有对 $f^<em>(question)$ 有任何限制，所以 $f^</em>(question)$ 甚至有可能比 $context + question$ 还要长，更需要 <code>LLM</code> 具有长文本的支持能力。
所以反过来也一样，<code>RAG</code> 带来的能力也取代不了长文本带来的能力。
这就是这一章开头提到的，这是两个完全不同的能力，没有可比性。</p>
</blockquote>
<h3 id="进一步解析">进一步解析</h3>
<p>如上文描述，通常只有 $context$ 很长的情况下，才能充分体现出 <code>LLM</code> 长文本的价值。但是 $context$ 很长，往往又意味着，$context$ 中的信息存在大量的冗余，于是 $f^*$ 相比于 $f^{Long}$ 就有了更加充分的优化空间。</p>
<p>为了说明这件事情，我们也用构造法构造一组解：</p>
<ol>
<li>将 $context$ 切片成很多个 $chunk_{i}$</li>
<li>利用 <code>LLM</code>，对 $chunk_{i}$ 做判断，判断其对回答 $question$ 是否有帮助。</li>
<li>将有帮助的 $chunk_{j}$ 保留并合并，即可形成新的 $context^*$ 相比于 $context$ 通常都是有提升的，（或者防杠精）<strong>至少是无害的，那在计算效率上也是提升</strong>。</li>
</ol>
<p>进一步，如果对 $question$ 有帮助的 $chunk_{i}$ 其实很少，那么我们对长文本的需求就急剧下降了。</p>
<blockquote>
<p>这一点是之前 <code>LLM</code> 长度不足时，大家利用 <code>RAG</code> 解决问题的理由之一；但真的没啥道理被误解成是 <code>RAG</code> 的主要作用。
而且从这里的解析来看，这种方法也确实是个非常好的解决办法——<strong>至少是无害的，那在计算效率上也是提升</strong></p>
</blockquote>
<p>甚至在有价值的 $chunk_{j}$ 很多的情况下，我们可以构造这样的解：</p>
<ol>
<li>
<p>将 $context$ 切片成很多个 $chunk_{i}$</p>
</li>
<li>
<p>利用 <code>LLM</code>，利用 $chunk_{i} + question$ 生成答案 $answer_{i}$</p>
</li>
<li>
<p>类似于第一章中的方法：</p>
<p>$$
f(question) = \mbox{根据} + question + \mbox{根据下述答案，生成更好的答案} + answer_{1} + answer_{2} + \dots$$</p>
</li>
</ol>
<p>这样可以让任意的长度的 $context$ 都可以被极大的压缩，减小对 <code>LLM</code> 长上下文的依赖，并且不会损失最终的效果。</p>
<blockquote>
<p>当然，这样的 $f$ 也还有进一步的提升空间。LamaIndex 中已经有好多中很好的 $f$ 的构造，本文就不赘述了。</p>
</blockquote>
<p>所以如果非要对长文本和 <code>RAG</code> 进行比较，更通常的情况下，确实是对 <code>RAG</code> 技术的需求很高，但对长文本的需求，绝大部分都可以转化成利用 <code>RAG</code> 技术来解决。</p>
<h2 id="为何是-rag">为何是 RAG</h2>
<blockquote>
<p>若如本文中的定义：“如何改写 prompt 以获得更好的结果”，那为什么我们还要把这个技术称为 <code>RAG</code> 呢？或者另一个相关的问题：“为何不通过 SFT 的方式让模型无需改写 prompt 即可获得更好的结果呢？”</p>
</blockquote>
<p>这是一个非常好的问题，甚至可以说，这个问题正好问出了本文给出 <code>LLM</code> 新定义的核心原因：<strong>因为 <code>LLM</code> 的能力强弱和真实场景下人们对答案的评估是不同的</strong>。</p>
<p><code>LLM</code> 的核心能力是泛化性，即在未见过的样本上正确估计的能力。所以强大的 <code>LLM</code> 应当是可以面对满屏错别字和语法混乱的文章，也能顺利读下来并理解其含义的模型，这是 <code>LLM</code> 能力的体现。当然，这样的 <code>LLM</code> 一定能，并且擅长胡说八道，因为这也是泛化性的体现——只不过，更强大的模型说胡话的本领会更强，更加让人察觉不到他在说胡话（指在不知道标准答案的情况下）。</p>
<p><strong>这种能力恰恰是 <code>LLM</code> 给这个世界最好的礼物。</strong></p>
<p>但很多具体的应用场景中，我们对实际的回答是有要求有约束的，我们的约束条件跟 <code>LLM</code> 的能力并不是完全一致的。本文的定义，其实就是从真实场景的实际约束条件出发给出的基本定义——即在有 $question$ 的条件下，找到 $answer^*=\arg\max score(answer)$（这个写法中没有 <code>LLM</code>，就是说这个问题并不是必须使用 <code>LLM</code> 来解决的）。</p>
<p>而非常有意思的是，解决这个问题的方法，在没有 <code>LLM</code> 之前，被定义为 <code>Retrieval</code>（或者是 <code>Search</code>）。</p>
<p>所以 <code>RAG</code> 或者被成为 <code>RCG</code> 都 OK，从历史的发展角度来看，其实应当是出现了 <code>LLM</code> 后，这样一个强大的工具如何如何让 <code>Retrieval</code> 获得更好的结果——无论有没有 <code>LLM</code> 这件事其实都是 <code>Retrieval</code></p>
<p>接下来十分自然的思考就是，<code>LLM</code> 的泛化性的能力（“聪明”的能力），如何帮助 <code>Retrieval</code> 更好的找到（或生成）准确的答案。十分自然的结合就是：用传统的 <code>Retrieval</code> 找到可能有价值的内容，让“聪明”的 <code>LLM</code> 最终判断、组合出最后的答案。这就和 <code>RAG</code> 的字面意义完全一致了。</p>
<blockquote>
<p>前文中也提到了，这其实也是当前大模型评估面临的问题：一些测试集测试的其实并不是大模型的泛化能力，而是真实场景下解决问题的能力。但如果模型针对这种问题提升所谓的“能力”，注定是以降低模型的泛化能力来实现的（参考《大模型对齐的数学理解》）。这其实也回答了，为什么不能用 SFT 替代 RAG。</p>
<p>所以解决这个问题，需要的是在 <code>LLM</code> 的能力和真实问题之间加一层技术解决方案，让这个方案来更好的利用 <code>LLM</code> 的能力，并且可以获得真实场景下的“好”的答案。这个思路其实就是本文 <code>RAG</code> 的定义方式。</p>
<p>或者更进一步描述一些行业的现象：为什么闭源的模型似乎比开源的模型好很多？不一定是模型天然的能力上，闭源比开源好；但在模型到真实问题之间，闭源的模型一定做了非常多的开源模型没有做的事情。</p>
<p><strong>有多少人工就有多少智能！</strong></p>
</blockquote>
<p>补充一点：这里并不是说 SFT 没有用。在 RAG 中，其实有非常多的 SFT 可以发挥价值的地方。这里的观点仅仅是：<strong>SFT 无法替代 RAG 的价值</strong>。</p>
<h3 id="为何-retrieval-总是有效果的">为何 Retrieval 总是有效果的？</h3>
<blockquote>
<p>哪怕接受了 Prompt 改写可以更好的解决真实场景的问题，但是也许有些人还是会对使用传统的 <code>Retrieval</code> 技术有芥蒂，希望有“端到端”的解决方案，少一些人工。所以这里解释清楚究竟 <code>Retrieval</code> 是如何发挥作用的。</p>
</blockquote>
<p>这里简单讨论的是一种很平凡的情况：</p>
<p>对于给定的 $question$ 和 $context$ 只有其中的某个 $chunk_k$ 是对 $answer$ 有帮助的（这里的 $answer$ 可以理解为“正确”的答案）：</p>
<p>$$
\begin{equation}
\begin{array}{ll}
p(answer,question|chunk_i)\leq p(answer,question), &amp; i\neq k \\
p(answer,question|chunk_i)&gt;p(answer,question), &amp; i = k
\end{array}
\end{equation}
$$</p>
<p>其中，$chunk_i$ 是 $context$ 的切片，即：</p>
<p>$$
\begin{equation}
\begin{array}{ll}
p(context) &amp;= p(chunk_1,chunk_2,\dots,chunk_n) \\
&amp;= p(chunk_1)p(chunk_2)\dots p(chunk_n)
\end{array}
\end{equation}
$$</p>
<p>由 (1) 可知，当 $i\neq k$ 时：</p>
<p>$$
p(answer,question|chunk_i) = \frac{p(answer,question,chunk_i)}{p(chunk_i)}\leq p(answer,question)
$$</p>
<p>即：</p>
<p>$$
p(answer,question)p(chunk_i)\geq p(answer,question|chunk_i)
$$</p>
<p>而我们想知道 $context$ 和 $chunk_k$ 谁对 $answer$ 的影响更大，需要比较 $p(answer,question|context)$ 和 $p(answer,question|chunk_k)$ 的大小，于是有：</p>
<p>$$
\begin{equation}
\begin{array}{ll}
\frac{p(answer,question|context)}{p(answer,question|chunk_k)} &amp;= \frac{p(answer,question,context)}{p(context)}\cdot\frac{p(chunk_k)}{p(answer,question,chunk_k)} \\
&amp;=\frac{p(answer,question,context)}{p(answer,question,chunk_k)\cdot\prod_{i\neq k}p(chunk_i)} \\
&amp;= \frac{\prod_{i\neq k} p(answer,question,chunk_i)}{\prod_{i\neq k} p(answer,question)\cdot p(chunk_i)} \\
&amp;\leq 1
\end{array}
\end{equation}
$$</p>
<p>从而证明了：</p>
<p>$$
p(answer,question|context)\leq p(answer,question|chunk_k)
$$</p>
<p>即，使用更短更精准的信息比更长的信息更有利于找到正确答案。</p>
<blockquote>
<p>推导过程中可看出，影响 $context$ 和 $chunk_{k}$ 对结果的影响产生差异的核心原因，不是 $chunk_{k}$ 对结果的正向影响里有多大，而是 $chunk_{i}$ 对结果的负向影响导致了 $context$ 的结果可靠性下降。</p>
<p>这个结论其实是很符合直觉的。但是为了防止有些人对这个概念有误解，还是认真的推演了一下全过程。而且这个推导过程中只使用了条件概率，也就是这个结论跟方法无关——不管是使用 <code>LLM</code> 还是使用 <code>RAG</code>，抑或是未来任何更先进的手段，都不会脱离这个结论——即检索可以让结果变得更好。</p>
<p>当然，在实操过程中，这个命题的条件未必可以完美的满足，例如检索过程中召回不足，或者有错误召回等等问题时，可能会导致结果更差。但这些问题其实才正是我们努力提升检索技术的动力。</p>
</blockquote>
<h4 id="再增加一些解释">再增加一些解释</h4>
<p>上文中对于 $chunk_i$ 的定义，只有一个要求：</p>
<p>$$
p(context) = p(chunk_1)p(chunk_2)\dots p(chunk_n)
$$</p>
<p>甚至对证明过程做一定的优化，只需要保证：</p>
<p>$$
p(context)=p(chunk_1,chunk_2,\dots,chunk_n)
$$</p>
<p>即可（例如切片之间有重叠的情况）。但对于切片的长度、位置、形式其实没有任何要求。</p>
<p>所以无论切片是从段落的维度，还是词句的维度，抑或是 token 的维度，乃至模型里面的 KVCache 的维度，都不影响我们的推导和结论。</p>
<p>反过来说，同样也是表达 <code>RAG</code> 并不局限于对文本切片，任何对 $context$ 的信息做“切片”并检索的方式，都是 <code>RAG</code>。</p>
<p><code>RAG</code> 是一类方法的定义，而不是某个具体实现的小技巧。</p>
<h3 id="不存在通用的-rag">不存在通用的 RAG</h3>
<p>首先从定义出发，不同的 $score(answer)$ 自然就会引导出不同的 <code>RAG</code> 解决方案。</p>
<p>这样的定性答案可能会引起些诸多的误解，所以我们再给出两个例子简单说明一下。</p>
<blockquote>
<p>这里用我最常用的，关于淘宝和小红书如何卖奢侈品的例子来说明。</p>
<p>如果是淘宝卖奢侈品，会认真介绍奢侈品品牌的历史，商品的原料，第三方评测，正品认证，物流，服务保障等等信息，来说明这个奢侈品的高端定位和专业性；</p>
<p>如果是小红书卖奢侈品，会出现这样的介绍：“这个包包非常的贵！但我的男朋友非常爱我，买给了我～你的男朋友爱你吗？你也值得拥有这么贵的包包”。</p>
</blockquote>
<p>有些场景，答案更需要的是精准性，还有一些场景，答案需要的是与用户共情。而在第二类场景中，更共情的答案往往不是更精准的答案。而在通常的聊天沟通场景下，第二类需求是主流。</p>
<blockquote>
<p>如果一个大模型，你跟它聊任何内容，他都可以用古诗（或者流行歌曲的歌词）来给予还比较恰当的回答，那相比于一个给出标准正确答案的大模型，你会更愿意跟这个有诗意（情趣）的大模型聊天。</p>
</blockquote>
<p>另一方面，精准性在不同的场景下往往也有不同的定义。例如实现一个算法，有些场景需要的是高精度，有些场景需要的是高性能，边缘设备上则需要对资源消耗的控制等等。种种不同的环境下，对 $answer$ 的评分就会形成天然的差别。</p>
<p>综上所述，即本节标题：不存在通用的 <code>RAG</code>。应当针对具体场景，针对性的对 <code>RAG</code> 调优。这个经验是跟传统的 <code>Search</code> 相一致的。</p>
<h3 id="本文思路下的一些研究方向">本文思路下的一些研究方向</h3>
<p>写这篇文章，并不是为了证明 <code>RAG</code> 有没有用，而是希望的是通过一个更加清晰的定义，让我们能够跳出一些技术细节，可以从一些宏观角度上来思考 <code>LLM</code> 到<strong>解决真实问题</strong>中间，还有哪些事情可以做。这里没有罗列传统检索中的常见技术或者已经被大部分人接受了的 <code>RAG</code> 技术或技巧。</p>
<h4 id="估计-scoreanswer">估计 $score(answer)$</h4>
<p>如果我们能很高效的估计出 $score(answer)$，极端理想的情况下，我们可以通过遍历所有的 $answer$ 来直接找到最好的 $answer$​，不依赖任何检索或者生成的能力。当然这件事从算力的角度来说也是不现实的。但是类似这样的思路，我们可以通过生成 + 评估的方式，将很多问题的优质答案提前索引起来，然后通过检索的方式快速召回，提升 <code>RAG</code> 的效果，减少 <code>LLM</code> 的资源消耗。</p>
<p>这件事已经有很多的尝试了，主流的方法就是通过 <code>GPT4</code> 来对结果进行评估。</p>
<h4 id="非构造的-f">非构造的 $f$</h4>
<p>当前的 <code>RAG</code> 基本上都是构造性的 $f$，这样的 $f$ 距离 $f^*$ 不会很近。</p>
<p>其实完全可能存在 $f(question)$ 是人类无法阅读的，但 <code>LLM</code> 可以很好的生成 $answer^*$ 的解。<a href="https://arxiv.org/abs/2403.12968">《LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression》</a> 就给出了一个这样的尝试。</p>
<h4 id="已知-answer-的情况下如何反推-question-或者-fquestion">已知 $answer$ 的情况下，如何反推 $question$ 或者 $f(question)$</h4>
<p>这个能力其实对于我们理解如何构造 $f$ 有着至关重要的作用，但现在我们对这件事其实还毫不了解。</p>
<h4 id="更科学评估-llm-的能力">更科学评估 <code>LLM</code> 的能力</h4>
<p>另一方面，跳开 <code>RAG</code> 的视角，而从本文最初的讨论出发，还有一个十分重要的问题值得解决。就是更合理的区分和评估 <code>LLM</code> 的泛化能力与掌握的知识。</p>
<p>其实这一点也是合理评估 <code>RAG</code> 的基础之一。因为我们经常看到一些研究，<code>LLM</code> + <code>RAG</code> 似乎就可以提升 <code>LLM</code> 的能力。但是 <code>RAG</code> 真的能提升 <code>LLM</code> 的能力吗？究竟提升的是智力水平？还是仅仅是开卷考试？</p>
<p>如果是泛泛的使用，这两种能力耦合在一起起作用，不做区分影响不大。但是如果是需要针对性的提升各方面能力，就需要能够对每一种能力做识别和判断，然后才能更好的带来进一步的提升。</p>
]]></content:encoded>
    </item>
    <item>
      <title>大模型对齐的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2024/02/27/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AF%B9%E9%BD%90%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3/</link>
      <pubDate>Tue, 27 Feb 2024 09:29:54 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/02/27/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AF%B9%E9%BD%90%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;AI alignment，广义的 SFT 技术，因为其多种多样的实现方式，包括 continue learning、fine turing、LoRA、RLHF 等等，往往让大家对这个过程充满了好奇和憧憬，觉得似乎任何 NLP 的问题，只要拥有了神乎奇迹的 SFT 能力，就能从 pre-train model 进行进一步的提升，从而解决问题。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p>AI alignment，广义的 SFT 技术，因为其多种多样的实现方式，包括 continue learning、fine turing、LoRA、RLHF 等等，往往让大家对这个过程充满了好奇和憧憬，觉得似乎任何 NLP 的问题，只要拥有了神乎奇迹的 SFT 能力，就能从 pre-train model 进行进一步的提升，从而解决问题。</p>
<p>所以本篇文档就是先放下一切具体的 AI alignment 的方法，从问题定义出发，理解清楚 AI alignment 究竟是个什么过程，能解决什么问题，不能解决什么问题。</p>
</blockquote>
<h2 id="大模型对齐是一个贝叶斯学习">大模型对齐是一个贝叶斯学习</h2>
<p>让我们先来看一看 <code>预训练模型</code> 和对齐获得 <code>chat 模型</code> 之间是什么关系</p>
<p><strong>定义：</strong></p>
<p>$content$ 表示语言序列，$chat$ 表示聊天对话。</p>
<p>于是 $p(content)$ 就是通过 <code>预训练模型</code> 获得的语言概率分布，而 <code>chat 模型</code> 则是我们希望获得的 $p(content|chat)$，即在 <code>chat 模型</code> 中，语言序列的分布情况。</p>
<p>有了上面的基本定义，非常容易看出这是一个很基础的贝叶斯学习问题：</p>
<p>$$
p(content|chat) =\frac{ p(chat|content)\cdot p(content)}{p(chat)}
$$</p>
<p>因为我们业务场景下需要用的只是 <code>chat 模型</code>，所以 $p(chat)=1$，那么想要获得 $p(content|chat)$，其实就变成了求解 $p(chat|content)$。</p>
<p>所以很容易跳过求解细节，得到最终的结论：</p>
<p>广义的 SFT 技术的本质，就是求解 $p(chat|content)$，更具体的说，就是获得“能够判别任意文本序列是否聊天对话”的能力。</p>
<blockquote>
<p>上面的推演过程可以将 chat 换成任何 SFT 的目标，例如 <code>指令跟随</code>，<code>Functall call</code>，<code>RAG 结果调优</code> 等等。</p>
</blockquote>
<h3 id="训练数据">训练数据</h3>
<p>因为要求解的是一个判别问题，所以训练样本的准备就是准备很多的打了 $[1,-1]$ 标签的文本序列，然后训练就完事了。这里面没有，也不需要更复杂的数学原理了。</p>
<h3 id="nlp-问题的学习范式">NLP 问题的学习范式</h3>
<p>因为通常我们是对语言序列（Language）做判别，而不是对编码序列（String）做判别，所以这里有一个常用的利用了贝叶斯思想的技巧：</p>
<p>我们要求的是 $p(chat|content)$，是判别一段人类语言文本是否是 chat，但是如果只是使用上一节中的样本，训练得到的将会是：$p(chat|string)$，也就是判别了一段随机字符串是否是 chat。这里有：</p>
<p>$$
p(chat|string) = p(chat|content)\cdot p(content|string)
$$</p>
<p>虽然理论上可以通过 $p(chat|string)$ 反推 $p(chat|content)$，但是训练获得的 $p^*(chat|string)$ 不是真实概率，相同数量的样本，在 $string$ 空间上训练 和在 $content$ 空间上训练，其误差和泛化性是有显著不同的（因为 $content$ 空间比 $string$ 空间小了很多很多）。</p>
<p>所以，通常 NLP 问题往往构造的训练数据不是直接进行判别模型的训练的，而是在一个预训练的语言模型 $p(content|string)$ 基础上进行训练的。而恰好，$p(content|string) = p(content)$ 就是我们通过预训练得到的基础语言模型，不需要额外再去寻找其他关于语言模型的先验概率了。</p>
<p>实操来说，就是会在 $p(content)$ 模型的基础上，进行判别模型的继续训练。</p>
<h3 id="用-boosting-的思想进行调优">用 Boosting 的思想进行调优</h3>
<p>因为我们准备的样本无法覆盖全部的 $content$ 空间，但我们比较确信我们的样本质量，那么就可以用 Boosting 的思想来优化我们的训练过程。具体来说，就是类似于如下的流程：</p>
<ol>
<li>用训练样本训练一个基础判别模型 $c^0$。</li>
<li>用 $c^0$ 对训练样本进行判别，会找到一些错判或者漏判的样本。</li>
<li>将错判和漏判的样本进行加权，然后重新训练模型，获得新的模型 $c^1$。</li>
<li>重复上面的 2～3 过程，让训练样本能够发挥更大的效果。</li>
</ol>
<p>这个方法很充分的利用了训练样本，但是也会带来<strong>过拟合</strong>的问题，会导致模型的泛化性下降。</p>
<p><strong>这一点很重要</strong>。因为和上面的 NLP 的范式结合，你会发现，在 NLP 的范式下来实践 Boosting 想法，提升 SFT 的训练样本的权重，实操上的执行手段就是 <strong>RLHF</strong>。</p>
<p>所以这里就能得到一个不需要看实操细节就可以得到的结论：</p>
<blockquote>
<p>RLHF 可以更好的利用有限的训练样本，但会影响模型的泛化性。所以 RLHF 只能做有限的调优手段，不能作为 SFT 的主要手段。</p>
</blockquote>
<h3 id="实践中的流程简化">实践中的流程简化</h3>
<p>因为我们最终要求解的是：$p(content|chat) = p(chat|content)\cdot p(content)$，同时呢，$p(chat|content)$ 又需要在 $p(content)$ 模型的基础上进行训练。所以在有深度模型这个描述能力极强的工具的帮助下，我们可以将两个流程合并到一起，直接通过训练样本训练出最终的 $p(content|chat)$ 模型。这个过程也是现在大家实践中真实使用的方式。</p>
<p>从结果来说，这个方式没有任何问题。但是从操作过程来看，会存在一些小隐患：</p>
<p>因为我们的样本本质上是要训练 $p(chat|content)$，所以评估训练质量是应该在 $p(chat|content)$ 空间上进行，看准确度和泛化性。但是我们现在直接拿到的是 $p(content|chat)$，就很难找到适合的评估指标了。两方面原因：</p>
<ol>
<li>$content$ 空间太大，本身就很难评估结果，包括不限于评估 $p(content)$ 和 $p(content|chat)$。</li>
<li>$p(content|chat)$ 其实同时在考量 $p(content)$ 和 $p(chat|content)$ 两个模型的效果，加上原本 $p(content)$ 在不同测试集下的表现就不尽相同，更难以衡量 $p(chat|content)$ 工作的质量了。</li>
</ol>
<p>所以给到的建议是，通过</p>
<p>$$
p(chat|content) = \frac{p(content|chat)}{p(content)}
$$</p>
<p>来计算原本 AI alignment 在执行的目标：${p(chat|content)}$，然后对这个目标进行准确性和泛化性的评估。无论是评测样本的构造还是评估效果的可靠性都会得到极大的提升。</p>
<h2 id="ai-alignment-这个手段能带来什么">AI Alignment 这个手段能带来什么</h2>
<p>从定义来看，AI alignment 做的事情是给预训练的大语言模型添加关于使用场景的限定条件，例如：</p>
<ol>
<li>现在是 chat 场景了，你只输出你原本输出中属于 chat 的部分。什么？你不知道哪些是 chat 的部分？我给你一个条件概率 $p(chat|content)$，现在你知道了，可以输出了。</li>
<li>现在你是一个文艺青年，你只输出你原本输出中文艺青年会表达的方式。什么？你不知道什么是文艺青年？我给你一个条件概率 $p(文艺青年|content)$，现在你知道了，可以输出了。</li>
<li>现在你是一个会按照特定结构（例如 json）输出结果的语言工具，……</li>
<li>……</li>
</ol>
<h3 id="ai-alignment-永远没办法让模型变得更好">AI Alignment 永远没办法让模型变得更好</h3>
<p>从 AI alignment 的名称就知道，它做的事情是对齐，而不是提升。相当于大语言模型有很多的使用场景，我们通常得到的模型是一个在场景选取方面用均匀概率分布的模型。AI alignment 则是更明确的给出关于使用场景的先验信息，让模型从通用场景模型转化到专用场景的模型。</p>
<p>但反过来说，如果一件事通用场景模型就做不到，那我们是没有办法找到一个专用场景，让模型能做到这件事的。</p>
<p>例如指令追随，如果模型的能力不足，通用模型就一点都 follow 不了指令，那无论怎样做 SFT，都没有办法实现这个能力。其他能力也类似，包括不限于：数学计算能力、数数能力、简单的推理能力、理解和翻译能力。</p>
<p>模型最终的能力不是：预训练模型的能力分 + Alignment 的能力分。而是这样的关系式：</p>
<p>模型最终的能力 &lt;= 预训练模型的能力分，Alignment 可以让不等式左边更加接近上界。</p>
<h2 id="关于贝叶斯思想">关于贝叶斯思想</h2>
<p>上面的讨论中两次用到了贝叶斯概率的思想。这个思想对于解决现实世界的问题其实往往起着至关重要的作用（注意，这里讨论的不是贝叶斯学派跟频率学派的纷争，他们争论的要点是关于参数究竟是参数空间的后验还是先验的问题，我们当面讨论的话题两个学派都没有任何分歧的。）</p>
<blockquote>
<p>以新冠检测为例：如果一项检测技术，新冠患者阳性的概率是 95%，而健康人阳性的概率是 5%。那么你进行了这项检测，并且呈阳性，你感染新冠的概率有多大？95%？</p>
<p>不，是需要看现在患者和健康人的比例才能知道你感染的概率（很有意思的现象，你是否感染，竟然跟所有人的比例有关）。</p>
<p>$$
\begin{equation}
\begin{array}{lll}
p(患者|阳性) =p(阳性|患者)\cdot p(患者) =0.95\cdot p(患者) \\
p(健康人|阳性) =p(阳性|健康人)\cdot p(患者) = 0.05\cdot p(健康人)
\end{array}
\end{equation}
$$</p>
<p>像现在这种新冠基本绝迹的情况下，检测阳性很大概率是假阳性，问题不大。</p>
<p>但如果是大流行的时候，往往没检测出阳性，也很有可能已经生病了。</p>
</blockquote>
<p>上面的例子给我们一个重要的冲击和启示：先验概率分布对后验概率分布的影响非常大。所以很多时候，或许更好的问题解决办法不是提升条件概率的精度，而是直接拿到更加准确的先验概率。</p>
<h3 id="nlp-领域的人容易钻的牛角尖">NLP 领域的人容易钻的牛角尖</h3>
<p>大模型是个很强大的工具，能解决很多以前解决不好的问题。所以 NLP 的“专家”们就开始拿着锤子找钉子，看到了很多以前做得不好的工作，希望用 NLP 来替代。但很可能他们找到的问题是优化条件概率，但对结果起更重要影响的是先验概率。</p>
<h4 id="搜索排序打分">搜索排序打分</h4>
<p>用户搜索一个 Query，我们需要判断返回结果的排序（给找回的文本打分，判断被用户选择的概率大小）</p>
<p>$$
p(answer|query) = p(query|answer)p(answer)
$$</p>
<p>NLP 的同学以为他们通过大模型拿到的是 $p(answer|query)$，但仔细思考，你会发现通用大模型获得的能力描述不了真实场景下的先验概率，所以 NLP 工具获得的只是 $p(query|answer)$，只表达了 query 和 answer 的相关性描述。表达不了用户对 $p(answer)$ 先验的喜好程度。</p>
<p>所以带来的结果就是，当前的大部分 RAG 体系，只通过向量检索计算了文本相似度，并没有关于文本的静态质量分。</p>
<p>但是对于真实用户的使用来说，更多的情况下，一个相对差一点的 $p(query|answer)$ 或许是可以接受的，但低质量的返回结果是不可接受的。</p>
<h4 id="搜索的先验概率思想">搜索的先验概率思想</h4>
<p>传统搜索中的很多解法其实都是更好的利用了先验概率，从而获得了更加精准和快速的结果。这里再举一个例子：</p>
<blockquote>
<p>用户搜索一个 Query，无论是大模型领域，还是搜索领域，第一步往往都值得做一个用户意图的识别，然后通过路由去寻找更好的专用模型（引擎）来回答问题。</p>
<p>用户意图识别在搜索领域被称为类目预测。</p>
<p>搜索中做类目预测的办法简单、粗暴、有效：</p>
<ol>
<li>将 query 分解成关键词</li>
<li>按关键词组合，统计历史上同样关键词组合的用户，真实点击的内容所在的类目，得到 关键词组合&lt;-&gt;类目 的映射关系</li>
<li>长尾关键词可能会找不到足够的历史数据而出错。但是搜索中大头流量都是可以找到精准的类目预测关系的。</li>
</ol>
</blockquote>
<p>这个过程中，没有任何试图理解词的真实语义的尝试，但已经十分精准的解决了搜索在绝大部分时间（流量）下的用户意图识别问题。而且解决方案直观、高效、精度高。</p>
<p>但是如果是使用大模型技术做意图识别，会漏掉先验概率对结果的影响（例如情人节搜鲜花和清明节搜鲜花，给出的鲜花品类应该是不同的，大语言模型是无法理解这种先验概率的变化的；搜索的模型也无法理解，但是可以快速捕捉到这种变化）。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Agent 使用指南</title>
      <link>https://blog.uglyboy.cn/slides/2023/12/10/agent-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</link>
      <pubDate>Sun, 10 Dec 2023 09:55:03 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/slides/2023/12/10/agent-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</guid>
      <description>&lt;div class=&#34;reveal&#34;&gt;
  &lt;div class=&#34;slides&#34;&gt;
    &lt;section data-markdown&gt;
      &lt;textarea data-template&gt;
      

## Agent 使用指南

---

### 如何在真实世界中使用大模型？

&amp;gt; 我说想开灯，大模型能帮我开灯吗？

---
- 我们希望大模型能够直接的解决问题
- 这样的使用方法现在被人们称为： **Agent**
---
#### 简单应用：如何修复一个 json 串？
-----
- 错误做法：
  - 问大模型这个字符串哪里错了？
  - 写代码修复字符串
- 正确做法：
  - &amp;#34;Do not change the specific content, fix the json, directly return the repaired JSON, without any explanation and dialogue.&amp;#34;

---
#### 什么情况下我们需要 Agent ？
-----
- 不需要的情况：
  - 操作简单，人可以快速解决；
  - 规则清晰，程序可以按规则解决；
- 需要的情况：
  - 解决步骤繁琐，对于人来说是重复劳动；
  - 情况复杂，不容易梳理清楚对应的解决规则；

---
#### 场景举例
-----
- Github 的 Copilot
- MacOS 的 Copilot
- 软件自动运维
- Yi 6B 的自动安装助手
- 上网助手
- $\dots$
---
#### 高级场景举例
-----
- 自动软件开发（GPT Engineer）
- 自动会议纪要
  - 自动任务记录
  - 自动会议预定
  - 自动消息通知
- 高级个人助理
- 一切自动化流程中需要人工参与的部分
---
#### 解决复杂问题有哪些难点？
-----
- 解决复杂问题往往需要很多步骤；
  - 而且正确的步骤往往也也需要探索；
- Agent 需要能够正确的了解自己当前解决问题的进度；
- 具体的步骤中，Agent 需要可以使用具体的工具；
---
#### 传统 Agent 的定义
###### **Agent = LLM &amp;#43; planning &amp;#43; memory &amp;#43; tools**
-----
&amp;lt;center&amp;gt;&amp;lt;img src=&amp;#34;https://s2.loli.net/2023/12/18/Pucaj8OELXfeTZm.jpg&amp;#34;&amp;gt;&amp;lt;/center&amp;gt;
---
### Agent 究竟是如何工作的？
---
#### ReAct：Agent 的核心模式
&amp;lt;center&amp;gt;&amp;lt;img src=&amp;#34;https://s2.loli.net/2023/12/18/klDFI9n2Mq5Kf61.png&amp;#34; width=&amp;#34;70%&amp;#34;&amp;gt;&amp;lt;/center&amp;gt;
---
#### ReAct 的核心思想
-----
- 循环试错（以此来取消幻觉）；
- 充分利用大模型的能力（复杂条件下也总能给出解决方案）；
  - 大模型的知识足够多，且有泛化性，适合解决复杂问题；
  - 大模型可以很好的理解复杂的条件，找出适合的解决方案；
- 通过对历史行为的记录（记忆），避免重复错误的尝试；
---
### Agent 测验
---

#### 如何打造一个联网的 LLM ？
-----
- **[Thought]** 针对用户输入，让 LLM 判断应该从哪个`搜索引擎`用什么`关键词` 获取结果；
- **[Act]** 获取`搜索引擎`在对应`关键词`下的搜索结果；
- **[Obs]** 具体返回的搜索结果；
- **[Thought]** 判断是否回答了用户的输入，如果没有，类似第一步，继续判断如何搜索；

---
#### 如何打造 Yi 6B 的自动安装助手
---
#### 如何打造 MacOS 的 Copilot
---
#### 如何打造一个可以自动软件开发的 Agent

---
### 这件事需要产品做什么？
---
- **找到适合的场景**
- 设计场景下的合理交互
- 总结场景下的最佳实践(作为数据喂给模型就可以了)
- 扩展场景
---
&amp;gt; 大模型还没有出现 Killer APP 是因为还没有懂 Agent 的产品经理
---

# Q &amp; A
      &lt;/textarea&gt;
    &lt;/section&gt;
  &lt;/div&gt;
&lt;/div&gt;</description>
      <content:encoded><![CDATA[<div class="reveal">
  <div class="slides">
    <section data-markdown>
      <textarea data-template>
      

## Agent 使用指南

---

### 如何在真实世界中使用大模型？

&gt; 我说想开灯，大模型能帮我开灯吗？

---
- 我们希望大模型能够直接的解决问题
- 这样的使用方法现在被人们称为： **Agent**
---
#### 简单应用：如何修复一个 json 串？
-----
- 错误做法：
  - 问大模型这个字符串哪里错了？
  - 写代码修复字符串
- 正确做法：
  - &#34;Do not change the specific content, fix the json, directly return the repaired JSON, without any explanation and dialogue.&#34;

---
#### 什么情况下我们需要 Agent ？
-----
- 不需要的情况：
  - 操作简单，人可以快速解决；
  - 规则清晰，程序可以按规则解决；
- 需要的情况：
  - 解决步骤繁琐，对于人来说是重复劳动；
  - 情况复杂，不容易梳理清楚对应的解决规则；

---
#### 场景举例
-----
- Github 的 Copilot
- MacOS 的 Copilot
- 软件自动运维
- Yi 6B 的自动安装助手
- 上网助手
- $\dots$
---
#### 高级场景举例
-----
- 自动软件开发（GPT Engineer）
- 自动会议纪要
  - 自动任务记录
  - 自动会议预定
  - 自动消息通知
- 高级个人助理
- 一切自动化流程中需要人工参与的部分
---
#### 解决复杂问题有哪些难点？
-----
- 解决复杂问题往往需要很多步骤；
  - 而且正确的步骤往往也也需要探索；
- Agent 需要能够正确的了解自己当前解决问题的进度；
- 具体的步骤中，Agent 需要可以使用具体的工具；
---
#### 传统 Agent 的定义
###### **Agent = LLM &#43; planning &#43; memory &#43; tools**
-----
&lt;center&gt;&lt;img src=&#34;https://s2.loli.net/2023/12/18/Pucaj8OELXfeTZm.jpg&#34;&gt;&lt;/center&gt;
---
### Agent 究竟是如何工作的？
---
#### ReAct：Agent 的核心模式
&lt;center&gt;&lt;img src=&#34;https://s2.loli.net/2023/12/18/klDFI9n2Mq5Kf61.png&#34; width=&#34;70%&#34;&gt;&lt;/center&gt;
---
#### ReAct 的核心思想
-----
- 循环试错（以此来取消幻觉）；
- 充分利用大模型的能力（复杂条件下也总能给出解决方案）；
  - 大模型的知识足够多，且有泛化性，适合解决复杂问题；
  - 大模型可以很好的理解复杂的条件，找出适合的解决方案；
- 通过对历史行为的记录（记忆），避免重复错误的尝试；
---
### Agent 测验
---

#### 如何打造一个联网的 LLM ？
-----
- **[Thought]** 针对用户输入，让 LLM 判断应该从哪个`搜索引擎`用什么`关键词` 获取结果；
- **[Act]** 获取`搜索引擎`在对应`关键词`下的搜索结果；
- **[Obs]** 具体返回的搜索结果；
- **[Thought]** 判断是否回答了用户的输入，如果没有，类似第一步，继续判断如何搜索；

---
#### 如何打造 Yi 6B 的自动安装助手
---
#### 如何打造 MacOS 的 Copilot
---
#### 如何打造一个可以自动软件开发的 Agent

---
### 这件事需要产品做什么？
---
- **找到适合的场景**
- 设计场景下的合理交互
- 总结场景下的最佳实践(作为数据喂给模型就可以了)
- 扩展场景
---
&gt; 大模型还没有出现 Killer APP 是因为还没有懂 Agent 的产品经理
---

# Q & A
      </textarea>
    </section>
  </div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>RAG 技术</title>
      <link>https://blog.uglyboy.cn/slides/2023/11/23/rag-%E6%8A%80%E6%9C%AF/</link>
      <pubDate>Thu, 23 Nov 2023 06:35:17 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/slides/2023/11/23/rag-%E6%8A%80%E6%9C%AF/</guid>
      <description>&lt;div class=&#34;reveal&#34;&gt;
  &lt;div class=&#34;slides&#34;&gt;
    &lt;section data-markdown&gt;
      &lt;textarea data-template&gt;
      
# RAG 技术

---

- 检索增强的生成系统（Retrieve Augment Generation）简称 RAG。
- 原理是在大语言模型的基础上，辅助检索技术，让大语言模型能够获得与用户问题相关的更多上下文信息，使得的大语言模型可以：
  - 降低幻觉出现概率
  - 适应垂直场景应用
  - 弥补数据实时性不足

---

### 一个典型 RAG 系统的架构

&amp;lt;center&amp;gt;&amp;lt;img src=&amp;#34;https://s2.loli.net/2023/11/26/UcTsI8xJVdMFfRt.jpg&amp;#34;&amp;gt;&amp;lt;/center&amp;gt;

---

### RAG 系统的核心技术要素

- 文档导入
- 文档切分
- 文档向量化
- 向量数据库选型
- 检索算法
- 文档排序
- Prompt 生成
- $\dots$
---

市面上大部分的关于 RAG 的介绍都是类似上面的逻辑进行的，然后就顺利的将 *某一种 RAG 的方法* 变成了 *通用 RAG 的框架*，从而让我们迷失了 RAG 的真正价值。

---

## 从定义出发，RAG 就是

## 检索 &amp;#43; 生成

---
- Chat With Documents 属于 RAG
- 用户对话中保留历史记忆 属于 RAG
- 网页搜索 &amp;#43; LLM 属于 RAG
- 自动调用 API 接口获取信息 属于 RAG
- 调用数据库获取信息 属于 RAG
- $\dots$
- **上面各种方法一起使用也属于 RAG**

---

## RAG 究竟意味着什么？

&amp;gt; 为什么我们要使用检索

---

- 人类行为的两种模式：主动获取信息（功利动机行为）和被动获取信息（共情动机行为）；
  - 通常在产品上，我们可以用 *Save time* 和 *Kill time* 的模式来区分
- 主动获取信息的手段被称为信息检索。
  - RAG 更标准的说法应当是有了 LLM 能力加持的信息检索。
---

## LLM 很难独立完成检索

---
- 最核心的问题是，对于如何引导大模型按照我们的意愿生成内容，**我们无法直接控制，我们只能通过增加上下文的方式来影响生成结果**。
  - 对于大模型来说，它会如何回答一个问题依赖的不是训练框架，而是训练数据。
  - 我们无法直接控制大模型的生成结果，但是我们可以通过增加上下文的方式来影响生成结果。
  - 一个问题，我们可以提供相关的上下文，然后利用大模型的泛化能力，让它生成我们想要的答案。

---

&amp;lt;center&amp;gt;&amp;lt;img src=&amp;#34;https://s2.loli.net/2023/11/27/B1NkImf2eKrZl7J.png&amp;#34;&amp;gt;&amp;lt;/center&amp;gt;
---

- 大模型的“记忆力”并不可靠，不同的上下文会引导出怎样的结果是不确定的。
  - 仅靠大模型，是无法取消幻觉的。
- 如果 RAG 做得不好，可能带来的是负面效果。
---

## RAG 的核心
## 如何用好检索

---

## 检索的发展史

1. 图书馆的索引式检索（Yahoo 等目录网页）；
2. 关键词召回（传统搜索）；
3. 向量相似度（个性化推荐）；
4. 自然语言回答问题（大模型）；

&amp;gt; 这些方法不是递进的，而是并列的。

---

## 新概念下的 RAG 框架

---
1. 对用户问题分类，判断使用哪些检索器；
2. 根据用户问题，找到最适合的检索器检索方式（Query、SQL、API 调用等）；
3. 召回的结果，判断与用户问题的相关性，进行合理过滤或改进；
4. 用适合的方式组织召回结果，提供给 LLM 进行汇总并回答用户问题；
5. （可选）判断是否很好的回答了用户的问题，是否需要重新再来一遍（这其实就进化成 Agent 了）。

---
- 可以使用不同的 LLM 来执行不同的任务，这样就可以在计算速度和资源上得到极大的节约，并针对特定问题取得更好的效果。
- 检索器的各种优化技术都值得使用：
  - 包括传统的关键词搜索（QP）
  - 向量检索只是其中的一种手段；同时向量检索也应当额外建立适合的索引。
  - 知识图谱是有效的检索器之一。
  - 利用好结构化信息（数据库 或 API）。
- 好的检索器依赖好的数据。
---

# Q &amp; A

---

#### 如果我们有一些私有的数据，如何让大模型能够利用这些私有数据呢？

-----
- 通过微调的方式，将私有数据加入到大模型的训练数据中。
- 通过检索的方式，将私有数据加入到大模型的上下文中。
- **以上方法都用**

---

#### 怎样才能更好的提升 RAG 的效果？

-----

最核心的要素其实是：找到更优质的数据（准确、结构化）

---

#### 产品和开发要深入研究 Prompt Engineering 吗？

-----

永远都不要这样做，这件事交给 SFT

- 概念对比
- Let’s think step by step
- 通用优化 Prompt 的 Prompt
- function call
- Self RAG
- 让模型来学习如何 Prompt Engineering
      &lt;/textarea&gt;
    &lt;/section&gt;
  &lt;/div&gt;
&lt;/div&gt;</description>
      <content:encoded><![CDATA[<div class="reveal">
  <div class="slides">
    <section data-markdown>
      <textarea data-template>
      
# RAG 技术

---

- 检索增强的生成系统（Retrieve Augment Generation）简称 RAG。
- 原理是在大语言模型的基础上，辅助检索技术，让大语言模型能够获得与用户问题相关的更多上下文信息，使得的大语言模型可以：
  - 降低幻觉出现概率
  - 适应垂直场景应用
  - 弥补数据实时性不足

---

### 一个典型 RAG 系统的架构

&lt;center&gt;&lt;img src=&#34;https://s2.loli.net/2023/11/26/UcTsI8xJVdMFfRt.jpg&#34;&gt;&lt;/center&gt;

---

### RAG 系统的核心技术要素

- 文档导入
- 文档切分
- 文档向量化
- 向量数据库选型
- 检索算法
- 文档排序
- Prompt 生成
- $\dots$
---

市面上大部分的关于 RAG 的介绍都是类似上面的逻辑进行的，然后就顺利的将 *某一种 RAG 的方法* 变成了 *通用 RAG 的框架*，从而让我们迷失了 RAG 的真正价值。

---

## 从定义出发，RAG 就是

## 检索 &#43; 生成

---
- Chat With Documents 属于 RAG
- 用户对话中保留历史记忆 属于 RAG
- 网页搜索 &#43; LLM 属于 RAG
- 自动调用 API 接口获取信息 属于 RAG
- 调用数据库获取信息 属于 RAG
- $\dots$
- **上面各种方法一起使用也属于 RAG**

---

## RAG 究竟意味着什么？

&gt; 为什么我们要使用检索

---

- 人类行为的两种模式：主动获取信息（功利动机行为）和被动获取信息（共情动机行为）；
  - 通常在产品上，我们可以用 *Save time* 和 *Kill time* 的模式来区分
- 主动获取信息的手段被称为信息检索。
  - RAG 更标准的说法应当是有了 LLM 能力加持的信息检索。
---

## LLM 很难独立完成检索

---
- 最核心的问题是，对于如何引导大模型按照我们的意愿生成内容，**我们无法直接控制，我们只能通过增加上下文的方式来影响生成结果**。
  - 对于大模型来说，它会如何回答一个问题依赖的不是训练框架，而是训练数据。
  - 我们无法直接控制大模型的生成结果，但是我们可以通过增加上下文的方式来影响生成结果。
  - 一个问题，我们可以提供相关的上下文，然后利用大模型的泛化能力，让它生成我们想要的答案。

---

&lt;center&gt;&lt;img src=&#34;https://s2.loli.net/2023/11/27/B1NkImf2eKrZl7J.png&#34;&gt;&lt;/center&gt;
---

- 大模型的“记忆力”并不可靠，不同的上下文会引导出怎样的结果是不确定的。
  - 仅靠大模型，是无法取消幻觉的。
- 如果 RAG 做得不好，可能带来的是负面效果。
---

## RAG 的核心
## 如何用好检索

---

## 检索的发展史

1. 图书馆的索引式检索（Yahoo 等目录网页）；
2. 关键词召回（传统搜索）；
3. 向量相似度（个性化推荐）；
4. 自然语言回答问题（大模型）；

&gt; 这些方法不是递进的，而是并列的。

---

## 新概念下的 RAG 框架

---
1. 对用户问题分类，判断使用哪些检索器；
2. 根据用户问题，找到最适合的检索器检索方式（Query、SQL、API 调用等）；
3. 召回的结果，判断与用户问题的相关性，进行合理过滤或改进；
4. 用适合的方式组织召回结果，提供给 LLM 进行汇总并回答用户问题；
5. （可选）判断是否很好的回答了用户的问题，是否需要重新再来一遍（这其实就进化成 Agent 了）。

---
- 可以使用不同的 LLM 来执行不同的任务，这样就可以在计算速度和资源上得到极大的节约，并针对特定问题取得更好的效果。
- 检索器的各种优化技术都值得使用：
  - 包括传统的关键词搜索（QP）
  - 向量检索只是其中的一种手段；同时向量检索也应当额外建立适合的索引。
  - 知识图谱是有效的检索器之一。
  - 利用好结构化信息（数据库 或 API）。
- 好的检索器依赖好的数据。
---

# Q & A

---

#### 如果我们有一些私有的数据，如何让大模型能够利用这些私有数据呢？

-----
- 通过微调的方式，将私有数据加入到大模型的训练数据中。
- 通过检索的方式，将私有数据加入到大模型的上下文中。
- **以上方法都用**

---

#### 怎样才能更好的提升 RAG 的效果？

-----

最核心的要素其实是：找到更优质的数据（准确、结构化）

---

#### 产品和开发要深入研究 Prompt Engineering 吗？

-----

永远都不要这样做，这件事交给 SFT

- 概念对比
- Let’s think step by step
- 通用优化 Prompt 的 Prompt
- function call
- Self RAG
- 让模型来学习如何 Prompt Engineering
      </textarea>
    </section>
  </div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>大模型的计算能力</title>
      <link>https://blog.uglyboy.cn/slides/2023/11/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/</link>
      <pubDate>Tue, 21 Nov 2023 01:45:37 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/slides/2023/11/21/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/</guid>
      <description>&lt;div class=&#34;reveal&#34;&gt;
  &lt;div class=&#34;slides&#34;&gt;
    &lt;section data-markdown&gt;
      &lt;textarea data-template&gt;
      
## 大语言模型的计算能力

---
### LLM 的几个核心数学问题
-----
1. N-GRAM 的计算能力问题
2. 过参数化模型的统计学习问题
3. 非凸的数值优化问题
4. 对深度神经网络的数学理解
5. Transformer 算子的含义
6. fine-tuning 的数学含义

---
### N-GRAM 的计算能力

---

- **大语言模型的基本范式：**
- 假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：

$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$

- 该模型是一个 $N-1$ 阶的马尔可夫链，称为 `N-GRAM` 模型。
- 该模型的计算能力是有限的，因为它的是个有穷自动机。

---

- `非确定型有穷自动机`(`NFA`) 是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中
  - $Q$ 是一个有穷集合，称为**状态集**。
  - $\Sigma$ 是一个有穷集合，称为**字母表**。
  - $\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是**转移函数**。
  - $q_0\in Q$ 是**起始状态**。
  - $F \subseteq Q$ 是**接受状态集**。

---
#### 大模型是有穷自动机的证明过程
-----
- 令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为

$$
\phi:\{s_0s_1s_2...s_{n-1}\}\rightarrow s_n,s_i \in \Sigma
$$

- 取 $\delta$ 为：

$$
\begin{equation}
\delta(q,\sigma) = 
\left \\\{
\begin{array}{ll}
q \circ\sigma &amp; \sigma \neq \varepsilon\\\\
q \circ\phi(q) &amp; \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$

- 也就是将 $Q$ 设置为已经拥有的上文，持续输入或预测下一个字符。

---

- 在 [Bhattamishra et al. 2020](https://arxiv.org/abs/2009.11264) 中也有类似的实验，实验中基于 Transformer 的大语言模型甚至只能识别弱化的正则语言。
  - 大模型无法判别一个 $\{[0|1]^*\}$ 序列中是否有奇数个 $1$。
  - 给定 $n$ 大模型无法生成 $(aa)^n$。
- 在 [Zhou et al. 2023](http://arxiv.org/abs/2310.16028) 中还尝试论述了基于 Transformer 的大语言模型也无法实现加法运算之类的复杂运算。
- 以及，有穷自动机无法判定 $\\\{0^n\\\#1^n\\\}$ 形式的序列；也无法进行基础的四则运算（无法处理括号的闭合和乘法的优先顺序）。

---
### 大模型计算能力是有限的
-----
- 大模型不具备推理能力，这件事与模型的规模无关。
- 大模型是通过语言概率分布的泛化来模拟出推理能力的假象。
  - 例如：大模型不会数数。他只是把看到过的数数类型的答案都记住了，然后用类比的方式去回答新的问题；如果答案在记忆中，就会回答正确。
  - 更大的模型可以记住更多的答案，但是这并不是我们想要的答案。
- 我们需要新的范式来解决 AGI 问题。

---
### Agent &amp;#43; LLM 可以成为完备图灵机
---
- Agent 的基本范式是一个 While 程序。
- 例如下面几种常见的 Agent：
  1. [ReAct](http://arxiv.org/abs/2210.03629) 获得反思推理能力
  2. [BabyAGI](https://github.com/yoheinakajima/babyagi) 基础的计划任务 Agent
  3. [Reflexion](http://arxiv.org/abs/2303.11366) 长期记忆和短期记忆（短期记忆就符合上述流程）
  4. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 第一个全能 Agent

---
- 代码示例：

```python
@dataclass
class ReAct(ABC):
    thought: Optional[str] = None
    action: Optional[str] = None

    def __post_init__(self):
        self.obs = self.run() # 获取执行 Action 的结果

act = ReAct()
acts = []
while not act.done:
    acts.append(act)
    prompt = get_prompt(acts)
    act: ReAct = ReAct.parse(llm.call(prompt)) # 调用大模型
```

---

#### 编程语言 WHILE 语义 (Semnatik)
-----
- 一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$
- 其他未定义的参数可以在程序里被调用,但是必须设定为 $0$
- WHILE 程序的结果在结束结束后通过 $x_0$ 传达
- 对于一个 WHILE 程序,有三种运行模式:
  - 变量赋值: $x_i=x_j&amp;#43;c,c\in\{0,1,−1\}$
  - $P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$
  - WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0
---
- **定理：编程语言 WHILE 是图灵完备的**
- **证明:** 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。
- 证明细节参看：[while循环](https://zhuanlan.zhihu.com/p/343107128) ，源自 [Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen](https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf)

---
- 实现图灵完备的方法不是唯一的。例如，[Schuurmans 2023](http://arxiv.org/abs/2301.04589) 就证明了，当可以使用外部存储时，大模型是图灵完备的。
- 但是，这并不意味着 Agent &amp;#43; LLM 就是 AGI，毕竟我们现在的电脑本身就是图灵完备的，但是我们并不认为电脑就是 AGI。
- 我们需要的是一个可以自适应的 Agent，而不是一个固定的 Agent。也就是可以通过学习来改变自己的行为，而不是通过人工的方式来改变自己的行为。

---
### 以整数加法为例
-----

&amp;gt; 我们希望 Agent 学会整数加法。但学会的方式不是我们给了它加法的程序让它执行，而是通过加法的真值，“推导”出加法的程序。模拟人做加法的方式，我们尝试让机器训练出加法程序

---
- **已知**：
  - 十以内的加减法（这部分的结果是背下来的）；
  - 用字符串来保存十进制的数字；
  - 有额外的存储可以利用（保存进位时的信息）；
- **目标：**
  - 懂得如何利用已知的十以内的加减法的结果，进行多位的加减法运算；
  - 计算时需要从后向前计算；
  - 需要保留进位的信息；
  - 确保计算结果一定是准确的；

---
- 这应该是一个强化学习的过程——只有计算正确时，才会获得奖励。其中，尝试探索的动作空间是：
  - 如何利用十以内的加减
  - 通过怎样的计算顺序
  - 如何利用额外的存储
- 最终以此来生成一个完整的加法运算流程。

---
### 使用 RL 训练 Agent
-----

Agent 的 While 循环模式恰好符合 Bellman 方程的形式。

---

&amp;gt; 通过强化学习训练 Agent，直观上，状态空间和奖励都不难定义，难的是究竟如何定义 Action 空间。

---
- 例如十以内的加减法为什么可以成为加法的 Action？
- 进一步，当我们尝试训练乘法时，九九乘法表一定在我们的 Action 空间中，那如何凭空让机器找到这个“九九乘法表”呢？
- 甚至计算乘法时，加法也需要在 Action 空间中。这意味着，强化学习能学会内容是有序的，而且 Action 空间的生成也是依赖之前的训练结果的。
- 这个结果并不意外，《技术的本质》一书中就提到过人类技术的发展是渐进的，而不是突变的。

---

&amp;gt; 牛顿无法简单的通过阅读代数和几何来发明微积分。问题是，要生成全新的想法，还缺什么？

**有可能达成 AGI 的新范式应当需要 Agent &amp;#43; LLM 的联合训练，而不是各自的单独训练。**

---

## Thanks
      &lt;/textarea&gt;
    &lt;/section&gt;
  &lt;/div&gt;
&lt;/div&gt;</description>
      <content:encoded><![CDATA[<div class="reveal">
  <div class="slides">
    <section data-markdown>
      <textarea data-template>
      
## 大语言模型的计算能力

---
### LLM 的几个核心数学问题
-----
1. N-GRAM 的计算能力问题
2. 过参数化模型的统计学习问题
3. 非凸的数值优化问题
4. 对深度神经网络的数学理解
5. Transformer 算子的含义
6. fine-tuning 的数学含义

---
### N-GRAM 的计算能力

---

- **大语言模型的基本范式：**
- 假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：

$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$

- 该模型是一个 $N-1$ 阶的马尔可夫链，称为 `N-GRAM` 模型。
- 该模型的计算能力是有限的，因为它的是个有穷自动机。

---

- `非确定型有穷自动机`(`NFA`) 是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中
  - $Q$ 是一个有穷集合，称为**状态集**。
  - $\Sigma$ 是一个有穷集合，称为**字母表**。
  - $\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是**转移函数**。
  - $q_0\in Q$ 是**起始状态**。
  - $F \subseteq Q$ 是**接受状态集**。

---
#### 大模型是有穷自动机的证明过程
-----
- 令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为

$$
\phi:\{s_0s_1s_2...s_{n-1}\}\rightarrow s_n,s_i \in \Sigma
$$

- 取 $\delta$ 为：

$$
\begin{equation}
\delta(q,\sigma) = 
\left \\\{
\begin{array}{ll}
q \circ\sigma & \sigma \neq \varepsilon\\\\
q \circ\phi(q) & \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$

- 也就是将 $Q$ 设置为已经拥有的上文，持续输入或预测下一个字符。

---

- 在 [Bhattamishra et al. 2020](https://arxiv.org/abs/2009.11264) 中也有类似的实验，实验中基于 Transformer 的大语言模型甚至只能识别弱化的正则语言。
  - 大模型无法判别一个 $\{[0|1]^*\}$ 序列中是否有奇数个 $1$。
  - 给定 $n$ 大模型无法生成 $(aa)^n$。
- 在 [Zhou et al. 2023](http://arxiv.org/abs/2310.16028) 中还尝试论述了基于 Transformer 的大语言模型也无法实现加法运算之类的复杂运算。
- 以及，有穷自动机无法判定 $\\\{0^n\\\#1^n\\\}$ 形式的序列；也无法进行基础的四则运算（无法处理括号的闭合和乘法的优先顺序）。

---
### 大模型计算能力是有限的
-----
- 大模型不具备推理能力，这件事与模型的规模无关。
- 大模型是通过语言概率分布的泛化来模拟出推理能力的假象。
  - 例如：大模型不会数数。他只是把看到过的数数类型的答案都记住了，然后用类比的方式去回答新的问题；如果答案在记忆中，就会回答正确。
  - 更大的模型可以记住更多的答案，但是这并不是我们想要的答案。
- 我们需要新的范式来解决 AGI 问题。

---
### Agent &#43; LLM 可以成为完备图灵机
---
- Agent 的基本范式是一个 While 程序。
- 例如下面几种常见的 Agent：
  1. [ReAct](http://arxiv.org/abs/2210.03629) 获得反思推理能力
  2. [BabyAGI](https://github.com/yoheinakajima/babyagi) 基础的计划任务 Agent
  3. [Reflexion](http://arxiv.org/abs/2303.11366) 长期记忆和短期记忆（短期记忆就符合上述流程）
  4. [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) 第一个全能 Agent

---
- 代码示例：

```python
@dataclass
class ReAct(ABC):
    thought: Optional[str] = None
    action: Optional[str] = None

    def __post_init__(self):
        self.obs = self.run() # 获取执行 Action 的结果

act = ReAct()
acts = []
while not act.done:
    acts.append(act)
    prompt = get_prompt(acts)
    act: ReAct = ReAct.parse(llm.call(prompt)) # 调用大模型
```

---

#### 编程语言 WHILE 语义 (Semnatik)
-----
- 一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$
- 其他未定义的参数可以在程序里被调用,但是必须设定为 $0$
- WHILE 程序的结果在结束结束后通过 $x_0$ 传达
- 对于一个 WHILE 程序,有三种运行模式:
  - 变量赋值: $x_i=x_j&#43;c,c\in\{0,1,−1\}$
  - $P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$
  - WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0
---
- **定理：编程语言 WHILE 是图灵完备的**
- **证明:** 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。
- 证明细节参看：[while循环](https://zhuanlan.zhihu.com/p/343107128) ，源自 [Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen](https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf)

---
- 实现图灵完备的方法不是唯一的。例如，[Schuurmans 2023](http://arxiv.org/abs/2301.04589) 就证明了，当可以使用外部存储时，大模型是图灵完备的。
- 但是，这并不意味着 Agent &#43; LLM 就是 AGI，毕竟我们现在的电脑本身就是图灵完备的，但是我们并不认为电脑就是 AGI。
- 我们需要的是一个可以自适应的 Agent，而不是一个固定的 Agent。也就是可以通过学习来改变自己的行为，而不是通过人工的方式来改变自己的行为。

---
### 以整数加法为例
-----

&gt; 我们希望 Agent 学会整数加法。但学会的方式不是我们给了它加法的程序让它执行，而是通过加法的真值，“推导”出加法的程序。模拟人做加法的方式，我们尝试让机器训练出加法程序

---
- **已知**：
  - 十以内的加减法（这部分的结果是背下来的）；
  - 用字符串来保存十进制的数字；
  - 有额外的存储可以利用（保存进位时的信息）；
- **目标：**
  - 懂得如何利用已知的十以内的加减法的结果，进行多位的加减法运算；
  - 计算时需要从后向前计算；
  - 需要保留进位的信息；
  - 确保计算结果一定是准确的；

---
- 这应该是一个强化学习的过程——只有计算正确时，才会获得奖励。其中，尝试探索的动作空间是：
  - 如何利用十以内的加减
  - 通过怎样的计算顺序
  - 如何利用额外的存储
- 最终以此来生成一个完整的加法运算流程。

---
### 使用 RL 训练 Agent
-----

Agent 的 While 循环模式恰好符合 Bellman 方程的形式。

---

&gt; 通过强化学习训练 Agent，直观上，状态空间和奖励都不难定义，难的是究竟如何定义 Action 空间。

---
- 例如十以内的加减法为什么可以成为加法的 Action？
- 进一步，当我们尝试训练乘法时，九九乘法表一定在我们的 Action 空间中，那如何凭空让机器找到这个“九九乘法表”呢？
- 甚至计算乘法时，加法也需要在 Action 空间中。这意味着，强化学习能学会内容是有序的，而且 Action 空间的生成也是依赖之前的训练结果的。
- 这个结果并不意外，《技术的本质》一书中就提到过人类技术的发展是渐进的，而不是突变的。

---

&gt; 牛顿无法简单的通过阅读代数和几何来发明微积分。问题是，要生成全新的想法，还缺什么？

**有可能达成 AGI 的新范式应当需要 Agent &#43; LLM 的联合训练，而不是各自的单独训练。**

---

## Thanks
      </textarea>
    </section>
  </div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>大语言模型原理分享</title>
      <link>https://blog.uglyboy.cn/slides/2023/11/21/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E5%88%86%E4%BA%AB/</link>
      <pubDate>Tue, 21 Nov 2023 01:45:37 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/slides/2023/11/21/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8E%9F%E7%90%86%E5%88%86%E4%BA%AB/</guid>
      <description>&lt;div class=&#34;reveal&#34;&gt;
  &lt;div class=&#34;slides&#34;&gt;
    &lt;section data-markdown&gt;
      &lt;textarea data-template&gt;
      
# 大语言模型&amp;lt;br&amp;gt;原理分享
---

## 什么是大语言模型？
---

当我说了很多话之后，我马上要说 $\Box$

---

## 数学公式描述
-----
$w_1, w_2,\dots, w_{N}$ 是一个单词序列，这个序列的概率分布是：

$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$

---

## 大语言模型能做什么？

---

- 大模型能记住它看到过的一切信息。
- 大模型对于已经看到过的信息，有一定的泛化能力（有限度的推广）。

---

### 大模型能达到怎样的泛化能力？

&amp;gt; 大模型可以涌现出智能吗？

---

## 大语言模型不能做什么？

---
1. 大模型无法判别一个 $\\\{[0|1]^*\\\}$ 序列中是否有奇数个 $1$。
2. 给定 $n$ 大模型无法生成 $(aa)^n$。
3. 大模型无法判定 $\\\{0^n\\\#1^n\\\}$ 形式的序列。
4. 大模型无法执行加法运算。
5. $\dots$

---

大语言模型没有，也不可能具有推理能力。

&amp;gt; 大语言模型只是记住了足够多的别人的推理，然后用类比的方法将这些推理泛化了而已。

---

## 大语言模型是如何将信息泛化的？

---

- 通过相似度计算来进行泛化，然后通过概率分布来进行选择。
    1. 粗略的可以如下理解：可以用同义词替代的都能被泛化。
    2. 这种泛化的替代能力是可以保留相对位置信息的（例如一道数学题中的数字变了，它可以泛化到后续的解题过程中，都用新数字替代原来的数字）。
    3. 在训练样本充分的情况下，可以跨语言进行同义词泛化。

---

# Q &amp; A
      &lt;/textarea&gt;
    &lt;/section&gt;
  &lt;/div&gt;
&lt;/div&gt;</description>
      <content:encoded><![CDATA[<div class="reveal">
  <div class="slides">
    <section data-markdown>
      <textarea data-template>
      
# 大语言模型&lt;br&gt;原理分享
---

## 什么是大语言模型？
---

当我说了很多话之后，我马上要说 $\Box$

---

## 数学公式描述
-----
$w_1, w_2,\dots, w_{N}$ 是一个单词序列，这个序列的概率分布是：

$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$

---

## 大语言模型能做什么？

---

- 大模型能记住它看到过的一切信息。
- 大模型对于已经看到过的信息，有一定的泛化能力（有限度的推广）。

---

### 大模型能达到怎样的泛化能力？

&gt; 大模型可以涌现出智能吗？

---

## 大语言模型不能做什么？

---
1. 大模型无法判别一个 $\\\{[0|1]^*\\\}$ 序列中是否有奇数个 $1$。
2. 给定 $n$ 大模型无法生成 $(aa)^n$。
3. 大模型无法判定 $\\\{0^n\\\#1^n\\\}$ 形式的序列。
4. 大模型无法执行加法运算。
5. $\dots$

---

大语言模型没有，也不可能具有推理能力。

&gt; 大语言模型只是记住了足够多的别人的推理，然后用类比的方法将这些推理泛化了而已。

---

## 大语言模型是如何将信息泛化的？

---

- 通过相似度计算来进行泛化，然后通过概率分布来进行选择。
    1. 粗略的可以如下理解：可以用同义词替代的都能被泛化。
    2. 这种泛化的替代能力是可以保留相对位置信息的（例如一道数学题中的数字变了，它可以泛化到后续的解题过程中，都用新数字替代原来的数字）。
    3. 在训练样本充分的情况下，可以跨语言进行同义词泛化。

---

# Q & A
      </textarea>
    </section>
  </div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>大语言模型的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2023/11/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3/</link>
      <pubDate>Thu, 09 Nov 2023 12:08:22 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/11/09/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3/</guid>
      <description>&lt;h2 id=&#34;大语言模型的基本逻辑&#34;&gt;大语言模型的基本逻辑&lt;/h2&gt;
&lt;p&gt;大语言模型的本质是一个 &lt;code&gt;N-GRAM&lt;/code&gt; 模型，即：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：&lt;/p&gt;
&lt;p&gt;$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$&lt;/p&gt;
&lt;p&gt;该模型是一个 $N-1$ 阶的马尔可夫链，称为 &lt;code&gt;N-GRAM&lt;/code&gt; 模型&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="大语言模型的基本逻辑">大语言模型的基本逻辑</h2>
<p>大语言模型的本质是一个 <code>N-GRAM</code> 模型，即：</p>
<p><strong>定义：</strong></p>
<p>假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：</p>
<p>$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$</p>
<p>该模型是一个 $N-1$ 阶的马尔可夫链，称为 <code>N-GRAM</code> 模型</p>
<p><strong>推论：</strong> 有限马尔可夫链（或 <code>N-GRAM</code> 模型）背后的「语法」是有穷自动机，也就是正则表达式。是 <code>乔姆斯基体系</code> 最底级的文法。</p>
<h3 id="agent--llm-可以成为完备图灵机">Agent + LLM 可以成为完备图灵机</h3>
<p>一般来说，希望将有穷自动机扩充成完备图灵机，朴素的想法就是添加外部存储，如 <a href="http://arxiv.org/abs/2301.04589">Schuurmans et al(2023)</a> 就证明了使用外部存储的大模型是图灵完备的。但这种图灵完备性的实现依然需要大量的人工介入。所以我们希望找到一种更加自然的，可以自我学习的具有图灵完备性的模式。</p>
<h4 id="while-循环的图灵完备性">While 循环的图灵完备性</h4>
<p>编程语言 WHILE 语义 (Semnatik):</p>
<ul>
<li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li>
<li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li>
<li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li>
<li>对于一个 WHILE 程序,有三种运行模式:
<ul>
<li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li>
<li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li>
<li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li>
</ul>
</li>
</ul>
<p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p>
<p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD, STORE, CLOAD, CADD, CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p>
<h4 id="agent-流程都是-while-循环">Agent 流程都是 While 循环</h4>
<p>典型的几个 Agent 流程：</p>
<ol>
<li><a href="http://arxiv.org/abs/2210.03629">ReAct</a> 获得反思推理能力</li>
<li><a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> 基础的计划任务 Agent</li>
<li><a href="http://arxiv.org/abs/2303.11366">Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> 第一个全能 Agent</li>
</ol>
<p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p>
<blockquote>
<p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p>
</blockquote>
<h2 id="大语言模型的泛化性">大语言模型的泛化性</h2>
<p>从机器学习的角度看，大语言模型是一个生成式模型——学习原始数据的概率分布。这里有一个基础的问题：用哪种机器学习的方法来学习这个生成式模型。</p>
<p>我们也逐步来分析这个问题，首先是传统统计学习的学习方法和深度神经网络之间的选择。从结果上看，我们选择了深度神经网络，因为我们不可能见过所有人说过的所有话，所以我们希望我们训练的模型在我们未见过的样本上也能取得很好的效果，这就是<strong>模型的泛化能力</strong>。而实验表明，深度神经网络的泛化性更好。为什么呢？</p>
<h3 id="过参数化是泛化性好的本质原因">过参数化是泛化性好的本质原因</h3>
<p>在传统统计学习中，我们希望使用的参数尽可能的少（奥卡姆剃刀原理），这样才能带来更好的泛化效果。另一方面，我们又希望我们的模型的表达能力尽可能的强，这样才能更好的拟合真实的概率空间。所以会有经典的微笑曲线：</p>
<p><img loading="lazy" src="https://s2.loli.net/2023/11/11/X9Q8xVuroRI3fzU.png" alt=""  />
</p>
<p>假设空间的大小不能太小，也不能太大（否则会过拟合）。</p>
<p>但当时的人们都没有尝试一件事情，就是如果进一步增大参数空间（已经发生过拟合之后），会发生什么？</p>
<p>下图是实际发生的事情：</p>
<p><img loading="lazy" src="https://s2.loli.net/2023/10/21/hIYNgQoBmRy32PH.png" alt=""  />
</p>
<p>随着参数空间的继续增大，泛化性又逐步的提升了，而且比过拟合之前的最优值还要好了。</p>
<p>这件事是深度学习拥有良好泛化性的本质原因——<strong>过参数化</strong>。如 <a href="http://arxiv.org/abs/1802.01396">Belkin et al(2018)</a> 中描述的，其实这种能力也并不是深度神经网络所独有的，而是一切过参数化的机器学习方法都能具备的性质。</p>
<p>深度神经网络一方面可以通过网络结构学习任意形状的可积函数的分布，另一方面，又可以通过过参数化获得良好的泛化性，于是就成为了真实世界大部分问题的最优机器学习方法——我们可以从猜测真实问题的函数结构中解脱出来，也不用担心样本量少无法遍历全部解空间。</p>
<h3 id="过参数化带来的思考">过参数化带来的思考</h3>
<p>过参数化的机器学习过程有无穷多最优解（训练数据上 Loss 为零），所以一定是一个非凸优化问题。但是不同的解对应的泛化性是不同的。而至今为止，我们也没有一个关于解的泛化性的指导性优化理论。所以深度学习能否获得良好的泛化性是一个随机事件。</p>
<p>但另一方面，从实践的角度我们能得到，深度学习获得良好泛化性又是一个大概率的事件。</p>
<p>结合深度学习中已经获得的大量实验结果，我们可以形成这样的物理认知：泛化性好的解空间应该是空间范围比较大（或者是梯度变化更平缓）的区域；而泛化性不好的空间则反之。从而自然会有结论：解落入更大空间的概率会更大，所以解能大概率是泛化性好的。</p>
<p>而基于上面这个未被证实认知也会带来一些推论：</p>
<ul>
<li><strong>收敛速度快的算法，可能其泛化性不如收敛速度慢的算法</strong>；</li>
<li><strong>增加收敛时的随机扰动可以提升泛化性</strong>；</li>
</ul>
<p>这些结论与已知的实验结果是相符的：Adam 收敛速度好于 SGD，但泛化性很多时候不如 SGD；而 SGD 的泛化性好于 GD。</p>
<p>以及，当下的一些研究，例如尝试将已经训练好的模型中的部分参数扣掉——“因为这些参数的变化不会影响训练集上的 Loss，或者我们已知的测试集上的 Loss”……这些尝试是危险的，很可能损失掉良好的泛化性。</p>
<p>过参数化的泛化性问题，现在还没有很好的数学解释，从而也没有合适的理论来衡量一个解的泛化性效果。一段时间之内，这个问题都会是大模型的“阿喀琉斯之踵”，考验大部分的深度学习优化算法——<strong>当你带来计算效率的提升时，是不是能确保泛化性不下降</strong>？</p>
<h2 id="大语言模型的-transformer-算子">大语言模型的 Transformer 算子</h2>
<p>当我们确定了使用 <code>N-GRAM</code> 作为语言模型，以及利用深度神经网络作为机器学习的方法，以获得模型良好的泛化能力。下一步就需要进一步研究模型更细节的结构上是否为大语言模型带来的新的能力，亦或者是限制了什么能力。</p>
<p>这里首先引入一个结论：</p>
<p>当前所有的深度学习中的算子，都可以展开成全链接网络。也就是说，当前的各种深度学习的算子，并不能获得全链接网络获得不了的能力。所以如果是作为基础能力的研究，例如“网络层深是如何带来更强的表达能力的”这种研究课题，是可以将任意算子都抽象成全链接网络来进行探索。这也是 <code>NTK</code> 理论的重要价值。</p>
<p>于是，各种具体算子带来的好处，是在于使用时效率的提升。这种提升等价于——给网络带来良好的先验知识。所以深度学习中的算子不存在优劣之分，只有不同的算子对于不同的数据，先验知识的匹配程度的差别。</p>
<p>所以下面我们即将讨论的 Transformer 算子，研究的重点是它带来了哪些先验知识（或者可以说它舍弃了哪些信息，而只关注哪些知识）。</p>
<h3 id="transformer-算子的位置编码">Transformer 算子的位置编码</h3>
<p><code>N-GRAM</code> 模型是时不变的，具体来说，就是一句话的分布，不会因为它前后位置的小变化而改变。例如一个文章中一句话前面多打了一两个空格，并不会影响将要说的这句话。</p>
<p>更具体来说，就是 <code>N-GRAM</code> 中的信息只与相对位置信息有关，而与绝对位置信息无关。基于这个信息，就可以优化全链接网络，设计出算子结构，使得其只与相对位置信息有关，而与绝对位置信息无关。</p>
<p>放到 Transformer 算子中来说，就是位置编码的设计应该满足：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,m−n)
$$</p>
<p>只与 $m-n$ 有关，而跟 $m,n$ 的具体数值无关。从这条性质就能比较容易地得到 <code>RoPE</code> 旋转位置编码。</p>
<h4 id="位置编码的内差">位置编码的内差</h4>
<p>大模型当前研究的重点之一是上下文窗口的大小，我们希望这个窗口可以进一步的扩大以捕获更多的输入信息。</p>
<p>但因为训练数据有限，以及模型本身需要有一个明确的形状，所以训练时的数据基本上还是要维持差不多在 4k 的水平上，但希望能对更长的文本进行预测。这时，从位置编码的性质来看，是与上下文窗口的大小无关的，所以是可以合理外推到无限大的。但是受限于训练样本的数量，当上下文窗口更大时，基本上还是只能有效捕获到训练窗口大小的信息，对更多的信息是无法利用的。</p>
<p>这时自然的想法时，如果我对信息内差（将更长的文本挤成短文本窗口大小的样子），就可以利用已经训练的信息来推测更多的信息了。</p>
<p>可以理解成，将位置编码设计成：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,\frac{m−n}{s})
$$</p>
<p>其中，$s$ 是窗口长度。这表达的是位置编码与相对位置的绝对大小无关，而只与相对位置的相对大小有关。</p>
<p>但是这样的问题是，在更常用的场景下，相对位置的绝对大小是更重要的，例如比较短的句子中，两个 token 究竟是相隔几个位置是十分重要的。这意味着，无法直接使用这样的位置编码获得任意的窗口能力。</p>
<p>所以，当下流行的位置编码内差的方法是：</p>
<ol>
<li>通过 <code>RoPE</code> 算法训练一个 $s$ 长的窗口</li>
<li>然后再用内差的办法，重新扩张了窗口大小，此时低频（长文本部分）通过内差获得了还不错的训练性能。但高频（短文本）部分却被严重破坏了。</li>
<li>此时重新微调模型，将高频部分调整到合适的位置，可以理解成只需要训练高频部分的信息（这部分信息其实也已经有了一些合理的先验知识了），所以可以更快的将短窗口扩展到长窗口。</li>
</ol>
<p><a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-aware Scaled RoPE</a> 方法，使用的是高频内推，低频外差的办法，做到了不需要额外训练即可扩大上下文窗口，即：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,l(m−n))
$$</p>
<p>其中：</p>
<p>$$
\begin{equation}
l(m-n) \approx
\left \{
\begin{array}{ll}
m-n &amp; \text{当}(m-n)\text{较小时} \\
\frac{m-n}{s} &amp; \text{当}(m-n)\text{较大时}
\end{array}
\right.
\end{equation}
$$</p>
<p>但这个方法还是会在很多位置上失去原本训练时学到的信息，使得外推时有性能损失。</p>
<p>类似的，其实我们还可以用这样的思考方式，重新设计位置编码，使得模型可以更好的利用现有的训练数据获得合理的外推能力。例如 <code>ReRoPE</code> 算法的位置编码的设计是这样的：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,l(m−n))
$$</p>
<p>其中：</p>
<p>$$
\begin{equation}
l(m-n) =
\left \{
\begin{array}{ll}
m-n &amp; m-n &lt; s\\
s &amp; m-n \geq s
\end{array}
\right.
\end{equation}
$$</p>
<p>这是因为，训练样本中，我们从未见过 $m-n&gt;s$ 的样本，所以更长程的样本都用 $s$ 来替代，近似的获得信息的利用。类似这样的编码设计，可以保证训练集上无性能损失，并且具备了一定的扩展能力，最大化的利用了训练信息。</p>
<p>基于这个思想，还可以扩展出很多的位置编码的设计，尽最大可能性来挖掘训练样本中的信息。</p>
<h4 id="位置编码是否需要时间衰减">位置编码是否需要时间衰减？</h4>
<p>包括 <code>RoPE</code> 在内的各种位置编码，都增加了时间衰减的先验。而这部分信息其实是可以通过训练来学习到的。所以是否真的需要时间衰减这个先验信息，它是否能更有效的帮助我们训练？是一个值得研究和思考的问题。</p>
<h3 id="transformer-算子的信息编码">Transformer 算子的信息编码</h3>
<p>类似于上面位置编码的分析，我们知道分析算子的核心，是考虑它保留了什么信息（或者说舍弃了什么信息，是否有不应当舍弃的信息被舍弃了）。</p>
<blockquote>
<p>Transformer 信息编码的设计表达的是：某个位置所蕴含的信息，只与这个位置之前的所有文本间两两的相似度信息有关。</p>
</blockquote>
<p>其中极为重要的信息是如下公式：</p>
<p>$$
Attention(Query,Source) = \sum_{i=1}Similarity(Query,Key_i)*Value_i
$$</p>
<p>于是也可以将 Attention 机制看作一种软寻址（Soft Addressing）。</p>
<p>至于注意力模型中是否丢失了什么重要的信息？是否有更加合理的选择？是进一步分析 Transformer 算子的核心。但这一部分同样也没有什么更加基础的数学依据，所以就没有什么进一步讨论的余地了。</p>
<p>稍值得留意的是，具体的 $Similarity$ 算法的选取，还是可以从一切其他不变量中获得部分更加有意义的约束的。例如，<a href="https://spaces.ac.cn/archives/8823">从熵不变性看Attention的Scale操作</a>，还是可以从提升上下文窗口外推能力的角度，获得一个更有效的系数项。</p>
<h2 id="大语言模型的对齐">大语言模型的对齐</h2>
<p>这部分其实在数学上值得分析的内容不多，因为对齐的操作本质上是一个偏应用的操作，是让预训练模型更加符合人类的使用场景的操作。所以对齐之后，模型能力层面是没有本质提升的，更多的是在方便人类使用的层面获得了提升。这部分从应用和工程角度是需要而且极为重要的，但没有额外的数学信息。</p>
<p>其中只有一个话题值得探索，即为什么对齐的操作选择了强化学习而不是继续用传统的模式识别的方法训练？</p>
<blockquote>
<p>坊间的笑谈是，当时 OpenAI 负责对齐的团队恰好手边有现成的 RL 的算法，所以就用它搞出了 RLHF。</p>
</blockquote>
<p>网上关于这个问题有一些解释，大体上就是表达 RL 的调整效率是高于传统的模式识别的。这部分内容我还没有仔细的研究，就先不胡扯了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>证明细节请看：·<a href="https://zhuanlan.zhihu.com/p/343107128">while循环</a> ，源自 <a href="https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf">Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>大语言模型的计算能力</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/30/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/</link>
      <pubDate>Mon, 30 Oct 2023 07:50:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/30/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B/</guid>
      <description>&lt;h2 id=&#34;大模型是有穷自动机&#34;&gt;大模型是有穷自动机&lt;/h2&gt;
&lt;h3 id=&#34;非确定型有穷自动机nfa的定义&#34;&gt;非确定型有穷自动机（NFA）的定义&lt;/h3&gt;
&lt;p&gt;非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Q$ 是一个有穷集合，称为&lt;strong&gt;状态集&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$\Sigma$ 是一个有穷集合，称为&lt;strong&gt;字母表&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是&lt;strong&gt;转移函数&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$q_0\in Q$ 是&lt;strong&gt;起始状态&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$F \subseteq Q$ 是&lt;strong&gt;接受状态集&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;大模型是-nfa-的证明&#34;&gt;大模型是 NFA 的证明&lt;/h3&gt;
&lt;p&gt;令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="大模型是有穷自动机">大模型是有穷自动机</h2>
<h3 id="非确定型有穷自动机nfa的定义">非确定型有穷自动机（NFA）的定义</h3>
<p>非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中</p>
<ol>
<li>$Q$ 是一个有穷集合，称为<strong>状态集</strong>。</li>
<li>$\Sigma$ 是一个有穷集合，称为<strong>字母表</strong>。</li>
<li>$\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是<strong>转移函数</strong>。</li>
<li>$q_0\in Q$ 是<strong>起始状态</strong>。</li>
<li>$F \subseteq Q$ 是<strong>接受状态集</strong>。</li>
</ol>
<h3 id="大模型是-nfa-的证明">大模型是 NFA 的证明</h3>
<p>令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为</p>
<p>$$
\phi:{s_0s_1s_2&hellip;s_{n-1}}\rightarrow s_n,s_i \in \Sigma
$$</p>
<p>取 $\delta$ 为：</p>
<p>$$
\begin{equation}
\delta(q,\sigma) = \left \{
\begin{array}{ll}
q \circ\sigma &amp; \sigma \neq \varepsilon \\
q\circ\phi(q) &amp; \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$</p>
<p>也就是将 $Q$ 设置为已经拥有的上文，连续预测下一个字符（若当前是输入过程，则只需要简单的叠加到状态集，不需要预测的过程）。这描述了大语言模型下的“<em>Next Token Prediction</em>” 范式。也就是说这个范式下的一切模型（无论是 Transformer 还是 其他的什么算子进行这种模式的预测），都跳不出这个基本的范式。</p>
<p>即当前的大模型无论如何提升自己的能力，其计算能力也不过是一个有穷自动机。</p>
<blockquote>
<p>也就是说，类似于 $\{0^n\#1^n\}$ 这个模式是无法被有穷自动机学习和预测出来的。换句话说，大模型的智能在这个例子上直接会被锁死，注定达不成所谓的**“AGI”**。</p>
<p>以这个例子泛化来说，我们仅通过构造正负样本和机器学习做概率预测的方式，永远也无法对上面的模式做完美的判定。这个结论正是上面的推理想表达的意思。</p>
<p>这件事可以拿 ChatGPT 来测试，对于 <code>0#1</code>，<code>00#11</code>，<code>000#111</code>，&hellip;，这个序列，让 ChatGPT 续写，它可以继续写下去且不出错（但这只是假象），而且也会明确的说出这个序列是 $\{0^n\#1^n\}$ 这个模式的产物。但当你要求它输出 n=100 时的输出，或者你拿 n=100 时的输入让 ChatGPT 判定时，它就会出错了。</p>
</blockquote>
<p>直接得到的重要启示是：</p>
<p>除了大模型，我们还需要新的范式来解决 <strong>AGI</strong> 问题。<strong>仅靠提升模型规模，注定有很多事情做不到</strong>。</p>
<h3 id="额外的说明">额外的说明</h3>
<p>有穷自动机是做不出基础四则运算的计算器 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 的。这也意味着大模型的推理能力是不存在的。</p>
<p>我们认为的推理能力，不过是在有限状态空间下的穷举，例如上文中的 $\{0^n\#1^n\}$ 这个例子。更大的模型可以通过训练模拟出更长的匹配，但是从“<strong>压缩比</strong>”的角度看，终究是没有能掌握这个规律，而是通过空间换时间的方式将更多的答案在训练的过程中记住。</p>
<p>所以可能又回到了最初的问题——大模型是不是必须要足够大？继续增加大模型的规模还可以进一步提升泛化性，在类似这样的原本有穷自动机解决不了的问题上缓存更多的答案，“<strong>假装</strong>”大模型是可以解决它的。但这不是我们想要的答案。</p>
<h2 id="agent--llm-可以成为完备图灵机">Agent + LLM 可以成为完备图灵机</h2>
<h3 id="while-循环的图灵完备性">While 循环的图灵完备性</h3>
<p>编程语言 WHILE 语义 (Semnatik):</p>
<ul>
<li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li>
<li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li>
<li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li>
<li>对于一个 WHILE 程序,有三种运行模式:
<ul>
<li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li>
<li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li>
<li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li>
</ul>
</li>
</ul>
<p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p>
<p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p>
<blockquote>
<p>需要注意的是，循环中 ${x_i}$ 的个数对其是否是图灵完备的有影响。具体来说，<strong>任意图灵机可以被拥有 $8$ 个变量的 WHILE 程序模拟计算</strong>。</p>
<p>这里的大部分变量其实是用来操控 RAM 或者用来操控图灵机的。真实使用时，不需要这么多的掣肘。</p>
</blockquote>
<h3 id="agent-的基本范式">Agent 的基本范式</h3>
<p>Agent 的基本范式恰好就是一个 While 程序，其 <code>Python</code> 描述如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ReAct</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">thought</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">()</span> <span class="c1"># 获取执行 Action 的结果</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        执行Action
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@classmethod</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&#34;ReAct&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        从大模型返回的文本解析成 ReAct 的实例
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">done</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        终止条件
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        从 ReAct 中抽取信息形成新的 Prompt
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">act</span> <span class="o">=</span> <span class="n">ReAct</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">acts</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">while</span> <span class="ow">not</span> <span class="n">act</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">acts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span> <span class="o">=</span> <span class="n">get_prompt</span><span class="p">(</span><span class="n">acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">act</span><span class="p">:</span> <span class="n">ReAct</span> <span class="o">=</span> <span class="n">ReAct</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">llm</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span> <span class="c1"># 调用大模型，并将 response 解析成 ReAct 的实例</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其中，存储和变量有两种选择：可以保存在函数 <code>get_prompt</code> 中（这意味着更多的人工控制设定），也可以保存在 <code>ReAct</code> 中（这意味着让大模型在上下文中自行决定保存哪些信息）。</p>
<p>所以，<strong>Agent 的基本范式是图灵完备的</strong>。</p>
<p>典型的几个 Agent 流程：</p>
<ol>
<li><a href="http://arxiv.org/abs/2210.03629">ReAct</a> 获得反思推理能力</li>
<li><a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> 基础的计划任务 Agent</li>
<li><a href="http://arxiv.org/abs/2303.11366">Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> 第一个全能 Agent</li>
</ol>
<p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p>
<blockquote>
<p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p>
</blockquote>
<h3 id="突破的方向可训练的-agent">突破的方向——可训练的 Agent</h3>
<p>如果想获得更强的计算能力，需要提升的不仅仅是 LLM，而是结合了 Agent 后的整体系统。所以微调（fine tuning）和对齐（Alignment）更应该在整合了一个可学习的 Agent 之后进行。</p>
<p>另外，基础的预训练模型或许并不需要特别的大（当然，越大性能越好的结论不变，但与其记更多的数据不如记更多的规律），而需要把更多的训练工作后置到集成了 Agent 之后进行，这样才有可能将有穷自动机无法识别的模式学习出来。</p>
<blockquote>
<p>Agent 的 While 程序模式，其实也恰好符合一个强化学习的学习过程，这里确实是可以做很多工作的。</p>
</blockquote>
<h2 id="这是通往-agi-之路吗">这是通往 AGI 之路吗</h2>
<p>到今天为止，其实我们也没有一个关于智能的合理定义。</p>
<blockquote>
<p>学会了人的技能就算是智能了吗？会不会千百万年后的未来人回头看，会觉得人类太傻，并不具有智能呢？所以大模型学习人这件事是不是就是最好的选择？</p>
</blockquote>
<p>但至少今天人能够完成的一切，都没有可以超出图灵机范式的计算能力，所以图灵机的计算能力可以当作今天人类的极限。</p>
<p>AGI 可以定义为:</p>
<blockquote>
<p>无需人类的介入，实现任意的图灵机能力。</p>
</blockquote>
<p>如果以这个定义来看，那么当下的 Agent + LLM 在理论上已经可以到达人类能够触达的一切天空了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>有穷计算机无法模拟括号的匹配和乘除法的运算优先级。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>证明细节请看：·<a href="https://zhuanlan.zhihu.com/p/343107128">while循环</a>，源自 <a href="https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf">Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>所谓“压缩即是智慧”毫无意义</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/25/%E6%89%80%E8%B0%93%E5%8E%8B%E7%BC%A9%E5%8D%B3%E6%98%AF%E6%99%BA%E6%85%A7%E6%AF%AB%E6%97%A0%E6%84%8F%E4%B9%89/</link>
      <pubDate>Wed, 25 Oct 2023 10:57:47 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/25/%E6%89%80%E8%B0%93%E5%8E%8B%E7%BC%A9%E5%8D%B3%E6%98%AF%E6%99%BA%E6%85%A7%E6%AF%AB%E6%97%A0%E6%84%8F%E4%B9%89/</guid>
      <description>&lt;h2 id=&#34;算数编码才是压缩的本质&#34;&gt;算数编码才是压缩的本质&lt;/h2&gt;
&lt;p&gt;一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 &lt;a href=&#34;https://www.youtube.com/watch?v=dO4TPJkeaaU&amp;amp;t=247s&#34;&gt;Compression for AGI - Jack Rae | Stanford MLSys #76&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;里面核心模式只有一个：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;假定我有一个程序 f，我将 f 的代码传输给另一端；&lt;/li&gt;
&lt;li&gt;我有一个序列需要传输，我通过 f 对逐个字符出现的概率进行了预测；&lt;/li&gt;
&lt;li&gt;我根据算数编码，将结果编码后，传输给了另一端；&lt;/li&gt;
&lt;li&gt;最后传输的信息量最小。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;这不过是算数编码的定义好不好！！！&lt;/strong&gt; 哪里有什么神奇的地方。。。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="算数编码才是压缩的本质">算数编码才是压缩的本质</h2>
<p>一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 <a href="https://www.youtube.com/watch?v=dO4TPJkeaaU&amp;t=247s">Compression for AGI - Jack Rae | Stanford MLSys #76</a></p>
<p>里面核心模式只有一个：</p>
<blockquote>
<ol>
<li>假定我有一个程序 f，我将 f 的代码传输给另一端；</li>
<li>我有一个序列需要传输，我通过 f 对逐个字符出现的概率进行了预测；</li>
<li>我根据算数编码，将结果编码后，传输给了另一端；</li>
<li>最后传输的信息量最小。</li>
</ol>
</blockquote>
<p><strong>这不过是算数编码的定义好不好！！！</strong> 哪里有什么神奇的地方。。。</p>
<p>如果非说细节，也不过就是说明了为什么不用传输参数，将大模型的训练跟编码合到了一起而已。这完全证明不了大模型为什么有效果，以及为什么更大的模型效果更好。说出来的道理仅仅是：<strong>概率预测得越准，使用算数编码的压缩率越高</strong>。这件事结合算数编码的定义，不就是显然的问题吗？</p>
<p>而且它原始的流程中，也没有能体现出“<em>Next Token Prediction</em>”的优越性和必要性。</p>
<ol>
<li>如果序列很小，那么压缩效率的核心是 f 的代码量。此时使用 <code>lambda:x=x</code> 达到的效果最好。</li>
<li>如果序列很大，那么传参也不会是压缩算法优劣的核心差别。那么其他模式训练出来的能对文本做良好概率预测的模型都可以达到好的压缩效果。</li>
<li>如果序列中等，我们需要的是是否存在一个方法，一次传输了多个算数编码和多个残差，能否通过这些信息还原出初始编码？针对这个问题，我们单独开一章来分析</li>
</ol>
<h2 id="是否只能用-ntp-做压缩">是否只能用 NTP 做压缩？</h2>
<p>由自然归纳法，如果一次传输两个编码和两个残差，能还原出原始信息，那么，一次传输 $n$ 个算数编码和 $n$ 个残差就一定可以还原出原始编码。</p>
<p>假设我们使用的算法的过程是先用除第一个字符以外的所有字符来预测第一个字符的概率，同时梯度下降；然后再用除第二个字符以外的其他字符预测第二个字符的概率，同时梯度下降。这样可以得到两个算数编码和两个残差，应该如何用这些信息还原初始的字符呢？</p>
<p>方法和不确定型自动机的原理类似，或者用更土的办法来理解算法：</p>
<blockquote>
<p>我们用词表中的所有字符，重试这个过程，看哪个字符可以匹配上。虽然计算效率相比原版的 $\mathcal{O}(1)$，这个方法的复杂度是 $\mathcal{O}(n^2)$，但至少从压缩率的角度来看，我们对算法的要求没有计算速度方面的考量，更不用提这个算法一定是可以被优化的。</p>
</blockquote>
<p>以此推广，也就是对于任何模式的文本预测算法，都可以用同样的方法进行信息解压缩。于是不同方法之间在压缩率方面的差距还是会回归到对概率预测的精度上。甚至理论上看，使用了更多上下文的算法，应当可以比只做 &ldquo;Next Token Prediction&rdquo; 的算法精度更高。</p>
<h3 id="其他的无效解读">其他的无效解读</h3>
<p>至于残差究竟是不是用信息熵，其实对这个压缩算法没有什么核心的影响，无论哪种残差该反向传播依旧按原本的方式传播，无所谓其物理意义。因为所有的意义都只体现在传递的残差能否还原原来的编码。残差能对应上什么物理意义的各种解释其实对压缩率和计算都没有帮助。</p>
<h2 id="结论">结论</h2>
<p>所以那个演讲其实不过是个披着数学魔术的神奇表演，本质上不过是说：大模型谁的性能好，谁就是更好的大模型——典型的废话文学新版本了。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Scaling Law 的数学解读</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/10/scaling-law-%E7%9A%84%E6%95%B0%E5%AD%A6%E8%A7%A3%E8%AF%BB/</link>
      <pubDate>Tue, 10 Oct 2023 11:50:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/10/scaling-law-%E7%9A%84%E6%95%B0%E5%AD%A6%E8%A7%A3%E8%AF%BB/</guid>
      <description>&lt;h2 id=&#34;dataset-size-和-loss-的关系&#34;&gt;Dataset Size 和 Loss 的关系&lt;/h2&gt;
&lt;h3 id=&#34;最大似然估计mle&#34;&gt;最大似然估计（MLE）&lt;/h3&gt;
&lt;p&gt;一切机器学习的本质都是最大似然估计：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;模型下的理想真实世界的概率分布：$p(x|\theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\theta|x)$&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="dataset-size-和-loss-的关系">Dataset Size 和 Loss 的关系</h2>
<h3 id="最大似然估计mle">最大似然估计（MLE）</h3>
<p>一切机器学习的本质都是最大似然估计：</p>
<ol>
<li>
<p>模型下的理想真实世界的概率分布：$p(x|\theta)$</p>
</li>
<li>
<p>我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\theta|x)$</p>
</li>
<li>
<p>现在 $x$ 已知，$\theta$ 未知，若对于两个参数 $\theta_1$ 和 $\theta_2$ 有</p>
<p>$$
L(\theta_1|x) = p(x|\theta_1) &gt; p(x|\theta_2) = L(\theta_2|x)
$$</p>
<p>那么意味着 $\theta=\theta_1$ 时，随机变量 $\theta_1$ 生成 $x$ 的概率大于当参数 $\theta=\theta_2$ 时。这也正是似然的意义所在，若观测数据为 $x$，那么 $\theta_1$ 是比 $\theta_2$ 更有可能为分布函数的参数。</p>
</li>
<li>
<p>在给定观测数据集 $X={x_n},n \in \mathbb{N}$ 时，真实世界最有可能的概率分布对应的参数 $\hat\theta$ 应该满足：</p>
<p>$$
L(\hat\theta|x) = p(x|\hat\theta) &gt; p(x|\theta) = L(\theta|x), \theta \in \mathbb{\Theta} 且 \theta \ne \hat\theta
$$</p>
<p>即：</p>
<p>$$
\hat\theta = \arg\max\limits_\theta L(\theta|x)
$$</p>
</li>
<li>
<p>求解最大似然函数：</p>
<p>$$
\frac{\mathrm{d}}{\mathrm{d}\theta} L(\theta|x) = 0
$$</p>
</li>
</ol>
<p>对这个方程数值求解的过程，对应的就是绝大部分机器学习算法中的梯度下降过程。</p>
<p>在测试集上评估的结果，我们预想的误差应当包含两部分：</p>
<ol>
<li>似然函数 $L(\theta|x)$ 对真实世界概率分布描述能力不足，带来的误差；</li>
<li>通过 $X$ 估计 $\theta$ 时，样本本身的误差；</li>
</ol>
<p>若假定我们可以通过梯度下降收敛（即上面最大似然函数的导数在 0 的一个很小的临域中），那么至少就是我们相信在观测数据集 $X$ 上，模型是正确的，那么评估的误差就更加明确的指向 $X$ 本身带来的误差。</p>
<h3 id="fisher-信息量">Fisher 信息量</h3>
<p>为了求解最大似然估计，我们常用的数值手段是：</p>
<p>假定观测数据集 $X$ 的真实世界概率对应的概率密度函数是 $f(x_i;\theta)$，定义似然函数：</p>
<p>$$
L(X;\theta) = \prod \limits^{n}_{i=1} f(x_i;\theta)
$$</p>
<p>求解时，先对 $L(X|\theta)$ 取对数，再求导，这个函数定义为 Score function：</p>
<p>$$
S(X;\theta) = \sum \limits^n_{i=1} \frac{\partial \ln f(x_i;\theta)}{\partial\theta}
$$</p>
<p>则 Fisher 信息量的定义就是这个 Score function 的二阶矩（second moment）</p>
<p>$$
I(\theta) = E[S(X;\theta)^2]
$$</p>
<p>Fisher 信息量最重要的意义是：通过中心极限定理，弱大数定律，依概率一致收敛，以及 Slutsky 定理，可以证明 MLE 的渐进分布是正态分布 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，即：</p>
<ol>
<li>$\hat \theta \stackrel{P}{\longrightarrow} \theta_0$，其中 $\theta_0$ 是参数的真实值；</li>
<li>$\sqrt{n}(\hat\theta - \theta_0) \stackrel{L}{\longrightarrow} N(0,I^{-1}(\theta))$ ;</li>
</ol>
<h3 id="数据量与误差的关系">数据量与误差的关系</h3>
<p>花了大量篇幅描述了 最大似然 和 Fisher 信息量后，最终真正值得我们关注的结论却异常的简单：</p>
<p>$$
L(D) \propto D^{-0.5}
$$</p>
<p>这个结论同计算均值时，数据样本带来的误差是完全一样的。</p>
<p>真实的机器学习条件下，我们的样本量的质量并不均匀，所以往往会优先使用更好的样本（小样本集不是大样本集的随机采样，而是精选），会导致观测数据集 $X$ 不能满足概率同分布，所以带来的结果是上述幂律关系中，实际的幂律值会小于 $0.5$。</p>
<p>理论上来说，如果我们能做到样本集随机采样，那样这个幂律就会更加接近 $0.5$，而如果样本集不能随机采样，某种意义上说，能否保持这种幂律关系是值得怀疑的。所以对于 OpenAI 和 Google 的 Scaling Law 的论文，在样本量同 Loss 的关系上，Google 的结果是更可信的。</p>
<p>哪怕依旧能维持幂律关系（维持幂律关系的数学基础是不存在的。。。），具体的数值也只能通过实际拟合来估计。因为这件事不是通用规律，只跟具体的训练数据集的分布有关，跟模型无关（前提条件是模型能在<strong>大数据</strong>下<strong>收敛</strong>，即满足大数定律、中心极限定律，并且模型可以拟合真实分布）。</p>
<h2 id="compute-和-loss-的关系">Compute 和 Loss 的关系</h2>
<h3 id="控制论和-pid-算法">控制论和 PID 算法</h3>
<p>梯度下降法的数值计算过程，某种视角下可以理解成就是控制论下的控制算法——我如何根据真实信息来控制我的预期值离目标值更近。</p>
<p>直观而好用的方法就是 PID 算法：</p>
<p>$$
u(t) = K_pe(t) + K_i\int^t_0 e(\tau)\mathrm{d}\tau + K_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}
$$</p>
<p>当然，我们的梯度下降法原没有 PID 算法如此之精密，实际流程大概率只使用了 P 的部分，也就是对误差做补偿。在深度学习中，被称为反向传播。</p>
<h3 id="单参数计算量与误差的关系">单参数计算量与误差的关系</h3>
<p>单目标的 PID（只省 P 过程了）算法，误差与计算量（迭代次数）之间的关系：</p>
<p>$$
L(C) \propto K_p^{C}=e^{\lambda C}
$$</p>
<p>即，误差同计算量之间的关系是指数关系，不是幂律关系。</p>
<p>这一点在 <a href="https://arxiv.org/abs/2206.14486">Sorscher et al. (2022)</a> 中有所体现，它的结论是：至少对于某些任务，损失可以随着数据集 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 大小呈指数级增长，而不是作为幂律。</p>
<h3 id="总计算量与误差的关系">总计算量与误差的关系</h3>
<p>不同于优化问题中，我们会通过反复迭代的方式增加计算量，深度学习的计算量基本上是同模型规模和数据量正相关的。反过来意味着对单参数的优化迭代很少的固定步数就可以收敛，所以在通常数据量规模下，可以将单参数计算量带来的优化效果视作常数（都能优化到收敛）。</p>
<p>单参数计算量带来的优化效果视为常数（不会随计算量、节点数、数据量变化而变化），意味着计算本身同误差之间没有直接关联，总计算量与误差之间的关联体现的是数据量与误差的关系和节点数（结构）与误差的关系。</p>
<p>总计算量与数据量成正比，而数据量同优化效果之间的关联我们已经在前文完成了论述。下一步我们将分析节点数和误差之间的关系，或者其实更加精确的说，应当是在单参数误差不变的条件下，节点数的变化与总计算量之间的关系，是这个关系蕴含了总计算量与误差之间的关联。</p>
<h2 id="compute-和-parameters-的关系">Compute 和 Parameters 的关系</h2>
<h3 id="分形维度">分形维度</h3>
<p>具有自相似性的结构就是分形。而我们的深度学习计算就是典型的分形结构——当模型规模扩大时，主流的扩大的方式就是增加层数 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，这带来的就是自相似性。</p>
<p>而自相似性带来的重要性质就是，系统会具有分形维度，分形维度会使得系统规模扩大时，对应的全局属性并不是等比增加，而是幂律增加，幂律的指数就是其分形维度。</p>
<blockquote>
<p>生物学中有重要的 $\frac{3}{4}$ 定律——生物随着重量的变大（原子数量的规模扩大），其相关的很多生物学特征，例如新陈代谢能力、血管长度、心跳、呼吸等等，并不与重量成正比，而是按照 $\frac{3}{4}$ 的幂律进行增长。
一个直观的理解，随着生物体长度增长，其体重会以幂律 $3$ 进行提升，而腿部的横截面则是幂律 $2$ 增长。所以生物的规模变大，就会带来腿部承受的压力不断变大，所以老鼠体型的动物的腿都很细，但大象规模的动物，腿都很粗；蚂蚁可以举起自身体重百倍的物品，但人只能举起和体重相仿的物品。这些都是因为规模变化带来的非线性，要求生物的动力学模型必须发生变化，而不能与小规模时一样。
类似的，在城市规模同城市中加油站、小超市、医院之类的城市核心建设之间，也存在着幂律增长的关系——相关幂律大约是 $0.85$。</p>
</blockquote>
<p>对应的，深度学习模型中，在保证单参数误差不变的条件下，Parameters 规模的增加所需要的 Compute 计算量的增加不是等比的，而是幂律的，而且这个幂律应当是小于 $1$ 的。</p>
<p>换句话说，计算量同损失之间的关系是伴生关系——计算量本身同损失是没有直接关联的。带来损失变化的根本原因不是计算不足，而是模型表达能力以及数据本身蕴含的信息带来的。</p>
<p>但因为这里的结论中，计算量与参数数量也是幂律关系，由前文，数据同损失也是幂律关系，如果参数数量同损失同样是幂律关系的化，那么计算量与损失也可以用幂律关系来表示。</p>
<h2 id="parameters-与-loss-之间的关系">Parameters 与 Loss 之间的关系</h2>
<p>这里要分析的是参数量增加为何能带来 Loss 的降低。这是因为 Parameters 的增加，可以提升模型的表达能力，可以更好的拟合目标函数。也就是说，一个模型距离真值的误差（Loss），除了因为 Dataset 自身的误差外，还有一部分是模型距离 Dataset 所描述的最大似然函数的误差。</p>
<p>这部分要是详尽分析起来会很复杂，幸好已经有一些这方面的研究：<a href="https://arxiv.org/abs/2004.10802">Sharma et al. (2020)</a> 和 <a href="https://arxiv.org/pdf/2102.06701.pdf">Bahri et al. (2021)</a> 都对这个问题进行了很好的分析，其结果也有对应的实验支撑。</p>
<blockquote>
<p>文章假定深度模型将数据映射到一个 $d$ 维数据流形上，增加的模型参数（无限数据的条件下）都会被模型用来将数据流形分割成更小的组件，然后模型将在数据流形的每个分量上进行独立的预测，以优化训练损失。</p>
<p>这样自然的，如果我们想让子区域的大小缩小 $2$ 倍，就需要增加 $2^d$ 倍的数据量或模型参数。进而就是直观的结论：</p>
<p>$$
L(P) \propto P^{-\frac{1}{d}}
$$</p>
<p>即 Loss 与 参数量之间是幂律关系，其幂律值小于 $1$（因为有 $d&gt;1$）。</p>
</blockquote>
<h2 id="总结">总结</h2>
<p>至此，关于 Scaling Law 的数学含义就已经基本都解释清楚了。</p>
<p>更重要的问题是，有了相关的理论支撑后，我们能做什么？哪些事情做不了。</p>
<h3 id="基于多份数据融合的实验结果预测">基于多份数据融合的实验结果预测</h3>
<p><strong>这件事是不可行的</strong>。</p>
<p>一切机器学习的基础都是最大似然估计，而最大似然估计的基础假设就是独立同分布。两组分布不同的数据融合，一定会破坏原有的分布，至于不同比例下融合后形成怎样的分布，具有怎样的特性，在两份数据的分布都已知的条件下，是可以计算的。但对于我们自己的机器学习任务，原本就是要去学习数据的分布，这就决定了，不可能在不了解数据分布的条件下，估计融合后的数据分布。</p>
<p>类似的，多分不同分布的数据集怎么融合能更贴近测试集也是不可知的，只能试出来。由于测试集也不是真值，甚至测试集对真实世界的表达很可能还不如训练集，所以针对测试集做针对性调优是不值得的。</p>
<p>这部分的定量分析，其实可以借鉴 OpenAI 关于 Scaling Laws 的经典文章 <a href="https://arxiv.org/abs/2001.08361">Kaplan et al. (2020)</a> 中尝试的方法：</p>
<blockquote>
<p>迁移学习与测试效果的提升：
当我们在与训练集分布不同的文本上评估模型时，其结果与训练验证集上的结果强烈相关，损失函数中有一个大致恒定的偏移量。换句话说，转移到不同的分布会带来一定的固定惩罚，但除此之外，其提升程度大致与训练集上的表现一致。</p>
</blockquote>
<p>可以用类似这样的方法，通过多份不同分布的测试集效果打分情况，评估模型表现。</p>
<p>当然，实操方面其实也不复杂，就是多看几个测试集的结果，记录下来。如果模型优化后，在各个测试集上的提升是基本一致的，那就说明这次改进不是因为数据分布变化带来的，而是因为模型能力带来的。</p>
<h3 id="判断最优的参数和模型数据量配比">判断最优的参数和模型数据量配比</h3>
<p>这件事不是特别值得做。因为我们当前模型的优质数据不够多。所以提供的数据质量是不稳定的。小模型上得到的预测数据值，在大模型上操作时，肯定不能按预测量来操作，而是还需要进一步增加数据量。但是具体增加多少，因为我们对数据质量无法在训练前得到评估，所以是不可预测的。</p>
<p>这件事值得做的条件是：我们已经用一份数据训练了一个很大的模型，然后我们可以通过抽样的方法构建小模型，用大模型预测小模型需要多少数据量，这件事是可行的。</p>
<p>当然，如果只是一个预估值做参考，这件事倒是可以做一下。</p>
<p>注：这件事值得做的数学理论基础是：我们需要找到样本的精度和模型训练的流型精度一致的对应比例。这件事的前提条件是：模型得到充分训练，且 Loss 与 样本、模型精度是同一个数量级（Loss 就是当前的精度）。如果这个精度不一致，loss 会被更大的精度所制约。带来的影响是会增加一定的无效计算量。</p>
<p>理论上，这件事更应该用适合的停机算法来避免冗余的计算，而不是需要精准的预估精度。</p>
<h3 id="尝试用更小的模型达到更优的效果">尝试用更小的模型达到更优的效果</h3>
<p>这件事价值不是特别大。</p>
<ol>
<li>不需要知道具体的比例，我们也知道，哪怕对于小模型，喂更多的数据可以达到更好的效果。</li>
<li>小模型的表达能力是有限的，所以也不是喂更多的数据就一定可以提升效果。</li>
</ol>
<p>于是哪怕做出了预估，也需要加好多限制条件，而实际应用场景也不多。</p>
<h3 id="其他">其他？</h3>
<p>昨天看完后，原本想说 Scaling Law 是个显然的结果，其规律并不蕴含更深层次的信息。但后来仔细想了想，可能还是有很多细节值得仔细的表述一下，以免遗漏什么可能性，所以写了这个文档。</p>
<p>总得来说，我对于 Scaling Law 并没有想到更深的应用场景，它所能表达的大概也只是：更多的数据、更大的模型（更多的模型参数）可以更好的拟合真实的概率分布。这件事对于机器学习来说，是自然的结论。这个规律几乎不涉及具体的模型形式——几乎只要是机器学习都符合这个规律。</p>
<p>所以从第一性原理角度出发，它算是一个数学上给出定性的存在性定理：我们的机器学习是可以不断优化的。但它不蕴含如何能更好地做优化的信息。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>这个定理的前置条件和证明过程这里就不赘述了，需要的话自己查一下。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>大部分的训练，增加迭代次数的方式都伴随着提供更多的训练样本，若模型距离收敛所需要的迭代次数比较多，例如如果 <code>学习律</code>（本质上就是 PID 中的 $K_p$）比较小，模型距离理论上限比较远，这时误差项主要不是来源于数据自身的误差，而是来自梯度下降逼近的误差，那么这个指数关系就会比较显著，对应的表象就是误差同数据量之间是指数关系。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>还可以增加每层的神经元数量（宽度），这种增加模式就不属于分层。类似于 Scaling Law 的规律，这种扩大的方式（形状变化）对于结果的影响不显著。当然，这件事是值得做实验，试一试少层数多神经元和多层数少神经元（参数总数一致）训练的结果是否一致。盲猜会有显著性能差异。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>换个角度看推荐</title>
      <link>https://blog.uglyboy.cn/slides/2017/10/11/%E6%8D%A2%E4%B8%AA%E8%A7%92%E5%BA%A6%E7%9C%8B%E6%8E%A8%E8%8D%90/</link>
      <pubDate>Wed, 11 Oct 2017 10:59:35 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/slides/2017/10/11/%E6%8D%A2%E4%B8%AA%E8%A7%92%E5%BA%A6%E7%9C%8B%E6%8E%A8%E8%8D%90/</guid>
      <description>&lt;div class=&#34;reveal&#34;&gt;
  &lt;div class=&#34;slides&#34;&gt;
    &lt;section data-markdown&gt;
      &lt;textarea data-template&gt;
      

## 换个角度看推荐

&amp;gt; 从业务视角理解算法，用好算法

---
## 人工智能——生产力工具

&amp;gt;导言

---
#### 从历史说起
- 1946 年宾西法尼亚大学诞生了世界第一台电脑——ENIAC；
- ENIAC 比人类算数快很多（每秒 5000 次加减法），引起世界一片惊叹和争论；
- ENIAC 是什么，能做什么，只有科学家懂；
- 我们不是科学家，但如今的我们可以用手机创造更大的生产力

---
#### 文明史就是工具进化史
- 第一次工业革命，掀起了“羊吃人”运动，解放了大量生产力，促使了资本主义的诞生；
- 第二次工业革命，人类进入电气化时代；包括后来计算机的普及，人没有被淘汰，而是利用工具创造了更大的价值；
- **人工智能**，依旧不是为了替代人而产生的，而是解放人们在智力上的重复劳动。

---
#### 谁最应该用好*算法*？
- 算法工程师类似于电脑制造商，负责造出更快的、性价比更高的电脑；他们的技能树不是“如何使用电脑”；
- 业务同学（运营、产品）不需要理解电脑底层的电路原理、运转机制，但他们才是真正发挥电脑作用的人；
	- 亚洲四大邪术之中国 PS 术，都是电脑达人吗？
---
## 算法原理

&amp;gt; 第 1 节

---

&amp;gt; 如何理解“算法（人工智能）是解放人类在智力上的重复劳动的工具”？

---
#### 运营是如何进行推荐的？
- 如果有一个货架需要运营同学来搭建，普通的运营会怎么做？
	- 先解决千人一面的问题。
	- 如果场景的最终指标是 ctr，那么就把 ctr 最高的那些商品放到场景中来；
	- 商品的 ctr 跟场景是相关的，所以要数据驱动，保持场景中的商品始终是当前 ctr 最高的那些商品。
---
#### 进阶一下：分人群运营
- 如果我们对人群做把控，男性和女性可以看到不同的货架，那么如下做就可以提升整体效果：
	- 男性看到对男性而言 ctr 最高的商品；
	- 女性看到对女性而言 ctr 最高的商品；
---
#### 再进阶一下：精细化运营
- 如果能将用户区分得更加精细（例如：中国东北 30～35 岁高购买力男性），类似这样的简单组合，就可以将人分为大概 700 多类。
- 推荐算法就是用类似的方法，可以自动的对上述分类进行数据驱动下的自动商品货架调整。
- 推荐中最有效的策略（i2i）是这种分类：刚刚看过某个商品的人群，对他们而言哪些商品的 ctr 更高。

---
## 算法的本质

&amp;gt; 第 2 节

---
### 推荐（搜索）算法的本质是学习用户的行为

---
#### 万物皆可“个性化”吗？
- 推荐系统能对彩票进行推荐吗？
	- 推荐算法学得是用户行为，而不是彩票；
	- 用户做不到的事情，算法就学不到；
- 思考：算法能学出商品的质量吗？

---
#### “学习用户行为”会发生什么？
- 为了预测用户的行为，则必须要从用户的行为中学习；
- 但如果一个场景下，用户也无法识别商品的好坏，那算法还有效吗？
	- 电商购物场景下，用户能有效识别商品的好坏吗？他们怎么识别的？
	- 算法同学说：“我的模型中考虑了客诉、物流、用户评论、DSR 等因素”，这些因素真的起作用了吗？
---
#### 帮助用户做出有效的行为
- 推荐算法有效，背后的假设：**用户行为值得学习**
- 产品设计上应当帮助用户做出有效的行为
	- 例如，辅助用户决策，将用户评论、好评率、DSR 等信息外显出来；
	- 增加点赞等丰富的用户后验的社交行为，帮助其他用户进行决策。
---
#### 用户行为都值得学习吗？
- 人都会有犯错的时候。犯错的行为不值得学习；
- 一些高门槛的领域里，大多数用户都不是资深用户，他们的行为就未必值得学习；
- 推荐系统要是学习了这种行为会怎样？
	- 推荐结果中充斥着大量标题党、牛皮癣、黄图、爆款等等；
	- 用户无从选择时，只能利用上述信息进行决策；反过来，也使此类内容更容易获得高点击；
	- 正确的做法：通过背书（例如品牌旗舰店），教会用户该看什么；

---
## 指标问题

&amp;gt; 第 3 节

---
#### 如何评估推荐的效果
- 推荐算法是学习过程，所以评估可以类比于考试：
	- 学得好，成绩会提升；
	- “应试教育”是存在的；
	- 不忘初心，不“唯指标”论。观察到指标的提升，还要想一想是不是效果真的提升了。

---
#### 评估结果不是一切
- 考试是衡量学习的手段，不要为了考试而学习；不要为了指标而制定策略。
- 面对指标时，不要盲目认定“这是个黑盒”便放弃了思考；成绩不好反映的是学习过程出现了问题。
- 成绩没反映出来问题也不代表没有问题

---
#### 统计数据会说谎
-----

&amp;gt; 如何让一个班级的成绩快速提升？

#### 开除这个班级平均分以下的学生

- 思考：
	- 算法可以引领消费升级吗？
	- 算法可以引领内容化吗？

---
#### 关于指标的误区
- 当我们考核点击率、客单价时，统计指标是否反应了我们的本意：
	- 我们希望同一拨人的指标均值得到提升；
	- 更准确的，我们期待人的成长，而不是数据的增长。不加限制（例如有些人流失了）的点击率提升证明不了任何事情。
	- 也回顾前面的话题：任何人群的指标增长都能证明业务的提升吗？例如作弊人群？新用户（无产品心智）人群？

---
#### 算法被应用时也需要小心

&amp;gt; 推荐算法不以改变用户为目标

- 我们希望学生学会更多知识，进而提升了成绩：
	- 教育工作者的重点会考察学生还有哪些知识没掌握，然后教会学生；
	- 算法会找出最容易考高分的卷子来提升成绩；
	- 短期内，算法指导得成绩一定更好，甚至任何时候互换老师（AB 测试），算法的成绩都更高；
	- 但算法无法指导学生变得更好。
---
## 总结
---
### 本文要点
- 算法是工具，需要人来使用好这个工具；
- 算法的基本原理：统计当前场景下，对于不同细分人群，怎样的推荐结果指标更好；
- 算法的本质：学习用户行为！用户不懂的，算法也学不会。
- 指标问题：算法是唯指标论的，但人不是。需要人把控算法，而不是算法控制人。

---
#### 算法在场景中应用的要点
- 场景要解决用户什么问题？这个问题是否可以通过学习用户行为而习得？值得学习哪些用户行为？
- 这件事用什么指标评估？需要哪些限定条件？除了指标，还有没有其他的佐证方式？
- 项目的不同阶段需要不同的指标和做法，不盲目使用算法，同算法工程师沟通清楚需求。

---
#### 最后的一个小问题

&amp;gt; 如果产品经理的 KPI 和 算法工程师的 KPI 是一样的，那产品和算法究竟应该听谁的？
      &lt;/textarea&gt;
    &lt;/section&gt;
  &lt;/div&gt;
&lt;/div&gt;</description>
      <content:encoded><![CDATA[<div class="reveal">
  <div class="slides">
    <section data-markdown>
      <textarea data-template>
      

## 换个角度看推荐

&gt; 从业务视角理解算法，用好算法

---
## 人工智能——生产力工具

&gt;导言

---
#### 从历史说起
- 1946 年宾西法尼亚大学诞生了世界第一台电脑——ENIAC；
- ENIAC 比人类算数快很多（每秒 5000 次加减法），引起世界一片惊叹和争论；
- ENIAC 是什么，能做什么，只有科学家懂；
- 我们不是科学家，但如今的我们可以用手机创造更大的生产力

---
#### 文明史就是工具进化史
- 第一次工业革命，掀起了“羊吃人”运动，解放了大量生产力，促使了资本主义的诞生；
- 第二次工业革命，人类进入电气化时代；包括后来计算机的普及，人没有被淘汰，而是利用工具创造了更大的价值；
- **人工智能**，依旧不是为了替代人而产生的，而是解放人们在智力上的重复劳动。

---
#### 谁最应该用好*算法*？
- 算法工程师类似于电脑制造商，负责造出更快的、性价比更高的电脑；他们的技能树不是“如何使用电脑”；
- 业务同学（运营、产品）不需要理解电脑底层的电路原理、运转机制，但他们才是真正发挥电脑作用的人；
	- 亚洲四大邪术之中国 PS 术，都是电脑达人吗？
---
## 算法原理

&gt; 第 1 节

---

&gt; 如何理解“算法（人工智能）是解放人类在智力上的重复劳动的工具”？

---
#### 运营是如何进行推荐的？
- 如果有一个货架需要运营同学来搭建，普通的运营会怎么做？
	- 先解决千人一面的问题。
	- 如果场景的最终指标是 ctr，那么就把 ctr 最高的那些商品放到场景中来；
	- 商品的 ctr 跟场景是相关的，所以要数据驱动，保持场景中的商品始终是当前 ctr 最高的那些商品。
---
#### 进阶一下：分人群运营
- 如果我们对人群做把控，男性和女性可以看到不同的货架，那么如下做就可以提升整体效果：
	- 男性看到对男性而言 ctr 最高的商品；
	- 女性看到对女性而言 ctr 最高的商品；
---
#### 再进阶一下：精细化运营
- 如果能将用户区分得更加精细（例如：中国东北 30～35 岁高购买力男性），类似这样的简单组合，就可以将人分为大概 700 多类。
- 推荐算法就是用类似的方法，可以自动的对上述分类进行数据驱动下的自动商品货架调整。
- 推荐中最有效的策略（i2i）是这种分类：刚刚看过某个商品的人群，对他们而言哪些商品的 ctr 更高。

---
## 算法的本质

&gt; 第 2 节

---
### 推荐（搜索）算法的本质是学习用户的行为

---
#### 万物皆可“个性化”吗？
- 推荐系统能对彩票进行推荐吗？
	- 推荐算法学得是用户行为，而不是彩票；
	- 用户做不到的事情，算法就学不到；
- 思考：算法能学出商品的质量吗？

---
#### “学习用户行为”会发生什么？
- 为了预测用户的行为，则必须要从用户的行为中学习；
- 但如果一个场景下，用户也无法识别商品的好坏，那算法还有效吗？
	- 电商购物场景下，用户能有效识别商品的好坏吗？他们怎么识别的？
	- 算法同学说：“我的模型中考虑了客诉、物流、用户评论、DSR 等因素”，这些因素真的起作用了吗？
---
#### 帮助用户做出有效的行为
- 推荐算法有效，背后的假设：**用户行为值得学习**
- 产品设计上应当帮助用户做出有效的行为
	- 例如，辅助用户决策，将用户评论、好评率、DSR 等信息外显出来；
	- 增加点赞等丰富的用户后验的社交行为，帮助其他用户进行决策。
---
#### 用户行为都值得学习吗？
- 人都会有犯错的时候。犯错的行为不值得学习；
- 一些高门槛的领域里，大多数用户都不是资深用户，他们的行为就未必值得学习；
- 推荐系统要是学习了这种行为会怎样？
	- 推荐结果中充斥着大量标题党、牛皮癣、黄图、爆款等等；
	- 用户无从选择时，只能利用上述信息进行决策；反过来，也使此类内容更容易获得高点击；
	- 正确的做法：通过背书（例如品牌旗舰店），教会用户该看什么；

---
## 指标问题

&gt; 第 3 节

---
#### 如何评估推荐的效果
- 推荐算法是学习过程，所以评估可以类比于考试：
	- 学得好，成绩会提升；
	- “应试教育”是存在的；
	- 不忘初心，不“唯指标”论。观察到指标的提升，还要想一想是不是效果真的提升了。

---
#### 评估结果不是一切
- 考试是衡量学习的手段，不要为了考试而学习；不要为了指标而制定策略。
- 面对指标时，不要盲目认定“这是个黑盒”便放弃了思考；成绩不好反映的是学习过程出现了问题。
- 成绩没反映出来问题也不代表没有问题

---
#### 统计数据会说谎
-----

&gt; 如何让一个班级的成绩快速提升？

#### 开除这个班级平均分以下的学生

- 思考：
	- 算法可以引领消费升级吗？
	- 算法可以引领内容化吗？

---
#### 关于指标的误区
- 当我们考核点击率、客单价时，统计指标是否反应了我们的本意：
	- 我们希望同一拨人的指标均值得到提升；
	- 更准确的，我们期待人的成长，而不是数据的增长。不加限制（例如有些人流失了）的点击率提升证明不了任何事情。
	- 也回顾前面的话题：任何人群的指标增长都能证明业务的提升吗？例如作弊人群？新用户（无产品心智）人群？

---
#### 算法被应用时也需要小心

&gt; 推荐算法不以改变用户为目标

- 我们希望学生学会更多知识，进而提升了成绩：
	- 教育工作者的重点会考察学生还有哪些知识没掌握，然后教会学生；
	- 算法会找出最容易考高分的卷子来提升成绩；
	- 短期内，算法指导得成绩一定更好，甚至任何时候互换老师（AB 测试），算法的成绩都更高；
	- 但算法无法指导学生变得更好。
---
## 总结
---
### 本文要点
- 算法是工具，需要人来使用好这个工具；
- 算法的基本原理：统计当前场景下，对于不同细分人群，怎样的推荐结果指标更好；
- 算法的本质：学习用户行为！用户不懂的，算法也学不会。
- 指标问题：算法是唯指标论的，但人不是。需要人把控算法，而不是算法控制人。

---
#### 算法在场景中应用的要点
- 场景要解决用户什么问题？这个问题是否可以通过学习用户行为而习得？值得学习哪些用户行为？
- 这件事用什么指标评估？需要哪些限定条件？除了指标，还有没有其他的佐证方式？
- 项目的不同阶段需要不同的指标和做法，不盲目使用算法，同算法工程师沟通清楚需求。

---
#### 最后的一个小问题

&gt; 如果产品经理的 KPI 和 算法工程师的 KPI 是一样的，那产品和算法究竟应该听谁的？
      </textarea>
    </section>
  </div>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>场景的用户认知度（熟客率）研究</title>
      <link>https://blog.uglyboy.cn/posts/2016/09/14/%E5%9C%BA%E6%99%AF%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%A4%E7%9F%A5%E5%BA%A6%E7%86%9F%E5%AE%A2%E7%8E%87%E7%A0%94%E7%A9%B6/</link>
      <pubDate>Wed, 14 Sep 2016 22:40:50 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2016/09/14/%E5%9C%BA%E6%99%AF%E7%9A%84%E7%94%A8%E6%88%B7%E8%AE%A4%E7%9F%A5%E5%BA%A6%E7%86%9F%E5%AE%A2%E7%8E%87%E7%A0%94%E7%A9%B6/</guid>
      <description>如何用数据描述用户心智</description>
      <content:encoded><![CDATA[<h2 id="背景">背景</h2>
<p>个性化推荐去年在我们集团取得了非常优异的成绩，为大促、日常场景都带来了非常大的突破，我身边的朋友们也都纷纷说现在的淘宝的个性化很棒。尤其是首图个性化技术，为我们打开了崭新的一扇窗，让诸多场景的日活、成交等核心指标都有了翻倍的提升。</p>
<p>有了更先进的技术，却也带来了一些新的疑问——首图个性化究竟给各个具体的业务 场景带来了什么？各种业务指标的提升可能更多的反映着首图个性化的效果，与场景内的效 果怎样并没有特别大的关系了。所以我们需要有一个新的判定标准，让我们自己知道——现在场景内的效果并不能让用户满意，不急于通过首图来引流；或者现在场景内的用户认知度 和归属感很高，应当获得更多的引流来吸引更多的用户。</p>
<p>如果存在这样一个判定标准，那么它应当是用户对一个场景的认知情况——即当用户 并不知道现在场景内的推荐效果时，他依然愿意来到这个场景的意愿。这便是我的一个基本假设：<strong>用户对一个场景的认知（喜爱程度）是可以不依赖于用户在场景内的行为，而仅仅通 过用户对场景的访问频率即可反映出来。</strong></p>
<h2 id="熟客率定义">熟客率定义</h2>
<p>依据上文的基本假设，我们可以给出熟客率的定义：</p>
<p>$$
\text{熟客率} = \frac{\text{单位时间内内用户访问某场景的次数}}{\text{单位时间内用户访问手淘的次数}}
$$</p>
<p>上述数据中有一些技术细节实现起来较复杂（例如一次访问手淘会多次访问某场景；或场景的 PV 日志是仅仅是请求的日志，未必是真实访问等等），我们用了一个粗略的方法来近似的得到熟客率：</p>
<p>$$
\text{熟客率} = \frac{\text{某场景月访问天数}}{\text{手淘的月访问天数}}
$$</p>
<p>于是对每一个用户，都可以算出他最近一个月在各个场景的访问频度（熟客率）了。</p>
<p>对某一个特定的场景而言，我们想要了解的是类似于：“高频用户的占比”这样的指标，来了解用户对于一个场景的喜爱程度，所以我们按照用户熟客率统计用户的分布情况：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/hoOVZyXNMgtTf9b.png" alt=""  />
</p>
<p>除去搜索的图型比较特别外，其他的场景数据都很像一个指数分布，于是我们对用户分布情况取对数再来观察：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/oFK9liBgCGrIesH.png" alt=""  />
</p>
<p>从这个图来看，基本符合我们的假设：用户分布情况是接近于指数分布的，但在访问频度为 1 的位置会产生很奇异的拐点，中间也有很多位置并不平滑。进一步分析发现，这样的现象主要是因为部分用户对手淘的访问天数过短，造成他们的访问信息并不具有有效性。为此，我们仅统计每月访问手淘 15 天以上的用户（用户量为：139523167，基本覆盖手淘的活跃用户）。</p>
<p>重新统计的上图如下：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/Sdw1re3Nvatq4uO.png" alt=""  />
</p>
<p>这一回，整个曲线都变得平滑了（仅仅是在 0.3~0.4 附近感觉有拐点）。根据这个图，我们增加如下的假设：对一个场景而言，有认知的用户的的分布模型是一个指数分布；高频访问的用户基本都是有认知的用户。</p>
<h2 id="场景指标的计算">场景指标的计算</h2>
<p>接上文，我们根据熟客率的指标可以用回归的方法统计出“有认知”的用户再场景内访问频率的分布情况：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">斜率</th>
          <th style="text-align: left">截距</th>
          <th style="text-align: left">有认知的用户量</th>
          <th style="text-align: left">认知用户平均熟客率</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">猜你喜欢</td>
          <td style="text-align: left">1.7431016306</td>
          <td style="text-align: left">16.357052585</td>
          <td style="text-align: left">7285426</td>
          <td style="text-align: left">57.37%</td>
      </tr>
      <tr>
          <td style="text-align: left">有好货</td>
          <td style="text-align: left">5.6656943459</td>
          <td style="text-align: left">16.580370553</td>
          <td style="text-align: left">2802272</td>
          <td style="text-align: left">17.65%</td>
      </tr>
      <tr>
          <td style="text-align: left">发现好店</td>
          <td style="text-align: left">7.198381533</td>
          <td style="text-align: left">17.412020797</td>
          <td style="text-align: left">5066519</td>
          <td style="text-align: left">13.89%</td>
      </tr>
      <tr>
          <td style="text-align: left">她在买</td>
          <td style="text-align: left">6.7192760078</td>
          <td style="text-align: left">13.955514186</td>
          <td style="text-align: left">171191</td>
          <td style="text-align: left">14.88%</td>
      </tr>
      <tr>
          <td style="text-align: left">女装</td>
          <td style="text-align: left">7.8873829626</td>
          <td style="text-align: left">16.842386083</td>
          <td style="text-align: left">2615908</td>
          <td style="text-align: left">12.68%</td>
      </tr>
      <tr>
          <td style="text-align: left">男装</td>
          <td style="text-align: left">9.3844322456</td>
          <td style="text-align: left">14.08435403</td>
          <td style="text-align: left">139428</td>
          <td style="text-align: left">10.66%</td>
      </tr>
      <tr>
          <td style="text-align: left">搜索</td>
          <td style="text-align: left">4.7601938755</td>
          <td style="text-align: left">19.56287163</td>
          <td style="text-align: left">65829803</td>
          <td style="text-align: left">21.01%</td>
      </tr>
  </tbody>
</table>
<p>其中的斜率就是指数分布的概率密度函数中的系数 $\lambda$，根据 $\lambda$ 可以算出数学期望（平均 熟客率），截距和 $\lambda$ 合在一起可以计算出指数分布的用户总量：</p>
<p>$$
f(x) = \lambda e^{-\lambda}
$$</p>
<p>从而就能得到上面表格中的数据指标。</p>
<p>我们再根据这个数据去反推一下没有场景认知的用户访问情况：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/KdSPQm8AXbMiHOE.png" alt=""  />
</p>
<p>图形也是呈指数分布，但这里的降幅快了很多（而且相比于原有的数据，误差增加了不少）。还是通过取对数，再根据散点图来观察：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/4Wi9dRLp7uoxzrX.png" alt=""  />
</p>
<p>通过这个图来观察的话，指数分布的趋势明显了很多。用类似的方法，我们可以得到：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">斜率</th>
          <th style="text-align: left">截距</th>
          <th style="text-align: left">无认知的用户量</th>
          <th style="text-align: left">无认知用户平均访问频率</th>
          <th style="text-align: left">认知用户占比</th>
          <th style="text-align: left">认知用户转化增益</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">猜你喜欢</td>
          <td style="text-align: left">9.3894679073</td>
          <td style="text-align: left">17.8609986824</td>
          <td style="text-align: left">6085443</td>
          <td style="text-align: left">10.65%</td>
          <td style="text-align: left">54.49%</td>
          <td style="text-align: left">5.4</td>
      </tr>
      <tr>
          <td style="text-align: left">有好货</td>
          <td style="text-align: left">19.6362279409</td>
          <td style="text-align: left">19.0102797422</td>
          <td style="text-align: left">9183358</td>
          <td style="text-align: left">5.09%</td>
          <td style="text-align: left">23.38%</td>
          <td style="text-align: left">3.5</td>
      </tr>
      <tr>
          <td style="text-align: left">发现好店</td>
          <td style="text-align: left">18.7503695019</td>
          <td style="text-align: left">18.2540393165</td>
          <td style="text-align: left">4514594</td>
          <td style="text-align: left">5.33%</td>
          <td style="text-align: left">52.88%</td>
          <td style="text-align: left">2.6</td>
      </tr>
      <tr>
          <td style="text-align: left">她在买</td>
          <td style="text-align: left">27.7360657413</td>
          <td style="text-align: left">19.191585643</td>
          <td style="text-align: left">7793901</td>
          <td style="text-align: left">3.61%</td>
          <td style="text-align: left">2.15%</td>
          <td style="text-align: left">4.1</td>
      </tr>
      <tr>
          <td style="text-align: left">女装</td>
          <td style="text-align: left">61.7233508719</td>
          <td style="text-align: left">21.8490324548</td>
          <td style="text-align: left">49941863</td>
          <td style="text-align: left">1.62%</td>
          <td style="text-align: left">4.98%</td>
          <td style="text-align: left">7.8</td>
      </tr>
      <tr>
          <td style="text-align: left">男装</td>
          <td style="text-align: left">22.816772562</td>
          <td style="text-align: left">17.6569131267</td>
          <td style="text-align: left">2041953</td>
          <td style="text-align: left">4.38%</td>
          <td style="text-align: left">6.39%</td>
          <td style="text-align: left">2.4</td>
      </tr>
  </tbody>
</table>
<p>其中的认知用户转化增益是指一个用户从无认知转变为有认知的用户，其访问频率提升的倍数。当然，并不是无认知的用户一定可以转化为有认知的用户，以女装为例，部分无认知的用户（例如男性）就很难转化为有认知的用户。</p>
<p>注：这里的结论是，各个场景的用户分布都是两个指数分布的叠加，两个指数分布的参数不同。对于这个结论，我这边也通过其他的数据分析验证了这样的结果。另外对于这两个指数分布的参数求解问题，上文中的方法并不是最合适的方法，更合理的方式应该是用 EM 算法来反复迭代求解两个系数。本文的数据足以说明结论，所以没有进一步采用更精确的求解方法。</p>
<p>另注：上述表格中用户量仅仅是统计分析的趋势值，并不是真实值预估，因为对重要指标计算基本没有影响，所以我并没有很细致的做归一化等工作。这个绝对数值还不能用来作为场景间评估和考量标准。</p>
<h2 id="指标的可靠性分析">指标的可靠性分析</h2>
<h3 id="用户维度熟客率指标的可靠性">用户维度熟客率指标的可靠性</h3>
<p>因为场景下的用户分为：有认知的和无认知的两类，尤其是在熟客率较低的部分，两类用户不可分。为了便于分析，本节的讨论都仅针对有认知的用户进行分析，采样的方法：仅取熟客率大于 $0.5$ 的用户</p>
<p>用户的熟客率是用一个月分访问频率来描述的，这个指标在某一天的体现即：一部分用户访问了手淘的用户这一天访问某场景的比例。熟客率的稳定性即：高频访问的用户和低频访问的用户是明显可分的——通常情况下不会出现大量的高频用户和低频用户的访问比例发生反转</p>
<p>为验证这个结论我们用猜你喜欢的数据，统计了熟客率在 $0.5$<del>$0.6$ 之间的用户和熟客率 再 $0.6$</del>$0.7$ 之间的用户，他们再过去一个月中，每天访问概率的变化情况：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/gdtCXfuTQrcIS6e.png" alt=""  />
</p>
<p>是跟次日回访率一致的——用户再某一天的访问概率跟入口情况和时间周期有密切的联系， 但依然可以非常明显的看出：不同熟客率区间的用户，他们的访问概率是很明显的不同的， 但趋势又非常的一致。这说明熟客率这个指标是一个很稳定的衡量用户对场景认知的指标。</p>
<h3 id="用户熟客率与客户价值之间的关联">用户熟客率与客户价值之间的关联</h3>
<p>用户价值是指用户在场景内的具体行为贡献，包括点击、购买、客单价等等。因为熟客率计算是完全不考虑用户在场景内的行为的，所以跟诸如：CTR，CVR 等指标没有直接关联。我们想统计一下熟客率是否跟用户价值是正相关的。为简化研究复杂度（或者说我偷了个懒），这里仅统计了点击率（uctr），来验证用户熟客率和客户价值之间的关系。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: left">0%</th>
          <th style="text-align: left">10%</th>
          <th style="text-align: left">20%</th>
          <th style="text-align: left">30%</th>
          <th style="text-align: left">40%</th>
          <th style="text-align: left">50%</th>
          <th style="text-align: left">60%</th>
          <th style="text-align: left">70%</th>
          <th style="text-align: left">80%</th>
          <th style="text-align: left">90%</th>
          <th style="text-align: left">100.00%</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">有好货</td>
          <td style="text-align: left">37.29%</td>
          <td style="text-align: left">41.91%</td>
          <td style="text-align: left">47.34%</td>
          <td style="text-align: left">50.48%</td>
          <td style="text-align: left">52.69%</td>
          <td style="text-align: left">54.36%</td>
          <td style="text-align: left">55.92%</td>
          <td style="text-align: left">57.71%</td>
          <td style="text-align: left">60.13%</td>
          <td style="text-align: left">63.20%</td>
          <td style="text-align: left">67.11%</td>
      </tr>
      <tr>
          <td style="text-align: left">猜你喜欢</td>
          <td style="text-align: left">57.78%</td>
          <td style="text-align: left">64.47%</td>
          <td style="text-align: left">69.98%</td>
          <td style="text-align: left">74.35%</td>
          <td style="text-align: left">78.15%</td>
          <td style="text-align: left">81.56%</td>
          <td style="text-align: left">84.59%</td>
          <td style="text-align: left">87.41%</td>
          <td style="text-align: left">90.20%</td>
          <td style="text-align: left">93.33%</td>
          <td style="text-align: left">95.60%</td>
      </tr>
  </tbody>
</table>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/8sYMwxuSc7j9C2E.png" alt=""  />
</p>
<p>如预期的，点击率同用户的熟客率很明显的正相关，即熟客率越高，点击率越高。有充分的理由相信，pctr，cvr， 客单价等指标也会有显著的相关性，但具体的结论还待进一步研究。</p>
<p>同时这个结论也反向反映了另一个结论：上文中场景认知用户的指数分布系数 λ 越小，场景内价值体现的越好（高熟客率的用户占比更多，从而价值指标的平均值更高）。用通俗的话来解释就是：我们应当培养更多的老用户，他们能贡献更大的价值（虽然看上去是很显然的结论，但数据可以模型化这个结论并进一步的量化带来的收益）。</p>
<h3 id="相关结果对场景优化方向的指导性">相关结果对场景优化方向的指导性</h3>
<p>我们反过来再分析一下场景下的“有认知用户平均熟客率（访问频度）”和“无认知用户平均熟客率”这两个指标。</p>
<p>无认知的用户更多的是通过偶然的点击或者首图个性化吸引到场景内的，好的首图个性化可以提升这部分用户的占比，同时也可以提升这部分用户的熟客率分布曲线，创造出更多的将无认知用户转化为有认知用户的可能性；同时，好的首图个性化也能提升有认知的用户的熟客率分布曲线，从而提升场景价值。</p>
<p>不同场景的“无认知用户平均熟客率”描述的是一个场景比较客观的，对新用户的友好程度——在手淘上位置隐藏越深，对新手而言越不友好；没有首图个性化对新手不友好；场景里用户心智不明确或者缺少新手引导等等，也是对新手的不友好。种种因素都会导致这个指标的高低；</p>
<p>认知用户占比，说明的是流量冗余的问题（上文中的认知用户占比的计算可能因为未归一化带来较大的误差，不能作为一个确定值来对待，但整体趋势问题不大，例如猜你喜欢的认知用户占比远高于女装）。如果认知用户占比很低，说明流量中大量的都是新用户或者无认知用户。这个指标也说明了另一个潜在的问题：我们现有统计用户留存的指标都没有考虑用户本身的属性，像女装这类场景的用户留存，可能更多的反映的是首图的引流效果，而并不是场景内的真实的用户留存能力。</p>
<p>对于占比很低的场景，需要的工作有三点：</p>
<ol>
<li>因种种原因决定了用户分布中大量的都是新用户，所以需要在产品设计上对新用户更友好；</li>
<li>提升场景内的用户留存能力（将新用户转化为老用户的能力）；</li>
<li>控制外部引流。现在场景的主要问题不是流量不足，而是场景本身兜不住这么大的流量。</li>
</ol>
<p>当然，最核心的，最能创造价值的，还是有认知用户的熟客率指标。不断提升有认知 用户的熟客率指标，是一个产品长久发展的核心！另外我们也可以根据现有的有认知用户， 观察一个场景究竟更能吸引怎样的用户，以此来决定产品定位的调整或者做定向的投放来引流。</p>
<h2 id="熟客率指标作为算法优化指标的尝试">熟客率指标作为算法优化指标的尝试</h2>
<p>以猜你喜欢数据为例，对两个不同的分桶统计了单日访问率指标和次日回访率（）曲线是两个桶之间的差值：</p>
<p><img loading="lazy" src="https://s2.loli.net/2024/09/18/UNTaBvFnRbzSIGx.png" alt=""  />
</p>
<p>因为我这边并不知道猜你喜欢上具体做得是什么实验，所以没有办法进行物理含义的分析。但是从图上还是可以看到，熟客率是一个可衡量的指标，能够对具体的算法实验进行效果评估。</p>
<p>另一方面，熟客率指标的整体趋势和回访率是比较接近的，但在一些特定的点上会有较大的差异性。带来这部分差异的，应该就是“有认知的用户”和“没有认知的用户”对待一些变化的不同反映。</p>
<p>关于熟客率作为算法指标的优化，这部分现在还不是很成熟，需要进一步的探究。</p>
<h2 id="展望">展望</h2>
<p>为了简化工作量，相关的统计和计算都做了不少的简化，也由此带来了一些系统误差。例如用访问天代替访问次，会在物理含义上出现较大的变化，也会带来数据分析上的一些问题（指数分布会限制在 $0$~$1$,$1$ 的地方在特定情况下会产生奇异点）。没有用 em 算法来计算，新用户的数据的可靠性也不能保证。所以后续希望能和 BI 的同学合作，把数据的准确性提升来。</p>
<p>另外前两天袁全给了一些指导性的建议，后续可以尝试用类似的方法考察用户在品牌（店铺）上的认知度，计算用户对商家而言的价值。</p>
<p>再者，通过 $\text{熟客率}&gt;0.5$ 的条件，可以圈定一个产品的高认知用户群，以此可以定向的分析用户图谱，做用户访谈等等，能更好的得到对产品而言更有价值的用户划分，以此来指导产品的后续改进方向。</p>
<p>另外能否产出算法可优化指标方面，可能还需要进一步的探索。个人预期是，至少找到一些明确的能提升用户熟客率的手段（也许会带来其他指标的下降），给算法优化背后的物理意义一些更明确的理论阐述</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
