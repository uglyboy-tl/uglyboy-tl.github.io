<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Scaling Law 的数学解读 | 拾柒读库</title>
<meta name=keywords content="Scaling Law,LLM"><meta name=description content="Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计： 模型下的理想真实世界的概率分布：$p(x|\theta)$ 我们不知道"><meta name=author content="癸老师"><link rel=canonical href=https://blog.uglyboy.cn/posts/2023-10-10/><link crossorigin=anonymous href=/assets/css/stylesheet.0934fef44c344e8edad5c64ba07ec04c38c6653786088a971daa2c30d25eecc9.css integrity="sha256-CTT+9Ew0To7a1cZLoH7ATDjGZTeGCIqXHaosMNJe7Mk=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.uglyboy.cn/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.uglyboy.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.uglyboy.cn/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.uglyboy.cn/apple-touch-icon.png><link rel=mask-icon href=https://blog.uglyboy.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{enableExplorer:!1,skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.12.1/polyfill.min.js integrity="sha512-uzOpZ74myvXTYZ+mXUsPhDF+/iL/n32GDxdryI2SJronkEyKC8FBFRLiBQ7l7U/PTYebDbgTtbqTa6/vGtU23A==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js></script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/lxgw-wenkai-webfont/1.6.0/style.min.css><style>body,code,section{font-family:lxgw wenkai,sans-serif}</style><script src=https://cdnjs.cloudflare.com/ajax/libs/valine/1.5.1/Valine.min.js integrity="sha512-FxPzcGnQGjqsUr1ofvicx9Y7T53thxBSvYCBdKeW/Sfr5dQVEkh70r7sl1TMsEVheXsoOTqXZ9z2t75JNeak/g==" crossorigin=anonymous referrerpolicy=no-referrer></script><meta property="og:title" content="Scaling Law 的数学解读"><meta property="og:description" content="Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计： 模型下的理想真实世界的概率分布：$p(x|\theta)$ 我们不知道"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.uglyboy.cn/posts/2023-10-10/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-10T11:50:00+08:00"><meta property="article:modified_time" content="2023-11-02T13:57:35+08:00"><meta property="og:site_name" content="拾柒读库"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scaling Law 的数学解读"><meta name=twitter:description content="Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计： 模型下的理想真实世界的概率分布：$p(x|\theta)$ 我们不知道"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"文章","item":"https://blog.uglyboy.cn/posts/"},{"@type":"ListItem","position":2,"name":"Scaling Law 的数学解读","item":"https://blog.uglyboy.cn/posts/2023-10-10/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Scaling Law 的数学解读","name":"Scaling Law 的数学解读","description":"Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计： 模型下的理想真实世界的概率分布：$p(x|\\theta)$ 我们不知道","keywords":["Scaling Law","LLM"],"articleBody":"Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计：\n模型下的理想真实世界的概率分布：$p(x|\\theta)$\n我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\\theta|x)$\n现在 $x$ 已知，$\\theta$ 未知，若对于两个参数 $\\theta_1$ 和 $\\theta_2$ 有\n$$ L(\\theta_1|x) = p(x|\\theta_1) \u003e p(x|\\theta_2) = L(\\theta_2|x) $$\n那么意味着 $\\theta=\\theta_1$ 时，随机变量 $\\theta_1$ 生成 $x$ 的概率大于当参数 $\\theta=\\theta_2$ 时。这也正是似然的意义所在，若观测数据为 $x$，那么 $\\theta_1$ 是比 $\\theta_2$ 更有可能为分布函数的参数。\n在给定观测数据集 $X={x_n},n \\in \\mathbb{N}$ 时，真实世界最有可能的概率分布对应的参数 $\\hat\\theta$ 应该满足：\n$$ L(\\hat\\theta|x) = p(x|\\hat\\theta) \u003e p(x|\\theta) = L(\\theta|x), \\theta \\in \\mathbb{\\Theta} 且 \\theta \\ne \\hat\\theta $$\n即：\n$$ \\hat\\theta = \\arg\\max\\limits_\\theta L(\\theta|x) $$\n求解最大似然函数：\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\theta} L(\\theta|x) = 0 $$\n对这个方程数值求解的过程，对应的就是绝大部分机器学习算法中的梯度下降过程。\n在测试集上评估的结果，我们预想的误差应当包含两部分：\n似然函数 $L(\\theta|x)$ 对真实世界概率分布描述能力不足，带来的误差； 通过 $X$ 估计 $\\theta$ 时，样本本身的误差； 若假定我们可以通过梯度下降收敛（即上面最大似然函数的导数在 0 的一个很小的临域中），那么至少就是我们相信在观测数据集 $X$ 上，模型是正确的，那么评估的误差就更加明确的指向 $X$ 本身带来的误差。\nFisher 信息量 为了求解最大似然估计，我们常用的数值手段是：\n假定观测数据集 $X$ 的真实世界概率对应的概率密度函数是 $f(x_i;\\theta)$，定义似然函数：\n$$ L(X;\\theta) = \\prod \\limits^{n}_{i=1} f(x_i;\\theta) $$\n求解时，先对 $L(X|\\theta)$ 取对数，再求导，这个函数定义为 Score function：\n$$ S(X;\\theta) = \\sum \\limits^n_{i=1} \\frac{\\partial \\ln f(x_i;\\theta)}{\\partial\\theta} $$\n则 Fisher 信息量的定义就是这个 Score function 的二阶矩（second moment）\n$$ I(\\theta) = E[S(X;\\theta)^2] $$\nFisher 信息量最重要的意义是：通过中心极限定理，弱大数定律，依概率一致收敛，以及 Slutsky 定理，可以证明 MLE 的渐进分布是正态分布 1，即：\n$\\hat \\theta \\stackrel{P}{\\longrightarrow} \\theta_0$，其中 $\\theta_0$ 是参数的真实值； $\\sqrt{n}(\\hat\\theta - \\theta_0) \\stackrel{L}{\\longrightarrow} N(0,I^{-1}(\\theta))$ ; 数据量与误差的关系 花了大量篇幅描述了 最大似然 和 Fisher 信息量后，最终真正值得我们关注的结论却异常的简单：\n$$ L(D) \\propto D^{-0.5} $$\n这个结论同计算均值时，数据样本带来的误差是完全一样的。\n真实的机器学习条件下，我们的样本量的质量并不均匀，所以往往会优先使用更好的样本（小样本集不是大样本集的随机采样，而是精选），会导致观测数据集 $X$ 不能满足概率同分布，所以带来的结果是上述幂律关系中，实际的幂律值会小于 $0.5$。\n理论上来说，如果我们能做到样本集随机采样，那样这个幂律就会更加接近 $0.5$，而如果样本集不能随机采样，某种意义上说，能否保持这种幂律关系是值得怀疑的。所以对于 OpenAI 和 Google 的 Scaling Law 的论文，在样本量同 Loss 的关系上，Google 的结果是更可信的。\n哪怕依旧能维持幂律关系（维持幂律关系的数学基础是不存在的。。。），具体的数值也只能通过实际拟合来估计。因为这件事不是通用规律，只跟具体的训练数据集的分布有关，跟模型无关（前提条件是模型能在大数据下收敛，即满足大数定律、中心极限定律，并且模型可以拟合真实分布）。\nCompute 和 Loss 的关系 控制论和 PID 算法 梯度下降法的数值计算过程，某种视角下可以理解成就是控制论下的控制算法——我如何根据真实信息来控制我的预期值离目标值更近。\n直观而好用的方法就是 PID 算法：\n$$ u(t) = K_pe(t) + K_i\\int^t_0 e(\\tau)\\mathrm{d}\\tau + K_d\\frac{\\mathrm{d}e(t)}{\\mathrm{d}t} $$\n当然，我们的梯度下降法原没有 PID 算法如此之精密，实际流程大概率只使用了 P 的部分，也就是对误差做补偿。在深度学习中，被称为反向传播。\n单参数计算量与误差的关系 单目标的 PID（只省 P 过程了）算法，误差与计算量（迭代次数）之间的关系：\n$$ L(C) \\propto K_p^{C}=e^{\\lambda C} $$\n即，误差同计算量之间的关系是指数关系，不是幂律关系。\n这一点在 Sorscher et al. (2022) 中有所体现，它的结论是：至少对于某些任务，损失可以随着数据集 2 大小呈指数级增长，而不是作为幂律。\n总计算量与误差的关系 不同于优化问题中，我们会通过反复迭代的方式增加计算量，深度学习的计算量基本上是同模型规模和数据量正相关的。反过来意味着对单参数的优化迭代很少的固定步数就可以收敛，所以在通常数据量规模下，可以将单参数计算量带来的优化效果视作常数（都能优化到收敛）。\n单参数计算量带来的优化效果视为常数（不会随计算量、节点数、数据量变化而变化），意味着计算本身同误差之间没有直接关联，总计算量与误差之间的关联体现的是数据量与误差的关系和节点数（结构）与误差的关系。\n总计算量与数据量成正比，而数据量同优化效果之间的关联我们已经在前文完成了论述。下一步我们将分析节点数和误差之间的关系，或者其实更加精确的说，应当是在单参数误差不变的条件下，节点数的变化与总计算量之间的关系，是这个关系蕴含了总计算量与误差之间的关联。\nCompute 和 Parameters 的关系 分形维度 具有自相似性的结构就是分形。而我们的深度学习计算就是典型的分形结构——当模型规模扩大时，主流的扩大的方式就是增加层数 3，这带来的就是自相似性。\n而自相似性带来的重要性质就是，系统会具有分形维度，分形维度会使得系统规模扩大时，对应的全局属性并不是等比增加，而是幂律增加，幂律的指数就是其分形维度。\n生物学中有重要的 $\\frac{3}{4}$ 定律——生物随着重量的变大（原子数量的规模扩大），其相关的很多生物学特征，例如新陈代谢能力、血管长度、心跳、呼吸等等，并不与重量成正比，而是按照 $\\frac{3}{4}$ 的幂律进行增长。 一个直观的理解，随着生物体长度增长，其体重会以幂律 $3$ 进行提升，而腿部的横截面则是幂律 $2$ 增长。所以生物的规模变大，就会带来腿部承受的压力不断变大，所以老鼠体型的动物的腿都很细，但大象规模的动物，腿都很粗；蚂蚁可以举起自身体重百倍的物品，但人只能举起和体重相仿的物品。这些都是因为规模变化带来的非线性，要求生物的动力学模型必须发生变化，而不能与小规模时一样。 类似的，在城市规模同城市中加油站、小超市、医院之类的城市核心建设之间，也存在着幂律增长的关系——相关幂律大约是 $0.85$。\n对应的，深度学习模型中，在保证单参数误差不变的条件下，Parameters 规模的增加所需要的 Compute 计算量的增加不是等比的，而是幂律的，而且这个幂律应当是小于 $1$ 的。\n换句话说，计算量同损失之间的关系是伴生关系——计算量本身同损失是没有直接关联的。带来损失变化的根本原因不是计算不足，而是模型表达能力以及数据本身蕴含的信息带来的。\n但因为这里的结论中，计算量与参数数量也是幂律关系，由前文，数据同损失也是幂律关系，如果参数数量同损失同样是幂律关系的化，那么计算量与损失也可以用幂律关系来表示。\nParameters 与 Loss 之间的关系 这里要分析的是参数量增加为何能带来 Loss 的降低。这是因为 Parameters 的增加，可以提升模型的表达能力，可以更好的拟合目标函数。也就是说，一个模型距离真值的误差（Loss），除了因为 Dataset 自身的误差外，还有一部分是模型距离 Dataset 所描述的最大似然函数的误差。\n这部分要是详尽分析起来会很复杂，幸好已经有一些这方面的研究：Sharma et al. (2020) 和 Bahri et al. (2021) 都对这个问题进行了很好的分析，其结果也有对应的实验支撑。\n文章假定深度模型将数据映射到一个 $d$ 维数据流形上，增加的模型参数（无限数据的条件下）都会被模型用来将数据流形分割成更小的组件，然后模型将在数据流形的每个分量上进行独立的预测，以优化训练损失。\n这样自然的，如果我们想让子区域的大小缩小 $2$ 倍，就需要增加 $2^d$ 倍的数据量或模型参数。进而就是直观的结论：\n$$ L(P) \\propto P^{-\\frac{1}{d}} $$\n即 Loss 与 参数量之间是幂律关系，其幂律值小于 $1$（因为有 $d\u003e1$）。\n总结 至此，关于 Scaling Law 的数学含义就已经基本都解释清楚了。\n更重要的问题是，有了相关的理论支撑后，我们能做什么？哪些事情做不了。\n基于多份数据融合的实验结果预测 这件事是不可行的。\n一切机器学习的基础都是最大似然估计，而最大似然估计的基础假设就是独立同分布。两组分布不同的数据融合，一定会破坏原有的分布，至于不同比例下融合后形成怎样的分布，具有怎样的特性，在两份数据的分布都已知的条件下，是可以计算的。但对于我们自己的机器学习任务，原本就是要去学习数据的分布，这就决定了，不可能在不了解数据分布的条件下，估计融合后的数据分布。\n类似的，多分不同分布的数据集怎么融合能更贴近测试集也是不可知的，只能试出来。由于测试集也不是真值，甚至测试集对真实世界的表达很可能还不如训练集，所以针对测试集做针对性调优是不值得的。\n这部分的定量分析，其实可以借鉴 OpenAI 关于 Scaling Laws 的经典文章 Kaplan et al. (2020) 中尝试的方法：\n迁移学习与测试效果的提升： 当我们在与训练集分布不同的文本上评估模型时，其结果与训练验证集上的结果强烈相关，损失函数中有一个大致恒定的偏移量。换句话说，转移到不同的分布会带来一定的固定惩罚，但除此之外，其提升程度大致与训练集上的表现一致。\n可以用类似这样的方法，通过多份不同分布的测试集效果打分情况，评估模型表现。\n当然，实操方面其实也不复杂，就是多看几个测试集的结果，记录下来。如果模型优化后，在各个测试集上的提升是基本一致的，那就说明这次改进不是因为数据分布变化带来的，而是因为模型能力带来的。\n判断最优的参数和模型数据量配比 这件事不是特别值得做。因为我们当前模型的优质数据不够多。所以提供的数据质量是不稳定的。小模型上得到的预测数据值，在大模型上操作时，肯定不能按预测量来操作，而是还需要进一步增加数据量。但是具体增加多少，因为我们对数据质量无法在训练前得到评估，所以是不可预测的。\n这件事值得做的条件是：我们已经用一份数据训练了一个很大的模型，然后我们可以通过抽样的方法构建小模型，用大模型预测小模型需要多少数据量，这件事是可行的。\n当然，如果只是一个预估值做参考，这件事倒是可以做一下。\n注：这件事值得做的数学理论基础是：我们需要找到样本的精度和模型训练的流型精度一致的对应比例。这件事的前提条件是：模型得到充分训练，且 Loss 与 样本、模型精度是同一个数量级（Loss 就是当前的精度）。如果这个精度不一致，loss 会被更大的精度所制约。带来的影响是会增加一定的无效计算量。\n理论上，这件事更应该用适合的停机算法来避免冗余的计算，而不是需要精准的预估精度。\n尝试用更小的模型达到更优的效果 这件事价值不是特别大。\n不需要知道具体的比例，我们也知道，哪怕对于小模型，喂更多的数据可以达到更好的效果。 小模型的表达能力是有限的，所以也不是喂更多的数据就一定可以提升效果。 于是哪怕做出了预估，也需要加好多限制条件，而实际应用场景也不多。\n其他？ 昨天看完后，原本想说 Scaling Law 是个显然的结果，其规律并不蕴含更深层次的信息。但后来仔细想了想，可能还是有很多细节值得仔细的表述一下，以免遗漏什么可能性，所以写了这个文档。\n总得来说，我对于 Scaling Law 并没有想到更深的应用场景，它所能表达的大概也只是：更多的数据、更大的模型（更多的模型参数）可以更好的拟合真实的概率分布。这件事对于机器学习来说，是自然的结论。这个规律几乎不涉及具体的模型形式——几乎只要是机器学习都符合这个规律。\n所以从第一性原理角度出发，它算是一个数学上给出定性的存在性定理：我们的机器学习是可以不断优化的。但它不蕴含如何能更好地做优化的信息。\n这个定理的前置条件和证明过程这里就不赘述了，需要的话自己查一下。 ↩︎\n大部分的训练，增加迭代次数的方式都伴随着提供更多的训练样本，若模型距离收敛所需要的迭代次数比较多，例如如果 学习律（本质上就是 PID 中的 $K_p$）比较小，模型距离理论上限比较远，这时误差项主要不是来源于数据自身的误差，而是来自梯度下降逼近的误差，那么这个指数关系就会比较显著，对应的表象就是误差同数据量之间是指数关系。 ↩︎\n还可以增加每层的神经元数量（宽度），这种增加模式就不属于分层。类似于 Scaling Law 的规律，这种扩大的方式（形状变化）对于结果的影响不显著。当然，这件事是值得做实验，试一试少层数多神经元和多层数少神经元（参数总数一致）训练的结果是否一致。盲猜会有显著性能差异。 ↩︎\n","wordCount":"4758","inLanguage":"zh-cn","datePublished":"2023-10-10T11:50:00+08:00","dateModified":"2023-11-02T13:57:35+08:00","author":{"@type":"Person","name":"癸老师"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.uglyboy.cn/posts/2023-10-10/"},"publisher":{"@type":"Organization","name":"拾柒读库","logo":{"@type":"ImageObject","url":"https://blog.uglyboy.cn/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.uglyboy.cn/ accesskey=h title="拾柒读库 (Alt + H)">拾柒读库</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.uglyboy.cn/ title=主页><span>主页</span></a></li><li><a href=https://blog.uglyboy.cn/slides/ title=演示><span>演示</span></a></li><li><a href=https://blog.uglyboy.cn/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://blog.uglyboy.cn/tags/ title=标签><span>标签</span></a></li><li><a href=https://blog.uglyboy.cn/archives/ title=时间轴><span>时间轴</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Scaling Law 的数学解读</h1><div class=post-meta><span title='2023-10-10 11:50:00 +0800 CST'>2023-10-10</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;癸老师</div></header><div class=post-content><h2 id=dataset-size-和-loss-的关系>Dataset Size 和 Loss 的关系<a hidden class=anchor aria-hidden=true href=#dataset-size-和-loss-的关系>#</a></h2><h3 id=最大似然估计mle>最大似然估计（MLE）<a hidden class=anchor aria-hidden=true href=#最大似然估计mle>#</a></h3><p>一切机器学习的本质都是最大似然估计：</p><ol><li><p>模型下的理想真实世界的概率分布：$p(x|\theta)$</p></li><li><p>我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\theta|x)$</p></li><li><p>现在 $x$ 已知，$\theta$ 未知，若对于两个参数 $\theta_1$ 和 $\theta_2$ 有</p><p>$$
L(\theta_1|x) = p(x|\theta_1) > p(x|\theta_2) = L(\theta_2|x)
$$</p><p>那么意味着 $\theta=\theta_1$ 时，随机变量 $\theta_1$ 生成 $x$ 的概率大于当参数 $\theta=\theta_2$ 时。这也正是似然的意义所在，若观测数据为 $x$，那么 $\theta_1$ 是比 $\theta_2$ 更有可能为分布函数的参数。</p></li><li><p>在给定观测数据集 $X={x_n},n \in \mathbb{N}$ 时，真实世界最有可能的概率分布对应的参数 $\hat\theta$ 应该满足：</p><p>$$
L(\hat\theta|x) = p(x|\hat\theta) > p(x|\theta) = L(\theta|x), \theta \in \mathbb{\Theta} 且 \theta \ne \hat\theta
$$</p><p>即：</p><p>$$
\hat\theta = \arg\max\limits_\theta L(\theta|x)
$$</p></li><li><p>求解最大似然函数：</p><p>$$
\frac{\mathrm{d}}{\mathrm{d}\theta} L(\theta|x) = 0
$$</p></li></ol><p>对这个方程数值求解的过程，对应的就是绝大部分机器学习算法中的梯度下降过程。</p><p>在测试集上评估的结果，我们预想的误差应当包含两部分：</p><ol><li>似然函数 $L(\theta|x)$ 对真实世界概率分布描述能力不足，带来的误差；</li><li>通过 $X$ 估计 $\theta$ 时，样本本身的误差；</li></ol><p>若假定我们可以通过梯度下降收敛（即上面最大似然函数的导数在 0 的一个很小的临域中），那么至少就是我们相信在观测数据集 $X$ 上，模型是正确的，那么评估的误差就更加明确的指向 $X$ 本身带来的误差。</p><h3 id=fisher-信息量>Fisher 信息量<a hidden class=anchor aria-hidden=true href=#fisher-信息量>#</a></h3><p>为了求解最大似然估计，我们常用的数值手段是：</p><p>假定观测数据集 $X$ 的真实世界概率对应的概率密度函数是 $f(x_i;\theta)$，定义似然函数：</p><p>$$
L(X;\theta) = \prod \limits^{n}_{i=1} f(x_i;\theta)
$$</p><p>求解时，先对 $L(X|\theta)$ 取对数，再求导，这个函数定义为 Score function：</p><p>$$
S(X;\theta) = \sum \limits^n_{i=1} \frac{\partial \ln f(x_i;\theta)}{\partial\theta}
$$</p><p>则 Fisher 信息量的定义就是这个 Score function 的二阶矩（second moment）</p><p>$$
I(\theta) = E[S(X;\theta)^2]
$$</p><p>Fisher 信息量最重要的意义是：通过中心极限定理，弱大数定律，依概率一致收敛，以及 Slutsky 定理，可以证明 MLE 的渐进分布是正态分布 <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，即：</p><ol><li>$\hat \theta \stackrel{P}{\longrightarrow} \theta_0$，其中 $\theta_0$ 是参数的真实值；</li><li>$\sqrt{n}(\hat\theta - \theta_0) \stackrel{L}{\longrightarrow} N(0,I^{-1}(\theta))$ ;</li></ol><h3 id=数据量与误差的关系>数据量与误差的关系<a hidden class=anchor aria-hidden=true href=#数据量与误差的关系>#</a></h3><p>花了大量篇幅描述了 最大似然 和 Fisher 信息量后，最终真正值得我们关注的结论却异常的简单：</p><p>$$
L(D) \propto D^{-0.5}
$$</p><p>这个结论同计算均值时，数据样本带来的误差是完全一样的。</p><p>真实的机器学习条件下，我们的样本量的质量并不均匀，所以往往会优先使用更好的样本（小样本集不是大样本集的随机采样，而是精选），会导致观测数据集 $X$ 不能满足概率同分布，所以带来的结果是上述幂律关系中，实际的幂律值会小于 $0.5$。</p><p>理论上来说，如果我们能做到样本集随机采样，那样这个幂律就会更加接近 $0.5$，而如果样本集不能随机采样，某种意义上说，能否保持这种幂律关系是值得怀疑的。所以对于 OpenAI 和 Google 的 Scaling Law 的论文，在样本量同 Loss 的关系上，Google 的结果是更可信的。</p><p>哪怕依旧能维持幂律关系（维持幂律关系的数学基础是不存在的。。。），具体的数值也只能通过实际拟合来估计。因为这件事不是通用规律，只跟具体的训练数据集的分布有关，跟模型无关（前提条件是模型能在<strong>大数据</strong>下<strong>收敛</strong>，即满足大数定律、中心极限定律，并且模型可以拟合真实分布）。</p><h2 id=compute-和-loss-的关系>Compute 和 Loss 的关系<a hidden class=anchor aria-hidden=true href=#compute-和-loss-的关系>#</a></h2><h3 id=控制论和-pid-算法>控制论和 PID 算法<a hidden class=anchor aria-hidden=true href=#控制论和-pid-算法>#</a></h3><p>梯度下降法的数值计算过程，某种视角下可以理解成就是控制论下的控制算法——我如何根据真实信息来控制我的预期值离目标值更近。</p><p>直观而好用的方法就是 PID 算法：</p><p>$$
u(t) = K_pe(t) + K_i\int^t_0 e(\tau)\mathrm{d}\tau + K_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}
$$</p><p>当然，我们的梯度下降法原没有 PID 算法如此之精密，实际流程大概率只使用了 P 的部分，也就是对误差做补偿。在深度学习中，被称为反向传播。</p><h3 id=单参数计算量与误差的关系>单参数计算量与误差的关系<a hidden class=anchor aria-hidden=true href=#单参数计算量与误差的关系>#</a></h3><p>单目标的 PID（只省 P 过程了）算法，误差与计算量（迭代次数）之间的关系：</p><p>$$
L(C) \propto K_p^{C}=e^{\lambda C}
$$</p><p>即，误差同计算量之间的关系是指数关系，不是幂律关系。</p><p>这一点在 <a href=https://arxiv.org/abs/2206.14486>Sorscher et al. (2022)</a> 中有所体现，它的结论是：至少对于某些任务，损失可以随着数据集 <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> 大小呈指数级增长，而不是作为幂律。</p><h3 id=总计算量与误差的关系>总计算量与误差的关系<a hidden class=anchor aria-hidden=true href=#总计算量与误差的关系>#</a></h3><p>不同于优化问题中，我们会通过反复迭代的方式增加计算量，深度学习的计算量基本上是同模型规模和数据量正相关的。反过来意味着对单参数的优化迭代很少的固定步数就可以收敛，所以在通常数据量规模下，可以将单参数计算量带来的优化效果视作常数（都能优化到收敛）。</p><p>单参数计算量带来的优化效果视为常数（不会随计算量、节点数、数据量变化而变化），意味着计算本身同误差之间没有直接关联，总计算量与误差之间的关联体现的是数据量与误差的关系和节点数（结构）与误差的关系。</p><p>总计算量与数据量成正比，而数据量同优化效果之间的关联我们已经在前文完成了论述。下一步我们将分析节点数和误差之间的关系，或者其实更加精确的说，应当是在单参数误差不变的条件下，节点数的变化与总计算量之间的关系，是这个关系蕴含了总计算量与误差之间的关联。</p><h2 id=compute-和-parameters-的关系>Compute 和 Parameters 的关系<a hidden class=anchor aria-hidden=true href=#compute-和-parameters-的关系>#</a></h2><h3 id=分形维度>分形维度<a hidden class=anchor aria-hidden=true href=#分形维度>#</a></h3><p>具有自相似性的结构就是分形。而我们的深度学习计算就是典型的分形结构——当模型规模扩大时，主流的扩大的方式就是增加层数 <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>，这带来的就是自相似性。</p><p>而自相似性带来的重要性质就是，系统会具有分形维度，分形维度会使得系统规模扩大时，对应的全局属性并不是等比增加，而是幂律增加，幂律的指数就是其分形维度。</p><blockquote><p>生物学中有重要的 $\frac{3}{4}$ 定律——生物随着重量的变大（原子数量的规模扩大），其相关的很多生物学特征，例如新陈代谢能力、血管长度、心跳、呼吸等等，并不与重量成正比，而是按照 $\frac{3}{4}$ 的幂律进行增长。
一个直观的理解，随着生物体长度增长，其体重会以幂律 $3$ 进行提升，而腿部的横截面则是幂律 $2$ 增长。所以生物的规模变大，就会带来腿部承受的压力不断变大，所以老鼠体型的动物的腿都很细，但大象规模的动物，腿都很粗；蚂蚁可以举起自身体重百倍的物品，但人只能举起和体重相仿的物品。这些都是因为规模变化带来的非线性，要求生物的动力学模型必须发生变化，而不能与小规模时一样。
类似的，在城市规模同城市中加油站、小超市、医院之类的城市核心建设之间，也存在着幂律增长的关系——相关幂律大约是 $0.85$。</p></blockquote><p>对应的，深度学习模型中，在保证单参数误差不变的条件下，Parameters 规模的增加所需要的 Compute 计算量的增加不是等比的，而是幂律的，而且这个幂律应当是小于 $1$ 的。</p><p>换句话说，计算量同损失之间的关系是伴生关系——计算量本身同损失是没有直接关联的。带来损失变化的根本原因不是计算不足，而是模型表达能力以及数据本身蕴含的信息带来的。</p><p>但因为这里的结论中，计算量与参数数量也是幂律关系，由前文，数据同损失也是幂律关系，如果参数数量同损失同样是幂律关系的化，那么计算量与损失也可以用幂律关系来表示。</p><h2 id=parameters-与-loss-之间的关系>Parameters 与 Loss 之间的关系<a hidden class=anchor aria-hidden=true href=#parameters-与-loss-之间的关系>#</a></h2><p>这里要分析的是参数量增加为何能带来 Loss 的降低。这是因为 Parameters 的增加，可以提升模型的表达能力，可以更好的拟合目标函数。也就是说，一个模型距离真值的误差（Loss），除了因为 Dataset 自身的误差外，还有一部分是模型距离 Dataset 所描述的最大似然函数的误差。</p><p>这部分要是详尽分析起来会很复杂，幸好已经有一些这方面的研究：<a href=https://arxiv.org/abs/2004.10802>Sharma et al. (2020)</a> 和 <a href=https://arxiv.org/pdf/2102.06701.pdf>Bahri et al. (2021)</a> 都对这个问题进行了很好的分析，其结果也有对应的实验支撑。</p><blockquote><p>文章假定深度模型将数据映射到一个 $d$ 维数据流形上，增加的模型参数（无限数据的条件下）都会被模型用来将数据流形分割成更小的组件，然后模型将在数据流形的每个分量上进行独立的预测，以优化训练损失。</p><p>这样自然的，如果我们想让子区域的大小缩小 $2$ 倍，就需要增加 $2^d$ 倍的数据量或模型参数。进而就是直观的结论：</p><p>$$
L(P) \propto P^{-\frac{1}{d}}
$$</p><p>即 Loss 与 参数量之间是幂律关系，其幂律值小于 $1$（因为有 $d>1$）。</p></blockquote><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>至此，关于 Scaling Law 的数学含义就已经基本都解释清楚了。</p><p>更重要的问题是，有了相关的理论支撑后，我们能做什么？哪些事情做不了。</p><h3 id=基于多份数据融合的实验结果预测>基于多份数据融合的实验结果预测<a hidden class=anchor aria-hidden=true href=#基于多份数据融合的实验结果预测>#</a></h3><p><strong>这件事是不可行的</strong>。</p><p>一切机器学习的基础都是最大似然估计，而最大似然估计的基础假设就是独立同分布。两组分布不同的数据融合，一定会破坏原有的分布，至于不同比例下融合后形成怎样的分布，具有怎样的特性，在两份数据的分布都已知的条件下，是可以计算的。但对于我们自己的机器学习任务，原本就是要去学习数据的分布，这就决定了，不可能在不了解数据分布的条件下，估计融合后的数据分布。</p><p>类似的，多分不同分布的数据集怎么融合能更贴近测试集也是不可知的，只能试出来。由于测试集也不是真值，甚至测试集对真实世界的表达很可能还不如训练集，所以针对测试集做针对性调优是不值得的。</p><p>这部分的定量分析，其实可以借鉴 OpenAI 关于 Scaling Laws 的经典文章 <a href=https://arxiv.org/abs/2001.08361>Kaplan et al. (2020)</a> 中尝试的方法：</p><blockquote><p>迁移学习与测试效果的提升：
当我们在与训练集分布不同的文本上评估模型时，其结果与训练验证集上的结果强烈相关，损失函数中有一个大致恒定的偏移量。换句话说，转移到不同的分布会带来一定的固定惩罚，但除此之外，其提升程度大致与训练集上的表现一致。</p></blockquote><p>可以用类似这样的方法，通过多份不同分布的测试集效果打分情况，评估模型表现。</p><p>当然，实操方面其实也不复杂，就是多看几个测试集的结果，记录下来。如果模型优化后，在各个测试集上的提升是基本一致的，那就说明这次改进不是因为数据分布变化带来的，而是因为模型能力带来的。</p><h3 id=判断最优的参数和模型数据量配比>判断最优的参数和模型数据量配比<a hidden class=anchor aria-hidden=true href=#判断最优的参数和模型数据量配比>#</a></h3><p>这件事不是特别值得做。因为我们当前模型的优质数据不够多。所以提供的数据质量是不稳定的。小模型上得到的预测数据值，在大模型上操作时，肯定不能按预测量来操作，而是还需要进一步增加数据量。但是具体增加多少，因为我们对数据质量无法在训练前得到评估，所以是不可预测的。</p><p>这件事值得做的条件是：我们已经用一份数据训练了一个很大的模型，然后我们可以通过抽样的方法构建小模型，用大模型预测小模型需要多少数据量，这件事是可行的。</p><p>当然，如果只是一个预估值做参考，这件事倒是可以做一下。</p><p>注：这件事值得做的数学理论基础是：我们需要找到样本的精度和模型训练的流型精度一致的对应比例。这件事的前提条件是：模型得到充分训练，且 Loss 与 样本、模型精度是同一个数量级（Loss 就是当前的精度）。如果这个精度不一致，loss 会被更大的精度所制约。带来的影响是会增加一定的无效计算量。</p><p>理论上，这件事更应该用适合的停机算法来避免冗余的计算，而不是需要精准的预估精度。</p><h3 id=尝试用更小的模型达到更优的效果>尝试用更小的模型达到更优的效果<a hidden class=anchor aria-hidden=true href=#尝试用更小的模型达到更优的效果>#</a></h3><p>这件事价值不是特别大。</p><ol><li>不需要知道具体的比例，我们也知道，哪怕对于小模型，喂更多的数据可以达到更好的效果。</li><li>小模型的表达能力是有限的，所以也不是喂更多的数据就一定可以提升效果。</li></ol><p>于是哪怕做出了预估，也需要加好多限制条件，而实际应用场景也不多。</p><h3 id=其他>其他？<a hidden class=anchor aria-hidden=true href=#其他>#</a></h3><p>昨天看完后，原本想说 Scaling Law 是个显然的结果，其规律并不蕴含更深层次的信息。但后来仔细想了想，可能还是有很多细节值得仔细的表述一下，以免遗漏什么可能性，所以写了这个文档。</p><p>总得来说，我对于 Scaling Law 并没有想到更深的应用场景，它所能表达的大概也只是：更多的数据、更大的模型（更多的模型参数）可以更好的拟合真实的概率分布。这件事对于机器学习来说，是自然的结论。这个规律几乎不涉及具体的模型形式——几乎只要是机器学习都符合这个规律。</p><p>所以从第一性原理角度出发，它算是一个数学上给出定性的存在性定理：我们的机器学习是可以不断优化的。但它不蕴含如何能更好地做优化的信息。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>这个定理的前置条件和证明过程这里就不赘述了，需要的话自己查一下。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>大部分的训练，增加迭代次数的方式都伴随着提供更多的训练样本，若模型距离收敛所需要的迭代次数比较多，例如如果 <code>学习律</code>（本质上就是 PID 中的 $K_p$）比较小，模型距离理论上限比较远，这时误差项主要不是来源于数据自身的误差，而是来自梯度下降逼近的误差，那么这个指数关系就会比较显著，对应的表象就是误差同数据量之间是指数关系。&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>还可以增加每层的神经元数量（宽度），这种增加模式就不属于分层。类似于 Scaling Law 的规律，这种扩大的方式（形状变化）对于结果的影响不显著。当然，这件事是值得做实验，试一试少层数多神经元和多层数少神经元（参数总数一致）训练的结果是否一致。盲猜会有显著性能差异。&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.uglyboy.cn/tags/scaling-law/>Scaling Law</a></li><li><a href=https://blog.uglyboy.cn/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://blog.uglyboy.cn/posts/2023-10-18/><span class=title>« 上一页</span><br><span>Hugo 搭建流程</span>
</a><a class=next href=https://blog.uglyboy.cn/posts/autorestic/><span class=title>下一页 »</span><br><span>Autorestic</span></a></nav></footer><div id=vcomments></div><script>new Valine({el:"#vcomments",appId:"7dZMqcgsP9ytL8PRta4yxE82-gzGzoHsz",appKey:"OZC3GQ22dNzPoeRiFcpbuplk",placeholder:"来都来了，说两句~"})</script></article></main><footer class=footer><span>&copy; 2023 <a href=https://blog.uglyboy.cn/>拾柒读库</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制代码";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制代码"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>