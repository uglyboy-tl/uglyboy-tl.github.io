<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>文章 on 拾柒读库</title>
    <link>https://blog.uglyboy.cn/posts/</link>
    <description>Recent content in 文章 on 拾柒读库</description>
    <generator>Hugo -- 0.143.1</generator>
    <language>zh-cn</language>
    <atom:link href="https://blog.uglyboy.cn/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>大模型的推理能力</title>
      <link>https://blog.uglyboy.cn/posts/2024/11/06/</link>
      <pubDate>Wed, 06 Nov 2024 18:44:15 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/11/06/</guid>
      <description>对推理能力的一个科学描述</description>
      <content:encoded><![CDATA[<h2 id="引言">引言</h2>
<p>自 GPT3.5 引爆大模型概念以来，大家都期盼着 AGI<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 的到来。但与此同时，当下各类大模型虽然依据 Scaling Law<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，不断提升各方面的性能，但是关于模型的推理能力，总显得不足。</p>
<p>甚至科研界针对大模型究竟是否可能具有推理能力，也争论不休。例如下列近期比较热烈的讨论：</p>
<ul>
<li>DeepMind 的 <a href="https://arxiv.org/html/2402.04494v1">Grandmaster-Level Chess Without Search</a></li>
<li>Apple 的 <a href="https://arxiv.org/abs/2410.05229">GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models</a></li>
<li>MIT 的 <a href="https://www.nature.com/articles/s41586-024-07522-w">Language is primarily a tool for communication rather than thought</a></li>
<li>Google Brain 的 <a href="https://arxiv.org/abs/2402.12875">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a></li>
<li><a href="https://www.strangeloopcanon.com/p/what-can-llms-never-do">What can LLMs never do?</a></li>
</ul>
<p>虽然笔者之前的文章已经用比较直观的方式证明了 LLM 的基本范式 <code>N-GRAM</code> 是一个有穷自动机，无法像通用图灵机一样完成各种任务，甚至简单的四则运算也做不到，从而侧面证明了 LLM 不可能具有推理能力。但是“究竟什么是推理能力？”这个话题，之前并没有进行过完整的阐述。</p>
<p>本文将对什么是推理能力这个话题进行深入的说明，进而将 LLM 未来如何能实现 AGI 的路线图进行一个完整而具体的展现。</p>
<h2 id="推理逻辑算法图灵机">推理、逻辑、算法、图灵机</h2>
<h3 id="推理和数理逻辑">推理和数理逻辑</h3>
<p>研究推理能力，那首先需要有一个“推理”的明确的说明或定义。推理这个概念来自于逻辑学，古早的逻辑学更偏向于哲学，但现代的逻辑学已经是数理逻辑的一部分了。</p>
<p><strong>数理逻辑</strong>（Mathematical logic）是用数学方法研究诸如推理的有效性、证明的真实性、数学的真理性和计算的可行性等这类现象中的逻辑问题的一门学问。现在公认的数理逻辑创始人是莱布尼兹。他的目的是选出一种“通用代数”，其中把一切推理都化归为计算。</p>
<blockquote>
<p>更为具体的解释：我们希望研究的是“正确的推理过程”，而数理逻辑的目标是将推理过程变成一种计算过程（算法），进而通过计算的结果来判断推理的正确性。</p></blockquote>
<p>这个愿望被布尔代数<strong>基本</strong>实现了——我们可以将任何数学语句或自然语言的命题翻译为为布尔代数的逻辑表达式，然后对表达式进行判定是否为真。当然，这个实现是不完美的，罗素在数理逻辑中发现了悖论；而哥德尔证明了悖论是一切逻辑体系中必然存在的命题——也就是说，一定存在我们无法判定是否为真的逻辑表达式。</p>
<h3 id="计算和图灵机">计算和图灵机</h3>
<p>有了数理逻辑这种工具，推理能力被转化为了计算能力。可计算能力又如何衡量呢？在另一条时间线上，一个叫图灵的“年轻人”在研究“算法”的能力，而这条线恰好将计算能力的清晰的描述了出来。</p>
<p>图灵提出了一种通用图灵机，并证明了当前人类能想象到的一切算法，都可以用通用图灵机实现。</p>
<blockquote>
<p>当然，他的副产物：有穷自动机、上下文无关文法等算法模式，能力是弱于图灵机的。这也是笔者用于证明当前的 LLM 不具有推理能力的重要工具。</p></blockquote>
<p>于此同时，图灵还深入研究了图灵机进行计算的可判定性，也就是停机定理：存在这样的图灵机，我们无法判断它是否陷入了无限循环，能否停机。</p>
<p>非常有趣的是，停机定理的证明和歌德尔不完备定理的证明思路很类似，都是将一切命题（算法）转化为一串数字，然后再证明这其中的不合理性。</p>
<blockquote>
<p>甚至，我们可以直接通过停机定理来证明歌德尔不完备定理：
我们将数理逻辑体系下的逻辑命题转化为一个计算问题，然后通过算法对这个问题计算。于是逻辑命题能否判定就变成了判断这个算法的结果或者判定这个算法无法停机（也就是这是个逻辑悖论）。
但由于停机定理的存在，我们并不能对一切逻辑命题给出上面的判定，从而，一定有逻辑命题是我们无法给出任何判定的。</p></blockquote>
<h3 id="计算机和可编译">计算机和可编译</h3>
<p>上面的论述是理论层面的描述，我们需要一个针对真实“推理”问题的可操作的工具。幸好还有一个人，跟图灵所处同一时代，用另一种方式真实实现了“通用图灵机”，并且它还成为了人类进步的重要工具：计算机之父——冯·诺依曼。</p>
<p>我们今天的手机、电脑等等各种电子产品，基本都是冯·诺依曼架构下实现的。而今天的程序员，则不断将图灵理论中的概念算法，转换成真实世界中不断运行中的程序。</p>
<p>而这其中，刨除停机定理中描述的无法判定的算法问题，绝大部分算法能否执行的重要判定标准是：<strong>可编译</strong> <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。</p>
<p>从而最终，我们有了关于推理的全部知识：<strong>一个正确的推理，是将其所有逻辑过程通过数理逻辑相容的编码进行描述，然后看该编码能否通过其对应的编译器的编译</strong>。</p>
<ul>
<li>Lean4 就是一门进行数学推理的可编译代码。</li>
<li><a href="https://arxiv.org/abs/2405.18357">Faithful Logical Reasoning via Symbolic Chain-of-Thought</a> 这篇文章介绍了一些外部推理工具如何帮助大模型进行无谬误的推理，并提出了一个新的推理编码（符号逻辑推理）。</li>
</ul>
<h2 id="提升推理能力">提升推理能力</h2>
<p>上一节中，我们有了清晰的定义，知道了什么叫做“推理”以及“可验证的正确推理”。那么什么是推理能力呢？</p>
<h3 id="推理过程的数学描述">推理过程的数学描述</h3>
<p>针对真实世界的推理问题，其基本范式都符合如下的描述：</p>
<ol>
<li>我们有一些已知条件或者背景信息（知识）；</li>
<li>我们有一个预期的结果：
<ul>
<li>可能是类似数学证明题，我们有一个命题，希望知道其是否正确；</li>
<li>可能是类似规划问题，虽然我们不知道最终的答案，但我们可以验证怎样的答案符合我们的预期；</li>
<li>也可能是类似计算机竞赛的算法问题，我们不仅仅要求答案符合预期，还期望计算的过程更快，使用更少的资源等等；</li>
</ul>
</li>
<li>我们的推理过程其实可以看作是一个检索过程——在已知信息和目标结果之间寻找并补充适合的逻辑代码，让其可通过我们所需要的推理编译器。</li>
</ol>
<h3 id="推理能力">推理能力</h3>
<p>当前大模型欠缺的，正是上面描述的推理过程的能力 <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>。</p>
<p>笔者以前文章中给出过结论：通过强化学习的 Agent 模式可以实现通用图灵机，进而实现 AGI。今天用新的关于推理的数学定义，再次推演出这个结论：</p>
<blockquote>
<p>强化学习（Reinforcement learning，RL）讨论的问题是一个<strong>智能体 (agent)</strong> 怎么在一个复杂不确定的 <strong>环境 (environment)</strong> 里面去极大化它能获得的奖励。通过感知所处环境的 <strong>状态 (state)</strong> 对 <strong>动作 (action)</strong> 的 <strong>反应 (reward)</strong>， 来指导更好的动作，从而获得最大的 <strong>收益 (return)</strong>，这被称为在交互中学习，这样的学习方法就被称作强化学习。
它的一个巨大优势，就是可以将多步之后的奖励，拆解反馈到很多步以前的状态中，从而可以更快的学习到重要的经验。
这一点和我们进行推理过程时所需要的能力是完全一致的：因为我们的推理过程的最终结果是可验证（可编译）的，通过时可以给予一个巨大的奖励。而强化学习的过程就是不断通过后验的奖励，让系统在下一次时更快更好的找到适合的推理过程。</p></blockquote>
<p>甚至可以给出结论：推理能力提升的过程，就是一个标准的强化学习过程 <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>。</p>
<h2 id="技术瓶颈">技术瓶颈</h2>
<p>从过去一年里 LLM 的发展来看，推理能力的进展无疑是缓慢的。甚至应该也有其他的研究者摸到了所谓“推理为何物”的基本认知，但依然在解决这个课题的方面，没看到什么显著突破。</p>
<p>核心的技术瓶颈，在于两个方面：</p>
<ol>
<li>对推理过程的符号化描述 <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>；</li>
<li>强化学习中对非确定型状态空间 <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> 的描述和建模。</li>
</ol>
<h3 id="推理的符号化描述">推理的符号化描述</h3>
<p>如果是从数学角度来看，推理的符号化描述的工具已经是完备的了。但从真实应用出发，却没那么容易将我们人类知识中的大量推理过程进行符号化表达。</p>
<p>这里的最核心的难点，不在于符号体系本身，而是人类有大量蕴含推理知识的表达，还没有用符号体系进行描述，于是很多人类的推理过程在转译成符号化描述时，是不完备甚至缺失的。</p>
<blockquote>
<p>这一点在 Lean4 语言中非常明显。理论上，Lean4 是完备的，可以进行各种数学推理。但是真正用起来的时候，却会发现基本没法用——因为人类的大量证明过程中，会用到很多理所应当的定理，比如理所应当的加法交换率，分配率，可被整除等等。但这些概念在数学中其实并不是公理，也不是基础概念，是需要从集合论一直证明到自然数是一个阿贝尔环，才能使用的逻辑前提。
所以 Lean4 中缺失的不是逻辑，而是很多基础推导过程的证明。而这种问题存在于所有的逻辑符号化编码中。</p></blockquote>
<p>人类的知识蕴含在语言中，而语言的进化又蕴含了非常多的逻辑知识。在这些基本的知识没有被全面的符号化描述前，任何符号化描述的推导过程都是不完备的。</p>
<p>而另一方面，我们无论是尝试用 LLM 来学习这种 符号化描述的表达，还是用强化学习，都需要大量的数据作为食材。而更不幸的时，我们连推理的符号化描述体系都还没统一和建立起来，更不用提所需要的大量学习样本了。</p>
<p>所以短期内快速的提升人工智能的推理能力，从数据和表达的层面来看，都是基本不可能的。</p>
<h3 id="强化学习方面的瓶颈">强化学习方面的瓶颈</h3>
<p>强化学习方面，我们的工具库也是不足的。</p>
<p>例如针对下面的命题，我们是有能力构建完善的符号化描述和样本库的：</p>
<ul>
<li>自然数（实数）加法的运算规则。</li>
</ul>
<p>我们希望用强化学习的方式，从足够多的样本中学习出加法的运算规则。但面对这个命题时，就会发现，如何在强化学习中表达这种非确定型的命题的状态空间和动作空间是一个巨大的难题。</p>
<p>如果单说解决这个问题，给出一些例如操作存储、按位移动等操作类型，或许也可以表达动作空间和状态空间，但是这种方式基本上是对着答案设计动作空间。针对更广泛的各种符号化描述的真实问题，如何描述状态空间和动作空间，并能进行有效的计算？是一个实打实的难题。</p>
<p>甚至相比于强化学习模型，LLM 模型可以在更长的序列中进行 token 的概率预测，甚至状态空间和动作空间都是 token 表达的泛化类型，这也使得 LLM 在统计已有的数据的奖励权重方面，其泛化性和易用性都是强于当下的强化学习模型的。</p>
<p>那有没有办法让强化学习也可以利用长序列编码的方式表达状态和动作空间，并用类似于 LLM 模型的方式来计算激励函数呢？看起来是个可行的路线，但似乎依然需要先有更适合的关于状态空间的符号化表达，以及全新的一套强化学习机制，来更通用的适应各种不同的非确定型的真实问题。</p>
<h2 id="结论">结论</h2>
<p>短期内 AGI 不可实现。而 LLM 的能力基本已到瓶颈，一段时间内大家的竞争将更聚焦在应用层面。</p>
<p>如果想尝试探索提升 LLM 的推理能力，那最重要的两个方向：</p>
<ol>
<li>推理的符号化表达，以及构建更多的符号化表达的数据（这里或许可以用 LLM 来帮忙生成或转译）；</li>
<li>更强大的强化学习范式。</li>
</ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>通用人工智能&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>大模型的尺度规则：随着参数规模的扩大，模型能力更强&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>甚至如果将回归测试也看作是编译的规则，那么对算法进行一定程度的准确性判断也可以通过编译的方式来实现。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>甚至“能力”一词表达的，都不是推理过程的效率高低，而是推理过程能成功的概率。&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>这也是现在包括 o1 在内的各种推理模型背后的核心原理。当然，经验不沉淀在强化学习模型中，而是沉淀在基座模型中，无疑会损失很多推理的能力。&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>其实叫做编码也可以，但容易跟传统的编码概念混淆，毕竟推理所要求的编码是可编译的；而 LLM 要求的编码只要有可预测性即可。&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>这种表述也许有更专业的表达。笔者想说的是，强化学习中动作空间当下是未知的，只有动作出现后，我们才会知道原来还有这样的动作；但是理论上最终的动作空间总是有办法可以被全部表达出来的，也许表达的方式是某个动作编码的判别器。所以这里或许需要有新的对动作空间的数学描述方式，才有可能用传统的强化学习的方式来枚举所有的动作空间。&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>编码的意义</title>
      <link>https://blog.uglyboy.cn/posts/2024/09/18/</link>
      <pubDate>Wed, 18 Sep 2024 10:38:40 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/09/18/</guid>
      <description>大模型发挥能力被忽视的要素</description>
      <content:encoded><![CDATA[<h2 id="导言">导言</h2>
<p>伴随着大模型研究的推进和在应用中的实践，我们发现了一个现象——对于现有的 LLM 模型而言，一个好的编码可能会对其模型能力带来极大的助力。</p>
<p>关于这方面的思考其实由来已久，早前在听闻“压缩即智能”的论断，以及相关的数学阐述时，就产生过一种奇妙的念头：</p>
<ul>
<li>如果我们对我们正常的语言进行编码压缩，这样得到的文本信息的压缩率会更高，对压缩过的文本再进行 LLM 的训练，会发生什么？</li>
<li>LLM 生成的编码还能被正常的解压缩成文本吗？解压缩之后的文本还能流畅通顺吗？</li>
<li>这样训练出的 LLM 会不会压缩率更高？那它会更加智能吗？</li>
</ul>
<p>后来，在搞清楚了“压缩即智能” 那篇演讲，背后所提到的<strong>压缩</strong> 概念，不过是<strong>算术编码</strong>的基础运用后，这种思考便放下了 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>现在重新聊回编码问题，从编码的角度来看看 LLM 的本质。</p>
<h2 id="信息和熵">信息和熵</h2>
<p>回归到 LLM 的模型范式，过去的研究主要都将重点集中在了 Transformer 和深度学习的神奇泛化能力方面，通过更大规模的训练（Scaling Law），降低泛化误差。但究其本质，训练的逼近只是一个数值优化（压缩映射）的过程，优化的目标才是这件事起到作用的关键。</p>
<h3 id="llm-更应该叫大编码模型"><strong>LLM</strong> 更应该叫大编码模型</h3>
<p>LLM 是信息预测模型：利用时序上已知的信息，对下一个时刻出现的信息进行预测。</p>
<p>但信息本身是需要被表征出来，才能进行读取（理解）和预测，而信息的表征方式就是<strong>编码</strong>。</p>
<blockquote>
<p>多模态的大模型相比于文本大模型本质上没有任何不同之处，仅仅是表达信息的编码不同，认为多模态比基于文本的大模型拥有更丰富的信息这一点是存疑的。但这一点还不是多模态大模型最大的问题。</p>
<p>另一个十分重要的因素，就是多模态的编码相比于语言这种编码，在进行预测时很可能天然就有十分巨大的劣势。这一点将在后文中引入了信息度量：“熵”之后，进行更深入的讨论。</p></blockquote>
<p>大模型的内核，正是基于已有的编码的信息，预测新的编码。预测的具体内容，是新编码出现的概率。</p>
<p>这里会有一个思考，不同的编码方式，是否会对预测本身的质量产生影响呢？</p>
<blockquote>
<p>从实践的角度来看，是有很大影响的。所以大语言模型都会有自已的 embedding 算法，而不是直接使用字母表或者 unicode 编码进行分析。</p>
<p>但我们如何从理论的解读来理解和分析这个结果呢？我们很需要引入一个能更好的描述编码相关特性的物理量。</p></blockquote>
<p>而在数学中刻画信息、编码、概率的模型是香农的信息论。其中最深刻的刻画就是信息熵。</p>
<h3 id="熵">熵</h3>
<p>熵的概念，本质上是一个系统里单位时间内的信息密度，或者说，是我们对未来时刻进行预测的难度。</p>
<p>所以自然的，一个时刻出现的概率非常的低，那么这一刻我们对未来的预测就会非常的难；同时因为熵是个可描述的物理量（具有可加性），自然就会引出其常见的定义方式：</p>
<p>$$
H = - \sum p \ln p
$$</p>
<p>在这个意义下，Boltzmann 熵、Gibbs 熵 和 香农的信息熵是一致的。于是很容易引入一个十分热门的概念：<strong>热力学第二定律</strong>。</p>
<h4 id="热力学第二定律">热力学第二定律</h4>
<blockquote>
<p>[!tip] 热力学第二定律</p>
<p>在不受任何外部影响的条件下，一个系统的熵会不断增加。更具体的说，就是系统中的每个个体都会走向更加随机（概率更小），导致整个系统的可预测性越来越低。</p></blockquote>
<p>需要注意的是，热力学第二定律并不是一个完美的时序判别器，它是一个概率性的时间方向指引：</p>
<ul>
<li>也就是说可能会存在很多个瞬间，系统的熵减少了；</li>
<li>甚至一定存在某个很极端的概率事件——系统的熵一直在减少；</li>
</ul>
<p>但这些都是概率空间上的正常表现，宏观视角来看，熵的增加是稳定的。</p>
<p>在这种情况下，我们很容易发现，信息几乎是不可预测——随着信息中的要素的增加，想要预测下一时刻的信息所需要的信息量会按照热力学第二定律，不断增加，直至不可计算 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。</p>
<blockquote>
<p>为了更好的理解熵和不可计算，我们以三体问题为例。</p>
<p>我们要想预测三体未来一小段时间的运动轨迹，所需要的信息是在这一时刻前更长一段时间的历史三体运动信息，获得的历史运动信息越多越精准，对未来的预测就能越精准，预测正确的时间就会更长一些。但永远没办法做到像计算行星轨道一样，给出公式性的结果，这种情况就属于不可计算。</p>
<p>从三体出发，如果想预测四体，需要比三体更加庞大的数据量才足以支撑相同时间长短的预测。随着 $n$ 体中 $n$ 的不断增长，我们能够有效的预测的时间就会越来越小。</p>
<p>这种现象背后的数学基础是动力系统中的混沌现象，本文不对这个问题进行更深入的解读，对此有兴趣可以参考我的《系统理论》进行理解。</p></blockquote>
<h3 id="编码">编码</h3>
<p>编码不是物理学自然的结果，编码是随着生物的出现而出现的。更聚焦的说，编码是生物体对于一些固定特征合集的概括。尤其是，编码的出现，其实意味着某些固定特征合集的重复出现（高概率）。而从信息论出发，对于高概率的事件，通过编码（更短的信号）传递，就可以更高效的表达信息。</p>
<p>编码的出现其实也意味着人们对信息的控制（熵减）——因为编码的表达（例如动作，叫声）本身，也是编码的一部分，都属于某种小概率的行为。但当这种行为被赋予编码的含义，反过来也意味着这种行为的出现频率将会变高，于是这种行为的熵变小，进而可预测性提升。</p>
<p>编码不仅仅是对客体的描述，也是对客体的控制，进而在局部可以逆转 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> 热力学第二定律。因为<strong>编码意味着一种控制行为，也就带来了可预测性的提升</strong>。</p>
<p>这个结论也昭示了本文的核心观点：好的编码才是 LLM 进行预测的关键。</p>
<blockquote>
<p>这也是为何 LLM 最早在 NLP 和 Coding 领域取得成功，因为这两者背后都依托于非常良好的编码。</p></blockquote>
<h4 id="编码的质量">编码的质量</h4>
<p>有了编码的帮助，生物就可以更好的控制自身和外部环境之间的交互，让原本更加随机不可控的生存，变得更加可控了。而这种控制的方式，就是通过编码，让随机事件中的小概率变得不那么随机（更有确定性），系统的熵降低了，但被编码的事件（信息）的熵 <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> 增加了。</p>
<p>所以系统更加可预测 -&gt;系统的熵降低 -&gt;信息编码的熵增加。</p>
<p>其实这件事在香农的信息论里本来就是比较显然的结果——更好的编码是按照信息的熵值大小进行编码的。甚至所谓的“压缩即智能”中的算术编码，也是自然的香农信息论的编码结果。</p>
<h2 id="编码的演进">编码的演进</h2>
<p>如果客观世界是无法被改变的，那么最优的编码是固定的，几乎不存在编码的选择问题。</p>
<p>不过编码是主观的，是生物控制自身适应环境的工具。所以编码是不断演进的，甚至编码本身的演进就是一种控制客观存在的能力的提升（例如计算机代码）。</p>
<p>所以我们需要从演进的视角来观察编码变化带来的价值，进而了解我们需要怎样的编码来更好的进行预测和控制。</p>
<h3 id="一些编码演进的案例">一些编码演进的案例</h3>
<blockquote>
<p>[!example] 从成语看语言的演进</p>
<p>各种语言的演进都是从拟声词开始的，而且这种能力并不是人类所独有，很多生物都会有自己的一些独特的信息编码表达（包括不同的叫声，也包括用排泄物标识领地等等）。</p>
<p>相比于动物，人类的独特之处在于创造了更多的编码，并开始组合使用这些编码。于是很多简单概念的编码就连接起来，形成了句子，形成了语言。</p>
<p>有了语言以后，人们就有能力更加便捷的传播信息，于是很多信息打包在一起，形成了故事，再进行传播。而这里则又出现了一次重要的编码演化——当故事传播后，它自身就成了一个编码，蕴含了远比普通的拟声表达、简单词语、句子更加丰富的内容。</p>
<p>这种组合，在不同的语言不同的地域有不同的表现形式，有的叫典故、有的会形成歇后语、而在汉语中，运用了一种比较独特的编码形式——<strong>成语</strong>，用四个字的方式将大量的类似的故事进行编码和传播，进一步提升了汉语的熵。</p>
<p>并且有了成语这种编码，再回到一般的语言表达时，句子的信息含量也得到极大的提升，甚至可以通过多个成语，再次形成新的成语，不断演进这个过程。</p></blockquote>
<blockquote>
<p>[!example] <code>Python</code> 和 <code>JavaScript</code></p>
<p>单从语言效率的方面来评判，<code>Python</code> 和 <code>JavaScript</code> 都算不得是很优秀的编程语言。但近些年，这两种编程语言却超越了传统的 <code>C</code>、<code>C++</code> 和 <code>Java</code>，成为了最流行的编程语言，这背后也是编码演进的功劳。</p>
<p>首先，<code>Python</code> 和 <code>JavaScript</code> 语言最大的特点都是入门简单，于是先天就会都拥有大量的普通用户。普通的用户对于编程语言没有很专业的训练，更需要的是简单快速可实现的能力，于是正应对上了 <code>Python</code> 和 <code>JavaScript</code> 发展中最重要的能力——包管理。</p>
<p>一个 <code>Python</code> 项目或者 <code>JavaScript</code> 项目，往往都会伴随一个巨大的第三方依赖文件夹，用 <code>pip</code> 或 <code>npm</code> 在项目中引入了巨量的代码，但对用户而言，在使用编程语言时，对于大量复杂的功能的使用，则只需要简单的接口或者函数就能实现，而不再需要关心各种通用功能的细节，让普通用户也可以实现复杂而高级的能力。</p>
<p>这种包引用的方式，其实就是编程语言的一种演进——将大量复杂的信息组合后，用新的编码来替代，就可以让编码的熵得到提升，进而让普通人也可以使用上（预测出）更可控的编码来控制外部世界。这个例子的说明了，好的编码确实可以让编码的预测性变得更好。</p>
<p>更直接的例子，现在无论哪种编程语言，进行数学计算时，都会直接用数学表达式进行表达，而不会用汇编将每一次的加法进行各种位移操作。这样才使得我们操控编码变得更加有效。</p></blockquote>
<blockquote>
<p>[!example] 数学</p>
<p>数学的发展史是极度依赖数学符号（编码）的演进的。今天的人可以比古人更明白数理，数学符号（编码）功不可没。</p>
<p>数学符号发展的第一步是从数演变成数字——即用具体的编码来指代不同的数量，而不是用数量本身来指代数量。这种变化才能让人类对数量的认知从 $10$ 以内（手指的数量），扩充到更多。</p>
<p>有了数字后，下一个重要的演进则是用不同的位置来标识不同的数字，也就是我们现在熟悉的进位制。这之后，大量级的数字才有机会被统计和计算，人类才真正将数学变成一个工具。</p>
<p>然后是分数的概念 <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>。或者更准确的说法，是除法的出现和表达。人们终于可以一定程度上在连续的数域上进行管理和控制。</p>
<p>虽然第一次数学危机引入了无理数，对数学史意义重大，但从数学的发展来看，无穷的出现和编码，才是真正的突破。有了对于无穷的理解，才有了后来牛顿的微积分（牛顿对于微积分的最重要贡献也是微积分的符号），数学才步入了近代的发展。</p>
<p>这之后，人们愈发理解了数学符号对数学发展的重要性，数学符号的演进也是飞速的，不再成为数学发展的掣肘。</p></blockquote>
<blockquote>
<p>[!example] 技术的演进</p>
<p>技术的演进其实也是编码化的——每一种新的技术都会有自己的编码描述，然后各种新技术（编码）进行重组，又会形成新的编码，提升技术的熵值，让人们可以控制更加难以控制（自然发生的概率更小）的事物。</p>
<p>例如元素周期表的出现就极大推动了化工以及材料、健康、能源、农业等相关产业的发展。元素周期表之前，少数的化学家也可以掌握一些物质的性质和信息，但组合使用是需要极高的门槛的。有了元素周期表，有了化学物质的新命名方式，以及更容易计算的化学反应方程，对于普通高中生而言一般的化学变化都可以轻松掌握。</p>
<p>类似的，每一种技术的发展，其实背后都是对应的编码的演进过程。如前面 <code>Python</code> 和 <code>JavaScript</code> 的例子是这样；工业生产、供养链、产业带的演进等等都是这样的过程。所以最终技术的成功不依赖于最初技术编码设计的是否巧妙，而是在于技术编码演进的过程中，对应的编码是否逐渐拥有了更大的熵。</p>
<p>以当下人类工业的顶点——光刻机为例，组装光刻机的企业并不掌握全部的工业技术，而是掌握了很多的技术编码，对编码进行组装；而上游的很多企业也只掌握了特定编码的生产，甚至也是更低级编码的组装。但足够复杂的编码构成，最终才形成了现在的可以按纳米级控制生产原件的机器的诞生。</p></blockquote>
<h3 id="编码的进化">编码的进化</h3>
<p>我们如何进一步提升编码的信息熵，让编码变得更好？从上文的几个案例中，其实可以找到一些共通点。</p>
<p>编码是需要不断组合，进而形成更加高级的概念。由此不断的可以控制客观发生概率更小的事情。</p>
<blockquote>
<p>客观的物理世界中随机出现一个 <code>iPhone</code> 手机基本是不可能的。我们人类是如何通过编码的控制创造了批量制造 <code>iPhone</code> 的方式呢？</p>
<p>我们需要 <code>集成电路</code> 这个编码，需要 <code>电池</code> 这个编码，需要 <code>高清屏幕</code> 这个编码，同时这些编码也不是天然存在的，是更往前的历史长河中，由更加基础的编码组合而成的。</p>
<p>所以当我们需要 <code>iPhone</code> 时，会拆解成 <code>集成电路</code>、<code>电池</code>、<code>高清屏幕</code>、……等等的概念，而这些概念继续拆解，直至拆解到类似于 <code>硅</code>、<code>胶水</code>、<code>玻璃</code> 等等相对基础的编码，用流程化的工程手段一步步组合成最终的高级的 <code>iPhone</code> 手机。这些编码就是人们控制生产的完整流程，有了这些编码组合，生产出 <code>iPhone</code> 就变成了一件确定性，可预测的时间了。</p></blockquote>
<p>编码进化的方式遵循着进化的三条基本规则：</p>
<ul>
<li><strong>自然选择</strong>：被使用得更多（概率更大）的编码会被保留下来；</li>
<li><strong>基因漂变</strong>：某一个编码的对应的信息会小幅度的进行一定程度的变化，以更好的适应自然选择；</li>
<li><strong>基因重组</strong>：形成新的编码的更高效的方式，就是两个已经存在的编码进行一定程度的关联，如果能够顺利关联上，就可以快速组合出新的优质编码。</li>
</ul>
<h3 id="编码不是信息组合">编码不是信息组合</h3>
<p>从前面的分析，我们知道高级的编码是很多低级编码的组合，因此蕴含了极大的信息熵，从而更好的控制了客观世界。</p>
<p>那么编码仅仅是信息打包的组合吗？或者更具体的说，人们使用编码的时候，需要能完全的理解编码背后的全部低级编码组合吗？令人庆幸的是，并不需要。当一个编码形成后，编码自身就已经是独立的控制手段了。大家可以通过编码交流，但并不需要完整的交流编码背后的全部信息，即可实现信息和控制的传递。</p>
<blockquote>
<p>关于中心极限定理，其数学表达和证明并不是很显然的，也不是未经训练的人就可以快速掌握的。</p>
<p>但它的直接结果：<code>正太分布</code>，却可以十分常见的应用于人们对外部世界的控制。</p></blockquote>
<p>所以编码本身不能简单的看作是信息组合的标识符，编码存在本身就已经意味着重要的意义。</p>
<h3 id="进化的结果是熵减更可控">进化的结果是熵减（更可控）</h3>
<p>编码的意义是指向一块小概率（自然发生的概率）的空间；通过编码进行的控制其实就是意欲将解空间限制在这块空间附近，进而提升解落在这块空间中的概率。于是编码就带来了熵减，而从应用的角度来理解，就是编码对应的行为更可控（可预测）。</p>
<p>而编码的进化过程，则是一种可持续的熵减演进的过程，带来结果的更加可控和可预测。</p>
<p>反过来这也说明了，哪怕是同一种编码，经过了不断进化后，也会提升其可预测性。这其实也反映了 LLM 的能力是收到编码的制约的——编码本身提供了可预测性的天花板。糟糕的编码 <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> 在 LLM 中也很难取得突出的效果。</p>
<h2 id="编码与-llm">编码与 LLM</h2>
<p>LLM 的目标和能力是对编码进行预测。通过上文的分析，我们已经知道，编码的作用原本就是是为了生物控制外界而诞生的；同时编码的演进方向其实也正是为了让编码变得更容易预测（增大解空间）。所以：</p>
<ul>
<li>好的 LLM 可以很好的复原编码的预测性（LLM 可以像人一样使用编码）；</li>
<li>好的编码可以提升 LLM 的可预测性；</li>
</ul>
<h3 id="语言编码">语言编码</h3>
<p>语言编码是当下最适合 LLM 的编码。也正如我们感受到的一样，LLM 在单纯的语言能力方面，已经达到甚至超过了人的能力。</p>
<p>因为语言编码经过千万年的演进，已经是一个比较优良的编码了。也正是因为这个原因，LLM 才被称为 Language 模型，而不是一个编码模型。</p>
<p>甚至可以说，在语言编码方面，LLM 已经进化到了极致——对编码本身的利用率已经几乎压榨到底了。甚至并不需要特别大的模型，就可以很好的利用语言编码的能力了（例如 14b 以下的小模型）。</p>
<h3 id="code-编码">Code 编码</h3>
<p>编程语言是人类设计出来的编码，同时参考了很多自然语言的用法和逻辑，尤其是有了包管理的概念后，Code 编码的演进速度甚至超过了语言编码。加上 Code 的语法相比语言更加严格（解空间更小），带来的熵减也比语言编码更多一些，所以具有更好的可预测性（可控性）。</p>
<p>所以 LLM 对 Code 编码的利用能力也是各种编码中最好的。面向未来的话，利用 LLM 解决 Coding 相关问题的能力一定也将会是率先迎来突破的领域（Cursor 已经初步展示了这种能力，并不需要利用更大的 LLM，现有的 LLM 甚至更小的都足够完成这种能力上的质变）。</p>
<h3 id="数学编码">数学编码</h3>
<p>数学的逻辑推理其实是十分严格的编码语言，同时数学的各种定理也足以构成更高级的编码。在这个意义上来说，数学编码将会是未来能取得重要突破的领域。</p>
<p>当下的问题是，lean 语言作为数学的编码，其语料太少，非常多的基础定理都还尚未被 lean 语言描述出证明过程。这使得 LLM 可利用的编码所蕴含的熵很少，所以可预测性很低。</p>
<p>类似于人做证明题，如果掌握了很多的定理，很多题目都会很显然。但如果没有任何的定理，所有数学命题都只能从定义出发进行推理，那么这件事将会变得非常的难。</p>
<p>而通过 lean 语言进行数学推理，DeepMind 利用强化学习，已经在几何领域中给出了成功的案例：通过强化学习生成更多的新的编码，然后学习如何利用新的编码，就可以掌握更加复杂的推理过程。</p>
<h3 id="多模态编码">多模态编码</h3>
<p>当前的多模态编码完全是一种无语意的数字化编码，而且以现在关于多模态的使用方式来看，这种编码也不具备进化的能力。所以注定多模态大模型大概率不会有什么突破性的能力出现，做到极致也仅仅是远弱于人的理解能力。</p>
<p>其实早期的 CV 研究，还是在沿着如何更好的对图像信息编码，甚至将编码不断演进的路子前行的。但后来在深度学习出现后，直接黑盒进行判别模型取得了远比编码的方式更优越的结果，就让大家逐渐放弃了编码这条路。</p>
<p>其实今天的 LLM 带给我们的关于 CV 的启示是，或许编码这条路没有错，仅仅是曾经的我们在这条路上走得还不够远。一些关于视觉方面更好的编码特征，可能才能让多模态大模型迸发全部的能量。</p>
<p>甚至更具体的说，如何能将多模态的编码与语言编码真形成对应（这就是人类或者动物真实完成的事情），然后更好的利用语言编码的能力，才能让多模态发挥更大的价值。</p>
<blockquote>
<p>更好的预测，意味着需要在原本真实物理体系的解空间中增加更多的限制，缩小解空间。单纯的图像信息不经过更好的编码是很难做到这件事情的。例如我们看到猫在跑，因为这是“猫”，所以它才会有“猫”独特的行为方式和风格，我们才得以对“猫”进行了预测。</p>
<p>如果我们不能将图像对应到“猫”这样的语言编码上，那么图像的解空间是无法被控制的，预测性就大打折扣。</p></blockquote>
<h3 id="agent-编码">Agent 编码</h3>
<p>大语言模型在提升生产力方面最大的想象空间就是 Agent。但诸多的实践都让我们发现，真实场景下，Agent 解决问题的能力非常的有限，远不及不使用大模型的传统自动化手段更高明。</p>
<p>其实这个世界中如此之多的自动化，如此之多的控制，都意味着这里天然存在某一种编码，才实现了对应的控制能力（极大的熵减）。</p>
<p>大模型之于 Agent 的核心问题，是在于现在没有一套合适的编码将各种控制手段简单直接的描述出来 <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>。如果有更加适合的语言来描述 Agent 的控制，则可以更快的推进 Agent 的发展，真的让 LLM 带来大幅的生产力的提升。</p>
<blockquote>
<p>如果从人的实践经验来类比，SOP 就比普通的自然语言更适合作为行为指引的编码，有效的控制了解空间的大小。</p></blockquote>
<p>现有的相关的编码注重的都不是信息熵的多少，而是偏操作的可解析性，容错性等问题。（例如 Json，或者 AutoGPT 的每一步的结果等等）。</p>
<p>这方面来说，或许从 Todo 体系 <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> 出发，设计一套可以引入包管理的更有利于 Agent 的编码，会是更好的解决方案。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>现在看来，这样的理解也确实有一定的问题，核心并不是压缩率，而是通过使用编码带来的熵减。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><strong>不可计算</strong>是指，无法通过数据点复原动力方程。真实世界中<strong>不可计算</strong>是普遍现象，三体问题就是最简单而经典的不可计算的例子。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>热力学第二定律本身是一个全局的物理学定律，任何物质的运动都无法脱离于物理学存在，所以热力学第二定律是不存在真正的所谓的“逆转”的。但在局部环境下，是可以通过一些控制手段，将这些局部的熵值降低的；所付出的代价一般是对外释放了更多的热能，进而让全局的熵值变得更大了。一些具体的例子：电冰箱；星云中形成恒星等等。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>信息熵大，意味着这件事更难被描述，也就是客观随机发生的概率更小。而原本随机发生概率小的事被控制以更大的概率发生，则意味着系统上的降低。&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>小数的概念要晚很多，而且小数不仅依赖分数的概念，还依赖无穷、近似等等很多数学概念的诞生。&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>在我看来多模态的大部分编码都属于糟糕的编码。语言模型在文字水平上达到甚至超越了人，但多模态模型远不及人类的能力；以及，多模态对于 AGI 将会毫无帮助，它的编码是无法进化的，除非转化为语言。&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>很明显人类的语言并不是自动化控制的适合编码，其中无效的信息过多，编码的熵太小。编程语言也充斥了过多的控制细节，编码的熵相对于这件事也太低。&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>我的另一个项目就在设计类似这样一个编码。还是回到编码演化的话题，这件事不在于编码设计得有多好，而在于编码是否能持续不断的获得更好的演化。例如如果能将很多智能家居的自动化流程传唤成某种编码，那就积累了大量的使用场景和数据，自然这种编码对应的熵减就会很大，编码就更容易带来好的结果。&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>谈一谈什么是幸福的工作</title>
      <link>https://blog.uglyboy.cn/posts/2024/07/21/</link>
      <pubDate>Sun, 21 Jul 2024 18:16:51 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/07/21/</guid>
      <description>为什么我们觉得工作不开心？</description>
      <content:encoded><![CDATA[<h2 id="从雷军的年度演讲说起">从雷军的年度演讲说起</h2>
<p>最近，小米的发布会上，雷军花了一个小时讲述小米造车的故事，又一次成了互联网的热点。当然，按照互联网成王败寇的准则，现在雷军身上的一切必将镀上一层光环，讲话中的每个做法，都会成为新的成功学标杆。所以在当下的时间点，每一种解读小米或雷军的成功之处的声音，都仿佛追逐热点的“苍蝇”，“很难评”。</p>
<p>但雷军冗长的演讲里，还是有一些观点，引起了我的共鸣（或许也跟现在的工作有关），忍不住拿出来，感慨一下“究竟什么是幸福的工作”。</p>
<h3 id="懂一行爱一行才能做好这一行">懂一行爱一行才能做好这一行</h3>
<blockquote>
<p>过去三年时间，其实我个人琢磨的最多的是，我怎么可以把车做好呢？其实我琢磨了很久，后来我发现其实很简单，就是懂一行爱一行才能做好这一行。如果我要做车的话，我一定要懂车，一定要爱车，才有机会把车做好。所以我个人造车的第一步，说出来大家可能不信，就是自己开车。</p></blockquote>
<p>其实答案就是这句话，<strong>懂一行爱一行才能做好这一行</strong>。反过来，当你爱一行的时候，在这一行工作自然也不会特别的艰难（也不完全，如果你身边都是不热爱这一行的人，那其实也依然痛苦）。</p>
<p>所以投身一个事业，必须是热爱的事业，才能让人幸福。同样只有热爱的事业，才有可能做好。</p>
<p>同样，也只有真的热爱，才能吸引有同样热爱的人。</p>
<blockquote>
<p>我收到了很多很多的信息，最打动我的一条是凌晨五点一个同事的信息。他叫刘安昱，是小米早期的创业者，37 号员工。他从小就痴迷汽车，以为这辈子跟汽车无缘了，在业余时间自己开来一辆大房车来过年。那天他正好在广州出差，看到我们自己的直播发布会以后，激动地当即拉着几个同事到珠江边喝酒，喝完以后还觉得不尽兴，又在江边走了整整一晚。到了凌晨五点，他终于下定了决心，这个机会他一定不能错过。他就给我发了一条信息，只要让他参与，干什么都行。</p>
<p>那一夜彻夜难眠的还有很多很多人，比如李田原，他当时是宝马德国总部的设计师，算是在欧洲混的很不错的中国设计师。小米的发布会深深地震撼了他，让他萌生了加入小米的愿望。各位想一想，对每一个人来说，放弃宝马总部的工作，放弃欧洲舒适的生活，说服家人一起回国，这真的不是一件简单的事情。但是他依然果断地收拾行李，举家回国，投身到小米汽车的创业中。还有胡峥楠，他之前两次拜访过小米，发布后的一个晚上，他给我打电话说。他造车二十多年，干过国企，干过民企，自己也创过业。他跟前东家的合同到期了，他特别想出来看看互联网到底是怎么造车的。后来他加入了我们，再后来他兼任了我的顾问。就这样，一群热爱汽车的人从五湖四海走到了一起。</p></blockquote>
<p>从组织理论出发，共同的热爱才是伟大组织最有效的组织共情。雷军的演讲提供了足够多的案例。可惜的是，真实的工作场景下，大部分企业的氛围里都很难找到这种对自己所从事的工作的热爱了——似乎每个人都只注重逐利，外部的利益刺激已经挤走了原本的快乐了。</p>
<h3 id="只要你真心喜欢只要你真心想干我觉得任何时候都是最好的时候">只要你真心喜欢，只要你真心想干，我觉得任何时候都是最好的时候</h3>
<p>这句话也是对方向选择的重要诠释。当我们用 10 年的视角来看行业变迁的话，常青树才是世间罕有的物种，同时哪怕有常青树，也还是会有“野火烧不尽”的野草蓬勃发芽。所以无论做什么，当作一生的事业的话，任何时候都是最好的时候。</p>
<p>想当初，淘宝那么如日中天，同时电商领域还有京东这样的市场第二。但也丝毫不影响拼多多的崛起——它的崛起没有什么移动互联网的东风，而所谓的下沉市场，走在前面的也是快手抖音，甚至是各种微商，村淘。拼多多虽然不是我喜欢的企业，但它更充分的证明了，没有什么行业是后来者注定无法成功的。</p>
<p>所以总结出来就是这句：“只要你真心喜欢，真心想干，任何时候都是最好的时候”</p>
<h3 id="尊重行业规律守正出奇">尊重行业规律，守正出奇</h3>
<p>当然，回到了具体做事上，还是必须要说的是：不是所有人都能成功的。前面两句，表达了我们应该做自己喜欢的事，无论什么时候做都不晚；但不是有了这两点就一定可以成功。</p>
<p>回到如何能做成一件事时，不存在一句话的灵丹妙药，不存在诀窍。最重要的事遵循原本的行业规律。很多时候，只要做好了这一点，就已经可以成功了。</p>
<p>不过回到互联网这个圈子，最不好的一个风气，就是对传统行业规律缺乏敬畏，总觉得自己是革新者，就应该革一切的命，甚至革自己的命更是手下不留情，最终往往是自己把自己玩死了。</p>
<p>例如大模型与搜索的结合：RAG。现在这个领域的从业者，几乎都对搜索技术不甚了解，也不屑于了解，已经过去了一年多，还在时不时的重复造轮子，造得还没原来的好。。。对传统缺乏敬畏，已经是这个行业大多数人的顽疾了。</p>
<h2 id="回到现实">回到现实</h2>
<p>小米是幸运的，因为有理想主义者雷军；小米是幸运的，因为有现实主义者雷军。</p>
<p>在互联网圈子兜兜转转多年，大部分的公司都没有理想，也没有现实主义，只剩下逐利、KPI、内斗。。。从经济学的角度来看，就是都陷在“委托-代理人”困境中出不来了。</p>
<p>组织共识才是解法，但这种需要自上而下的机制又对管理者有太高的要求，大部分企业只能做到创始人的人格魅力带来组织共识，没办法成为组织长久的一部分；更多的企业甚至连创始人的人格魅力都形不成。</p>
<p>有时想想，可能还真不如曾经国企的文化呢（曾经的国企，不是现在的国企）。</p>
]]></content:encoded>
    </item>
    <item>
      <title>RAG 的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2024/04/07/</link>
      <pubDate>Sun, 07 Apr 2024 10:24:34 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/04/07/</guid>
      <description>&lt;h2 id=&#34;问题的定义&#34;&gt;问题的定义&lt;/h2&gt;
&lt;p&gt;探讨 RAG 之前，我们需要对我们要解决的问题做一个重新的理解。传统的 LLM 是一个语言的概率预测模型，它描述的是语言的自然分布概率，所以对于这样的模型，没有回答的答案哪个更好的说法，只有回答的答案哪个概率更高的描述。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="问题的定义">问题的定义</h2>
<p>探讨 RAG 之前，我们需要对我们要解决的问题做一个重新的理解。传统的 LLM 是一个语言的概率预测模型，它描述的是语言的自然分布概率，所以对于这样的模型，没有回答的答案哪个更好的说法，只有回答的答案哪个概率更高的描述。</p>
<p>但对于 LLM 的真实场景应用来说，我们不仅仅希望 LLM 给出像人一样的回答，而且对回答的答案是有着有别于语言概率分布的预期的。所以对于实际应用场景下的 LLM 的评估，并不是评估 LLM 原本的能力（泛化性），而是评估在很多具体问题下的回答距离某个我们认为合理的回答是否更接近。</p>
<h3 id="更具体的阐述">更具体的阐述</h3>
<p>什么叫做好的 <code>LLM</code>？是回答问题更加准确的模型吗？从 <code>LLM</code> 的训练过程的输入输出（优化目标）来看，非也。</p>
<p><code>LLM</code> 的训练优化的核心目标是泛化性。这个能力其实和人类的智力测试所考察的能力是很类似的——是希望 <code>LLM</code> 具有更强的举一反三，寻找规律的能力，甚至是记忆力（长上下文的大海捞针），但是并不需要考察它已经背下了多少信息。</p>
<blockquote>
<p>很多时候我们对模型的考核是综合的，某种意义上也就混淆了“模型更聪明”和“知道的更多”之间的界限。
所以一般我们认为更加精心挑选的训练样本可以提升模型的能力，以及更小的模型可以比参数更多的模型能力强，基本上都是过多的考察了“知道的更多”这部分的能力；但回归到我们更希望模型具有的“聪明”的能力上，例如“结构化输出”、“Function Cal”、一些简单的推理等等，模型规模带来的界限就异常的坚固了。</p></blockquote>
<p>对于 <code>LLM</code> 我们更看重它的“聪明”还是“知道的更多”呢？很遗憾，这两种能力不是完全兼容的。</p>
<p>例如：一个具有良好泛化性能力的模型，是可以兼容各种粗别字、拼写错误、语法错误的语言模型。但如果一个模型具有这种能力，就需要其训练样本中存在含有错别字、拼写错误、语法错误的训练样本。这样的样本训练出来的结果，自然也会有概率输出错误。但这样的模型是更聪明了还是更笨了呢？</p>
<p>类似的，如果希望模型能够充满创造力，那模型就需要拥有想像的能力，需要看到过各种神奇的想像。于是幻觉也正是创造力无法分割的一部分。那有幻觉的模型是更聪明的模型还是更笨的模型呢？</p>
<p>所以这也带来了另一个问题：对于 <code>LLM</code> 现有的评测集究竟考察了哪种 <code>LLM</code> 的能力？是考察了它背住的正确答案的多少？还是在考察 <code>LLM</code> 的泛化性？</p>
<p>因为从 <code>LLM</code> 的优化目标上来看，我们已经有了关于聪明的描述（就是泛化性，虽然我们当前的很多测试集存在问题）。但还有很多的使用场景下，我们更关注的不是模型的“聪明”程度，而是希望模型“知道的更多”，所以本文需要给这种能力重新做一个相对清晰的定义，然后才能进一步分析如何提升这种能力。</p>
<blockquote>
<p>其实这也是 RAG 对这类真实问题能极大提升效果的原因——提升“知道的更多”这个问题的精度的最重要的办法就是背更多的答案。利用了 RAG 做答案检索可以更加精准的给出结果，自然就更容易提升效果。这个后文中会有更多的论述和证明。</p></blockquote>
<h3 id="llm-能力的重新定义">LLM 能力的重新定义</h3>
<p>$question$ 表示输入的语言序列，$answer$ 表示输出的语言序列，$score^{question}(answer)$ 表示在 $question$ 这个问题下，$answer$ 这个回答的评分，后面简记为：$score(answer)$。</p>
<p>一个语言模型 <code>LLM</code> 是指一组概率分布：</p>
<p>$$
p^{LLM}(answer|question)
$$</p>
<p>表示在 <code>LLM</code> 这个语言模型中，通过 $question$ 这个输入生成 $answer$ 的概率。</p>
<p>从而我们得到了语言模型在<strong>回答真实问题中</strong>的能力定义：</p>
<p>$$
Score^{LLM} = \sum p^{LLM}(answer|question)\cdot score(answer)
$$</p>
<p>通常，我们无法遍历得到全部的 $question$，同时也无法对全部的 $question$ 进行打分，所以模型实际的分数往往是通过采样的方式得到的估计值。</p>
<h3 id="rag-的定义">RAG 的定义</h3>
<blockquote>
<p>这里给出的定义不是从 RAG 的名词含义出发得到的定义，而是从 RAG 这种手段的本质出发，得到的定义，即——<strong>如何改写 prompt 以获得更好的结果</strong>。</p></blockquote>
<p>对于一个已知的语言模型 <code>LLM</code>，我们希望找到最好的一组变换 $f:X\to X,X是语言序列$ 使得：</p>
<p>$$
Score^* = \max_{f} \sum p^{LLM}(answer|f(question))\cdot score(answer)
$$</p>
<p>并将最好的这组变换记作 $f^*$:</p>
<p>$$
f^* = \arg\max_{f} \sum p(answer|f(question))\cdot score(answer)
$$</p>
<p>当取 $f(x)=x$ 时，$Score_{f} = Score_{LLM}$，所以至少有：</p>
<p>$$
Score^* \geq Score_{LLM}
$$</p>
<p>即，一个合理的 RAG 手段至少不会让结果变得更差。</p>
<h3 id="rag-进一步解析">RAG 进一步解析</h3>
<blockquote>
<p>这一节要解答：是不是使用 <code>RAG</code> 总能带来收益呢？如果原本的 <code>LLM</code> 已经很好了，是不是就不需要 <code>RAG</code> 了呢？</p></blockquote>
<p>从定义出发，很容易知道，<code>RAG</code> 一定是提效的。上一节也给出了一个简单的理论证明。但更加实践的角度来看，怎么保证 <code>RAG</code> 能带来更好的提升呢？</p>
<h4 id="prompt-engineering">Prompt Engineering</h4>
<p>对于大语言模型，大家都知道神奇的 “Let&rsquo;s think step by step”，对于大部分的 $question$，对其进行改写，加入 <code>Let's think step by step</code>，就能提升回答的效果。</p>
<p>类似的，现在也有诸多的手段进行 <strong>Prompt Engineering</strong>，都是可以提升原始 LLM 的回答结果的。</p>
<p>当然，这些手段是符合我们的 RAG 定义的，但是往往因为其没有更加充分的利用 $question$ 和 $answer$ 的有效信息，不会被归类到通常意义下的 RAG 当中。这部分内容也不放在我们讨论的重点。</p>
<p>但这里有一个相关的技巧，是利用了通常意义下的 RAG 技术，所以值得拿出来单独说明一下的。</p>
<h5 id="cot">CoT</h5>
<p><code>CoT</code> 可以提升 <code>LLM</code> 的效果这一点也是被大家认同的结论。通常意义上来说，这也属于 <code>Prompt Engineering</code> 的一种技巧。但是 <code>CoT</code> 有别于 <code>Let's think step by step</code>，它需要根据 $question$ 和 $answer$ 进行构造。</p>
<p>所以我们可以将已经构造好的 <code>CoT</code> Prompt 放到引擎中，即可通过检索的方式实现我们需要的 $f$，所以是可以通过 <code>RAG</code> 的方式实现 <code>CoT</code> 来优化 <code>LLM</code> 的结果的。</p>
<h4 id="当我们能直接找到-answer">当我们能直接找到 $Answer$</h4>
<p>这是另一个极端——极端乐观的情况。</p>
<p>如果我们可以通过一些手段直接获得</p>
<p>$$
answer^* = \arg\max_{answer} score(answer)
$$</p>
<p>或者一个分数很高的 $answer^*$。则我们的问题将变成：</p>
<p>找到 $f$ 使得 $f(question) = answer$。</p>
<p>通过类似构造的方法：</p>
<p>$$
f(question) = \mbox{请输出如下内容} + answer
$$</p>
<p>即可得到这样的 $f$。</p>
<h5 id="当我们能找到-n-个-answer">当我们能找到 $n$ 个 $Answer$</h5>
<p>如果我们能准确的获得 $score(answer)$ 用上节中的方法，直接利用 $answer^<em>$ 构造 $f^</em>$ 即可。但是真实场景中，我们无法精准的获得 $score(answer)$ 的，所以可以利用 <code>LLM</code> 的能力，构造如下的 $f$：</p>
<p>$$
f(question) = \mbox{根据} + question + \mbox{从下述答案中选出最好的答案} + answer_{1} + answer_{2} + \dots
$$</p>
<p>这里的假定是 <code>LLM</code> 有更大的概率可以根据 $question$ 选出更优质的答案。虽然 “<code>LLM</code> 能选出最优答案”这件事未必令人确信，但如果不利用 <code>LLM</code>，则最合理的答案是几个答案的均匀分布随机，但这样未能更好的利用 $question$ 和 $answer$ 的相关信息。</p>
<blockquote>
<p>可以看到，在这个条件下，我们所获得的 <code>RAG</code> 的映射函数 $f^*$ 已经和现实意义下的 <code>RAG</code> 很相似了。</p>
<p>这样的结果其实充分的说明了，我们通常意义下所使用的 <code>RAG</code> 方法，正是我们这个定义下的最自然的解决方案之一。也侧面说明了本文定义的合理性。</p></blockquote>
<h2 id="长文本与-rag">长文本与 RAG</h2>
<p>其实从本文的定义很容易看出，长文本和 <code>RAG</code> 根本上就是完全不同的两个概念，完全没有什么可以比较的意义，讨论长文本和 <code>RAG</code> 的关系这件事其实非常的业余。无奈总有人带节奏说这个话题，所以还是单独拿出一定的篇幅来针对一个很细节的应用场景，来聊一聊这部分内容。</p>
<h3 id="长文本应用的定义">长文本应用的定义</h3>
<p>大语言模型的能力（应用价值）的定义如前文：</p>
<p>$$
Score_{LLM} = \sum p^{LLM}(answer|question)\cdot score(answer)
$$</p>
<p>长文本模型的能力并不会超出这样一个定义，所以想要体现长文本模型能力的价值，需要对上面的使用场景再添加更多的限制条件：</p>
<ol>
<li>我们有一段很长的上下文信息 $context$；</li>
<li>我们的问题 $question$ 需要 $context$ 才能获得优质的答案 $answer$；</li>
<li>原本的 <code>LLM</code> 不一定有能力直接容纳 $context + question$ 这么长的输入文本；</li>
</ol>
<p>在上述限制条件下我们称之为<strong>长文本应用</strong>。</p>
<p>进而，通常意义下长文本给出的解决方案按照本文的定义，即：</p>
<p>$$
f^{Long}(question) = context + question
$$</p>
<p>当然，这样的 $f$ 是需要 <code>LLM</code> 具有长文本能力，才能实现的。</p>
<h3 id="上文条件下的-rag-解">上文条件下的 <code>RAG</code> 解</h3>
<p>从定义出发，</p>
<p>$$
f^* = \arg\max_{f} \sum p(answer|f(question))\cdot score(answer)
$$</p>
<p>所以</p>
<p>$$
f^* \geq f^{Long}
$$</p>
<p>而且因为 $f^{Long}$ 是个平凡的构造解，所以等号成立的概率几乎为零。即长文本带来的能力完全取代不了 <code>RAG</code> 带来的能力</p>
<blockquote>
<p>当然，因为我们并没有对 $f^<em>(question)$ 有任何限制，所以 $f^</em>(question)$ 甚至有可能比 $context + question$ 还要长，更需要 <code>LLM</code> 具有长文本的支持能力。
所以反过来也一样，<code>RAG</code> 带来的能力也取代不了长文本带来的能力。
这就是这一章开头提到的，这是两个完全不同的能力，没有可比性。</p></blockquote>
<h3 id="进一步解析">进一步解析</h3>
<p>如上文描述，通常只有 $context$ 很长的情况下，才能充分体现出 <code>LLM</code> 长文本的价值。但是 $context$ 很长，往往又意味着，$context$ 中的信息存在大量的冗余，于是 $f^*$ 相比于 $f^{Long}$ 就有了更加充分的优化空间。</p>
<p>为了说明这件事情，我们也用构造法构造一组解：</p>
<ol>
<li>将 $context$ 切片成很多个 $chunk_{i}$</li>
<li>利用 <code>LLM</code>，对 $chunk_{i}$ 做判断，判断其对回答 $question$ 是否有帮助。</li>
<li>将有帮助的 $chunk_{j}$ 保留并合并，即可形成新的 $context^*$ 相比于 $context$ 通常都是有提升的，（或者防杠精）<strong>至少是无害的，那在计算效率上也是提升</strong>。</li>
</ol>
<p>进一步，如果对 $question$ 有帮助的 $chunk_{i}$ 其实很少，那么我们对长文本的需求就急剧下降了。</p>
<blockquote>
<p>这一点是之前 <code>LLM</code> 长度不足时，大家利用 <code>RAG</code> 解决问题的理由之一；但真的没啥道理被误解成是 <code>RAG</code> 的主要作用。
而且从这里的解析来看，这种方法也确实是个非常好的解决办法——<strong>至少是无害的，那在计算效率上也是提升</strong></p></blockquote>
<p>甚至在有价值的 $chunk_{j}$ 很多的情况下，我们可以构造这样的解：</p>
<ol>
<li>
<p>将 $context$ 切片成很多个 $chunk_{i}$</p>
</li>
<li>
<p>利用 <code>LLM</code>，利用 $chunk_{i} + question$ 生成答案 $answer_{i}$</p>
</li>
<li>
<p>类似于第一章中的方法：</p>
<p>$$
f(question) = \mbox{根据} + question + \mbox{根据下述答案，生成更好的答案} + answer_{1} + answer_{2} + \dots$$</p>
</li>
</ol>
<p>这样可以让任意的长度的 $context$ 都可以被极大的压缩，减小对 <code>LLM</code> 长上下文的依赖，并且不会损失最终的效果。</p>
<blockquote>
<p>当然，这样的 $f$ 也还有进一步的提升空间。LamaIndex 中已经有好多中很好的 $f$ 的构造，本文就不赘述了。</p></blockquote>
<p>所以如果非要对长文本和 <code>RAG</code> 进行比较，更通常的情况下，确实是对 <code>RAG</code> 技术的需求很高，但对长文本的需求，绝大部分都可以转化成利用 <code>RAG</code> 技术来解决。</p>
<h2 id="为何是-rag">为何是 RAG</h2>
<blockquote>
<p>若如本文中的定义：“如何改写 prompt 以获得更好的结果”，那为什么我们还要把这个技术称为 <code>RAG</code> 呢？或者另一个相关的问题：“为何不通过 SFT 的方式让模型无需改写 prompt 即可获得更好的结果呢？”</p></blockquote>
<p>这是一个非常好的问题，甚至可以说，这个问题正好问出了本文给出 <code>LLM</code> 新定义的核心原因：<strong>因为 <code>LLM</code> 的能力强弱和真实场景下人们对答案的评估是不同的</strong>。</p>
<p><code>LLM</code> 的核心能力是泛化性，即在未见过的样本上正确估计的能力。所以强大的 <code>LLM</code> 应当是可以面对满屏错别字和语法混乱的文章，也能顺利读下来并理解其含义的模型，这是 <code>LLM</code> 能力的体现。当然，这样的 <code>LLM</code> 一定能，并且擅长胡说八道，因为这也是泛化性的体现——只不过，更强大的模型说胡话的本领会更强，更加让人察觉不到他在说胡话（指在不知道标准答案的情况下）。</p>
<p><strong>这种能力恰恰是 <code>LLM</code> 给这个世界最好的礼物。</strong></p>
<p>但很多具体的应用场景中，我们对实际的回答是有要求有约束的，我们的约束条件跟 <code>LLM</code> 的能力并不是完全一致的。本文的定义，其实就是从真实场景的实际约束条件出发给出的基本定义——即在有 $question$ 的条件下，找到 $answer^*=\arg\max score(answer)$（这个写法中没有 <code>LLM</code>，就是说这个问题并不是必须使用 <code>LLM</code> 来解决的）。</p>
<p>而非常有意思的是，解决这个问题的方法，在没有 <code>LLM</code> 之前，被定义为 <code>Retrieval</code>（或者是 <code>Search</code>）。</p>
<p>所以 <code>RAG</code> 或者被成为 <code>RCG</code> 都 OK，从历史的发展角度来看，其实应当是出现了 <code>LLM</code> 后，这样一个强大的工具如何如何让 <code>Retrieval</code> 获得更好的结果——无论有没有 <code>LLM</code> 这件事其实都是 <code>Retrieval</code></p>
<p>接下来十分自然的思考就是，<code>LLM</code> 的泛化性的能力（“聪明”的能力），如何帮助 <code>Retrieval</code> 更好的找到（或生成）准确的答案。十分自然的结合就是：用传统的 <code>Retrieval</code> 找到可能有价值的内容，让“聪明”的 <code>LLM</code> 最终判断、组合出最后的答案。这就和 <code>RAG</code> 的字面意义完全一致了。</p>
<blockquote>
<p>前文中也提到了，这其实也是当前大模型评估面临的问题：一些测试集测试的其实并不是大模型的泛化能力，而是真实场景下解决问题的能力。但如果模型针对这种问题提升所谓的“能力”，注定是以降低模型的泛化能力来实现的（参考《大模型对齐的数学理解》）。这其实也回答了，为什么不能用 SFT 替代 RAG。</p>
<p>所以解决这个问题，需要的是在 <code>LLM</code> 的能力和真实问题之间加一层技术解决方案，让这个方案来更好的利用 <code>LLM</code> 的能力，并且可以获得真实场景下的“好”的答案。这个思路其实就是本文 <code>RAG</code> 的定义方式。</p>
<p>或者更进一步描述一些行业的现象：为什么闭源的模型似乎比开源的模型好很多？不一定是模型天然的能力上，闭源比开源好；但在模型到真实问题之间，闭源的模型一定做了非常多的开源模型没有做的事情。</p>
<p><strong>有多少人工就有多少智能！</strong></p></blockquote>
<p>补充一点：这里并不是说 SFT 没有用。在 RAG 中，其实有非常多的 SFT 可以发挥价值的地方。这里的观点仅仅是：<strong>SFT 无法替代 RAG 的价值</strong>。</p>
<h3 id="为何-retrieval-总是有效果的">为何 Retrieval 总是有效果的？</h3>
<blockquote>
<p>哪怕接受了 Prompt 改写可以更好的解决真实场景的问题，但是也许有些人还是会对使用传统的 <code>Retrieval</code> 技术有芥蒂，希望有“端到端”的解决方案，少一些人工。所以这里解释清楚究竟 <code>Retrieval</code> 是如何发挥作用的。</p></blockquote>
<p>这里简单讨论的是一种很平凡的情况：</p>
<p>对于给定的 $question$ 和 $context$ 只有其中的某个 $chunk_k$ 是对 $answer$ 有帮助的（这里的 $answer$ 可以理解为“正确”的答案）：</p>
<p>$$
\begin{equation}
\begin{array}{ll}
p(answer,question|chunk_i)\leq p(answer,question), &amp; i\neq k \\
p(answer,question|chunk_i)&gt;p(answer,question), &amp; i = k
\end{array}
\end{equation}
$$</p>
<p>其中，$chunk_i$ 是 $context$ 的切片，即：</p>
<p>$$
\begin{equation}
\begin{array}{ll}
p(context) &amp;= p(chunk_1,chunk_2,\dots,chunk_n) \\
&amp;= p(chunk_1)p(chunk_2)\dots p(chunk_n)
\end{array}
\end{equation}
$$</p>
<p>由 (1) 可知，当 $i\neq k$ 时：</p>
<p>$$
p(answer,question|chunk_i) = \frac{p(answer,question,chunk_i)}{p(chunk_i)}\leq p(answer,question)
$$</p>
<p>即：</p>
<p>$$
p(answer,question)p(chunk_i)\geq p(answer,question|chunk_i)
$$</p>
<p>而我们想知道 $context$ 和 $chunk_k$ 谁对 $answer$ 的影响更大，需要比较 $p(answer,question|context)$ 和 $p(answer,question|chunk_k)$ 的大小，于是有：</p>
<p>$$
\begin{equation}
\begin{array}{ll}
\frac{p(answer,question|context)}{p(answer,question|chunk_k)} &amp;= \frac{p(answer,question,context)}{p(context)}\cdot\frac{p(chunk_k)}{p(answer,question,chunk_k)} \\
&amp;=\frac{p(answer,question,context)}{p(answer,question,chunk_k)\cdot\prod_{i\neq k}p(chunk_i)} \\
&amp;= \frac{\prod_{i\neq k} p(answer,question,chunk_i)}{\prod_{i\neq k} p(answer,question)\cdot p(chunk_i)} \\
&amp;\leq 1
\end{array}
\end{equation}
$$</p>
<p>从而证明了：</p>
<p>$$
p(answer,question|context)\leq p(answer,question|chunk_k)
$$</p>
<p>即，使用更短更精准的信息比更长的信息更有利于找到正确答案。</p>
<blockquote>
<p>推导过程中可看出，影响 $context$ 和 $chunk_{k}$ 对结果的影响产生差异的核心原因，不是 $chunk_{k}$ 对结果的正向影响里有多大，而是 $chunk_{i}$ 对结果的负向影响导致了 $context$ 的结果可靠性下降。</p>
<p>这个结论其实是很符合直觉的。但是为了防止有些人对这个概念有误解，还是认真的推演了一下全过程。而且这个推导过程中只使用了条件概率，也就是这个结论跟方法无关——不管是使用 <code>LLM</code> 还是使用 <code>RAG</code>，抑或是未来任何更先进的手段，都不会脱离这个结论——即检索可以让结果变得更好。</p>
<p>当然，在实操过程中，这个命题的条件未必可以完美的满足，例如检索过程中召回不足，或者有错误召回等等问题时，可能会导致结果更差。但这些问题其实才正是我们努力提升检索技术的动力。</p></blockquote>
<h4 id="再增加一些解释">再增加一些解释</h4>
<p>上文中对于 $chunk_i$ 的定义，只有一个要求：</p>
<p>$$
p(context) = p(chunk_1)p(chunk_2)\dots p(chunk_n)
$$</p>
<p>甚至对证明过程做一定的优化，只需要保证：</p>
<p>$$
p(context)=p(chunk_1,chunk_2,\dots,chunk_n)
$$</p>
<p>即可（例如切片之间有重叠的情况）。但对于切片的长度、位置、形式其实没有任何要求。</p>
<p>所以无论切片是从段落的维度，还是词句的维度，抑或是 token 的维度，乃至模型里面的 KVCache 的维度，都不影响我们的推导和结论。</p>
<p>反过来说，同样也是表达 <code>RAG</code> 并不局限于对文本切片，任何对 $context$ 的信息做“切片”并检索的方式，都是 <code>RAG</code>。</p>
<p><code>RAG</code> 是一类方法的定义，而不是某个具体实现的小技巧。</p>
<h3 id="不存在通用的-rag">不存在通用的 RAG</h3>
<p>首先从定义出发，不同的 $score(answer)$ 自然就会引导出不同的 <code>RAG</code> 解决方案。</p>
<p>这样的定性答案可能会引起些诸多的误解，所以我们再给出两个例子简单说明一下。</p>
<blockquote>
<p>这里用我最常用的，关于淘宝和小红书如何卖奢侈品的例子来说明。</p>
<p>如果是淘宝卖奢侈品，会认真介绍奢侈品品牌的历史，商品的原料，第三方评测，正品认证，物流，服务保障等等信息，来说明这个奢侈品的高端定位和专业性；</p>
<p>如果是小红书卖奢侈品，会出现这样的介绍：“这个包包非常的贵！但我的男朋友非常爱我，买给了我～你的男朋友爱你吗？你也值得拥有这么贵的包包”。</p></blockquote>
<p>有些场景，答案更需要的是精准性，还有一些场景，答案需要的是与用户共情。而在第二类场景中，更共情的答案往往不是更精准的答案。而在通常的聊天沟通场景下，第二类需求是主流。</p>
<blockquote>
<p>如果一个大模型，你跟它聊任何内容，他都可以用古诗（或者流行歌曲的歌词）来给予还比较恰当的回答，那相比于一个给出标准正确答案的大模型，你会更愿意跟这个有诗意（情趣）的大模型聊天。</p></blockquote>
<p>另一方面，精准性在不同的场景下往往也有不同的定义。例如实现一个算法，有些场景需要的是高精度，有些场景需要的是高性能，边缘设备上则需要对资源消耗的控制等等。种种不同的环境下，对 $answer$ 的评分就会形成天然的差别。</p>
<p>综上所述，即本节标题：不存在通用的 <code>RAG</code>。应当针对具体场景，针对性的对 <code>RAG</code> 调优。这个经验是跟传统的 <code>Search</code> 相一致的。</p>
<h3 id="本文思路下的一些研究方向">本文思路下的一些研究方向</h3>
<p>写这篇文章，并不是为了证明 <code>RAG</code> 有没有用，而是希望的是通过一个更加清晰的定义，让我们能够跳出一些技术细节，可以从一些宏观角度上来思考 <code>LLM</code> 到<strong>解决真实问题</strong>中间，还有哪些事情可以做。这里没有罗列传统检索中的常见技术或者已经被大部分人接受了的 <code>RAG</code> 技术或技巧。</p>
<h4 id="估计-scoreanswer">估计 $score(answer)$</h4>
<p>如果我们能很高效的估计出 $score(answer)$，极端理想的情况下，我们可以通过遍历所有的 $answer$ 来直接找到最好的 $answer$​，不依赖任何检索或者生成的能力。当然这件事从算力的角度来说也是不现实的。但是类似这样的思路，我们可以通过生成 + 评估的方式，将很多问题的优质答案提前索引起来，然后通过检索的方式快速召回，提升 <code>RAG</code> 的效果，减少 <code>LLM</code> 的资源消耗。</p>
<p>这件事已经有很多的尝试了，主流的方法就是通过 <code>GPT4</code> 来对结果进行评估。</p>
<h4 id="非构造的-f">非构造的 $f$</h4>
<p>当前的 <code>RAG</code> 基本上都是构造性的 $f$，这样的 $f$ 距离 $f^*$ 不会很近。</p>
<p>其实完全可能存在 $f(question)$ 是人类无法阅读的，但 <code>LLM</code> 可以很好的生成 $answer^*$ 的解。<a href="https://arxiv.org/abs/2403.12968">《LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression》</a> 就给出了一个这样的尝试。</p>
<h4 id="已知-answer-的情况下如何反推-question-或者-fquestion">已知 $answer$ 的情况下，如何反推 $question$ 或者 $f(question)$</h4>
<p>这个能力其实对于我们理解如何构造 $f$ 有着至关重要的作用，但现在我们对这件事其实还毫不了解。</p>
<h4 id="更科学评估-llm-的能力">更科学评估 <code>LLM</code> 的能力</h4>
<p>另一方面，跳开 <code>RAG</code> 的视角，而从本文最初的讨论出发，还有一个十分重要的问题值得解决。就是更合理的区分和评估 <code>LLM</code> 的泛化能力与掌握的知识。</p>
<p>其实这一点也是合理评估 <code>RAG</code> 的基础之一。因为我们经常看到一些研究，<code>LLM</code> + <code>RAG</code> 似乎就可以提升 <code>LLM</code> 的能力。但是 <code>RAG</code> 真的能提升 <code>LLM</code> 的能力吗？究竟提升的是智力水平？还是仅仅是开卷考试？</p>
<p>如果是泛泛的使用，这两种能力耦合在一起起作用，不做区分影响不大。但是如果是需要针对性的提升各方面能力，就需要能够对每一种能力做识别和判断，然后才能更好的带来进一步的提升。</p>
]]></content:encoded>
    </item>
    <item>
      <title>大模型对齐的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2024/02/27/</link>
      <pubDate>Tue, 27 Feb 2024 09:29:54 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/02/27/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;AI alignment，广义的 SFT 技术，因为其多种多样的实现方式，包括 continue learning、fine turing、LoRA、RLHF 等等，往往让大家对这个过程充满了好奇和憧憬，觉得似乎任何 NLP 的问题，只要拥有了神乎奇迹的 SFT 能力，就能从 pre-train model 进行进一步的提升，从而解决问题。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p>AI alignment，广义的 SFT 技术，因为其多种多样的实现方式，包括 continue learning、fine turing、LoRA、RLHF 等等，往往让大家对这个过程充满了好奇和憧憬，觉得似乎任何 NLP 的问题，只要拥有了神乎奇迹的 SFT 能力，就能从 pre-train model 进行进一步的提升，从而解决问题。</p>
<p>所以本篇文档就是先放下一切具体的 AI alignment 的方法，从问题定义出发，理解清楚 AI alignment 究竟是个什么过程，能解决什么问题，不能解决什么问题。</p></blockquote>
<h2 id="大模型对齐是一个贝叶斯学习">大模型对齐是一个贝叶斯学习</h2>
<p>让我们先来看一看 <code>预训练模型</code> 和对齐获得 <code>chat 模型</code> 之间是什么关系</p>
<p><strong>定义：</strong></p>
<p>$content$ 表示语言序列，$chat$ 表示聊天对话。</p>
<p>于是 $p(content)$ 就是通过 <code>预训练模型</code> 获得的语言概率分布，而 <code>chat 模型</code> 则是我们希望获得的 $p(content|chat)$，即在 <code>chat 模型</code> 中，语言序列的分布情况。</p>
<p>有了上面的基本定义，非常容易看出这是一个很基础的贝叶斯学习问题：</p>
<p>$$
p(content|chat) =\frac{ p(chat|content)\cdot p(content)}{p(chat)}
$$</p>
<p>因为我们业务场景下需要用的只是 <code>chat 模型</code>，所以 $p(chat)=1$，那么想要获得 $p(content|chat)$，其实就变成了求解 $p(chat|content)$。</p>
<p>所以很容易跳过求解细节，得到最终的结论：</p>
<p>广义的 SFT 技术的本质，就是求解 $p(chat|content)$，更具体的说，就是获得“能够判别任意文本序列是否聊天对话”的能力。</p>
<blockquote>
<p>上面的推演过程可以将 chat 换成任何 SFT 的目标，例如 <code>指令跟随</code>，<code>Functall call</code>，<code>RAG 结果调优</code> 等等。</p></blockquote>
<h3 id="训练数据">训练数据</h3>
<p>因为要求解的是一个判别问题，所以训练样本的准备就是准备很多的打了 $[1,-1]$ 标签的文本序列，然后训练就完事了。这里面没有，也不需要更复杂的数学原理了。</p>
<h3 id="nlp-问题的学习范式">NLP 问题的学习范式</h3>
<p>因为通常我们是对语言序列（Language）做判别，而不是对编码序列（String）做判别，所以这里有一个常用的利用了贝叶斯思想的技巧：</p>
<p>我们要求的是 $p(chat|content)$，是判别一段人类语言文本是否是 chat，但是如果只是使用上一节中的样本，训练得到的将会是：$p(chat|string)$，也就是判别了一段随机字符串是否是 chat。这里有：</p>
<p>$$
p(chat|string) = p(chat|content)\cdot p(content|string)
$$</p>
<p>虽然理论上可以通过 $p(chat|string)$ 反推 $p(chat|content)$，但是训练获得的 $p^*(chat|string)$ 不是真实概率，相同数量的样本，在 $string$ 空间上训练 和在 $content$ 空间上训练，其误差和泛化性是有显著不同的（因为 $content$ 空间比 $string$ 空间小了很多很多）。</p>
<p>所以，通常 NLP 问题往往构造的训练数据不是直接进行判别模型的训练的，而是在一个预训练的语言模型 $p(content|string)$ 基础上进行训练的。而恰好，$p(content|string) = p(content)$ 就是我们通过预训练得到的基础语言模型，不需要额外再去寻找其他关于语言模型的先验概率了。</p>
<p>实操来说，就是会在 $p(content)$ 模型的基础上，进行判别模型的继续训练。</p>
<h3 id="用-boosting-的思想进行调优">用 Boosting 的思想进行调优</h3>
<p>因为我们准备的样本无法覆盖全部的 $content$ 空间，但我们比较确信我们的样本质量，那么就可以用 Boosting 的思想来优化我们的训练过程。具体来说，就是类似于如下的流程：</p>
<ol>
<li>用训练样本训练一个基础判别模型 $c^0$。</li>
<li>用 $c^0$ 对训练样本进行判别，会找到一些错判或者漏判的样本。</li>
<li>将错判和漏判的样本进行加权，然后重新训练模型，获得新的模型 $c^1$。</li>
<li>重复上面的 2～3 过程，让训练样本能够发挥更大的效果。</li>
</ol>
<p>这个方法很充分的利用了训练样本，但是也会带来<strong>过拟合</strong>的问题，会导致模型的泛化性下降。</p>
<p><strong>这一点很重要</strong>。因为和上面的 NLP 的范式结合，你会发现，在 NLP 的范式下来实践 Boosting 想法，提升 SFT 的训练样本的权重，实操上的执行手段就是 <strong>RLHF</strong>。</p>
<p>所以这里就能得到一个不需要看实操细节就可以得到的结论：</p>
<blockquote>
<p>RLHF 可以更好的利用有限的训练样本，但会影响模型的泛化性。所以 RLHF 只能做有限的调优手段，不能作为 SFT 的主要手段。</p></blockquote>
<h3 id="实践中的流程简化">实践中的流程简化</h3>
<p>因为我们最终要求解的是：$p(content|chat) = p(chat|content)\cdot p(content)$，同时呢，$p(chat|content)$ 又需要在 $p(content)$ 模型的基础上进行训练。所以在有深度模型这个描述能力极强的工具的帮助下，我们可以将两个流程合并到一起，直接通过训练样本训练出最终的 $p(content|chat)$ 模型。这个过程也是现在大家实践中真实使用的方式。</p>
<p>从结果来说，这个方式没有任何问题。但是从操作过程来看，会存在一些小隐患：</p>
<p>因为我们的样本本质上是要训练 $p(chat|content)$，所以评估训练质量是应该在 $p(chat|content)$ 空间上进行，看准确度和泛化性。但是我们现在直接拿到的是 $p(content|chat)$，就很难找到适合的评估指标了。两方面原因：</p>
<ol>
<li>$content$ 空间太大，本身就很难评估结果，包括不限于评估 $p(content)$ 和 $p(content|chat)$。</li>
<li>$p(content|chat)$ 其实同时在考量 $p(content)$ 和 $p(chat|content)$ 两个模型的效果，加上原本 $p(content)$ 在不同测试集下的表现就不尽相同，更难以衡量 $p(chat|content)$ 工作的质量了。</li>
</ol>
<p>所以给到的建议是，通过</p>
<p>$$
p(chat|content) = \frac{p(content|chat)}{p(content)}
$$</p>
<p>来计算原本 AI alignment 在执行的目标：${p(chat|content)}$，然后对这个目标进行准确性和泛化性的评估。无论是评测样本的构造还是评估效果的可靠性都会得到极大的提升。</p>
<h2 id="ai-alignment-这个手段能带来什么">AI Alignment 这个手段能带来什么</h2>
<p>从定义来看，AI alignment 做的事情是给预训练的大语言模型添加关于使用场景的限定条件，例如：</p>
<ol>
<li>现在是 chat 场景了，你只输出你原本输出中属于 chat 的部分。什么？你不知道哪些是 chat 的部分？我给你一个条件概率 $p(chat|content)$，现在你知道了，可以输出了。</li>
<li>现在你是一个文艺青年，你只输出你原本输出中文艺青年会表达的方式。什么？你不知道什么是文艺青年？我给你一个条件概率 $p(文艺青年|content)$，现在你知道了，可以输出了。</li>
<li>现在你是一个会按照特定结构（例如 json）输出结果的语言工具，……</li>
<li>……</li>
</ol>
<h3 id="ai-alignment-永远没办法让模型变得更好">AI Alignment 永远没办法让模型变得更好</h3>
<p>从 AI alignment 的名称就知道，它做的事情是对齐，而不是提升。相当于大语言模型有很多的使用场景，我们通常得到的模型是一个在场景选取方面用均匀概率分布的模型。AI alignment 则是更明确的给出关于使用场景的先验信息，让模型从通用场景模型转化到专用场景的模型。</p>
<p>但反过来说，如果一件事通用场景模型就做不到，那我们是没有办法找到一个专用场景，让模型能做到这件事的。</p>
<p>例如指令追随，如果模型的能力不足，通用模型就一点都 follow 不了指令，那无论怎样做 SFT，都没有办法实现这个能力。其他能力也类似，包括不限于：数学计算能力、数数能力、简单的推理能力、理解和翻译能力。</p>
<p>模型最终的能力不是：预训练模型的能力分 + Alignment 的能力分。而是这样的关系式：</p>
<p>模型最终的能力 &lt;= 预训练模型的能力分，Alignment 可以让不等式左边更加接近上界。</p>
<h2 id="关于贝叶斯思想">关于贝叶斯思想</h2>
<p>上面的讨论中两次用到了贝叶斯概率的思想。这个思想对于解决现实世界的问题其实往往起着至关重要的作用（注意，这里讨论的不是贝叶斯学派跟频率学派的纷争，他们争论的要点是关于参数究竟是参数空间的后验还是先验的问题，我们当面讨论的话题两个学派都没有任何分歧的。）</p>
<blockquote>
<p>以新冠检测为例：如果一项检测技术，新冠患者阳性的概率是 95%，而健康人阳性的概率是 5%。那么你进行了这项检测，并且呈阳性，你感染新冠的概率有多大？95%？</p>
<p>不，是需要看现在患者和健康人的比例才能知道你感染的概率（很有意思的现象，你是否感染，竟然跟所有人的比例有关）。</p>
<p>$$
\begin{equation}
\begin{array}{lll}
p(患者|阳性) =p(阳性|患者)\cdot p(患者) =0.95\cdot p(患者) \\
p(健康人|阳性) =p(阳性|健康人)\cdot p(患者) = 0.05\cdot p(健康人)
\end{array}
\end{equation}
$$</p>
<p>像现在这种新冠基本绝迹的情况下，检测阳性很大概率是假阳性，问题不大。</p>
<p>但如果是大流行的时候，往往没检测出阳性，也很有可能已经生病了。</p></blockquote>
<p>上面的例子给我们一个重要的冲击和启示：先验概率分布对后验概率分布的影响非常大。所以很多时候，或许更好的问题解决办法不是提升条件概率的精度，而是直接拿到更加准确的先验概率。</p>
<h3 id="nlp-领域的人容易钻的牛角尖">NLP 领域的人容易钻的牛角尖</h3>
<p>大模型是个很强大的工具，能解决很多以前解决不好的问题。所以 NLP 的“专家”们就开始拿着锤子找钉子，看到了很多以前做得不好的工作，希望用 NLP 来替代。但很可能他们找到的问题是优化条件概率，但对结果起更重要影响的是先验概率。</p>
<h4 id="搜索排序打分">搜索排序打分</h4>
<p>用户搜索一个 Query，我们需要判断返回结果的排序（给找回的文本打分，判断被用户选择的概率大小）</p>
<p>$$
p(answer|query) = p(query|answer)p(answer)
$$</p>
<p>NLP 的同学以为他们通过大模型拿到的是 $p(answer|query)$，但仔细思考，你会发现通用大模型获得的能力描述不了真实场景下的先验概率，所以 NLP 工具获得的只是 $p(query|answer)$，只表达了 query 和 answer 的相关性描述。表达不了用户对 $p(answer)$ 先验的喜好程度。</p>
<p>所以带来的结果就是，当前的大部分 RAG 体系，只通过向量检索计算了文本相似度，并没有关于文本的静态质量分。</p>
<p>但是对于真实用户的使用来说，更多的情况下，一个相对差一点的 $p(query|answer)$ 或许是可以接受的，但低质量的返回结果是不可接受的。</p>
<h4 id="搜索的先验概率思想">搜索的先验概率思想</h4>
<p>传统搜索中的很多解法其实都是更好的利用了先验概率，从而获得了更加精准和快速的结果。这里再举一个例子：</p>
<blockquote>
<p>用户搜索一个 Query，无论是大模型领域，还是搜索领域，第一步往往都值得做一个用户意图的识别，然后通过路由去寻找更好的专用模型（引擎）来回答问题。</p>
<p>用户意图识别在搜索领域被称为类目预测。</p>
<p>搜索中做类目预测的办法简单、粗暴、有效：</p>
<ol>
<li>将 query 分解成关键词</li>
<li>按关键词组合，统计历史上同样关键词组合的用户，真实点击的内容所在的类目，得到 关键词组合&lt;-&gt;类目 的映射关系</li>
<li>长尾关键词可能会找不到足够的历史数据而出错。但是搜索中大头流量都是可以找到精准的类目预测关系的。</li>
</ol></blockquote>
<p>这个过程中，没有任何试图理解词的真实语义的尝试，但已经十分精准的解决了搜索在绝大部分时间（流量）下的用户意图识别问题。而且解决方案直观、高效、精度高。</p>
<p>但是如果是使用大模型技术做意图识别，会漏掉先验概率对结果的影响（例如情人节搜鲜花和清明节搜鲜花，给出的鲜花品类应该是不同的，大语言模型是无法理解这种先验概率的变化的；搜索的模型也无法理解，但是可以快速捕捉到这种变化）。</p>
]]></content:encoded>
    </item>
    <item>
      <title>UglyChain，面向开发者的大模型开发框架</title>
      <link>https://blog.uglyboy.cn/posts/2024/02/15/</link>
      <pubDate>Thu, 15 Feb 2024 20:13:32 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/02/15/</guid>
      <description>一个更好用的 LLM 应用构建工具</description>
      <content:encoded><![CDATA[<h2 id="最近的新玩具uglychain">最近的新玩具：<strong>UglyChain</strong></h2>
<p>先上链接：<a href="https://github.com/uglyboy-tl/UglyChain">UglyChain</a> ，文档：<a href="https://uglychain.uglyboy.cn/">https://uglychain.uglyboy.cn</a></p>
<p>最近几个月都在开发这个项目。一方面是自己有更多的关于大模型开发的定制需求，需要更加底层的接触模型的接口，从零开始开发适合自己的脚手架对熟悉大模型应用有很大的帮助；另一方面也确实是因为当前主流的大模型开发框架，例如 <code>LangChain</code>，<code>LlamaIndex</code> 等工具对开发者并不友好，所有的功能都封装得太过，使得使用起来很不灵活，尤其是大模型这种常常遇到不可预期的错误的情况，很难定位和解决问题。</p>
<p>这篇文章就不去赘述 UglyChain 的各种特性了，总得来说，预期的功能大部分都已经开发完成了，未来希望添加更多的高级功能，具体来说就是科研和业界最新最先进的技术栈，当然，以我的性格，从来都不是追求新，只是大模型的高速发展，新才有可能带来新能力新突破。</p>
<p>折腾这个项目的过程中，还是有很多收获的，这里对这些做一些简单的记录。</p>
<h3 id="python-项目管理新工具-poetry">Python 项目管理新工具 Poetry</h3>
<p>以前都是用 venv + pip 来进行 Python 的虚拟空间管理，项目中的包管理并不便捷，而且 pip 删除包时并不能清除其依赖关系，导致时不时需要推倒重建虚拟空间，甚至曾经我最早想用大模型做 AutoDevOpt 时，想象的场景就是让大模型自动构建虚拟空间。</p>
<p>poetry 可以很好的管理包的依赖关系，而且可以对安装的内容进行分组，例如一些包可以设置在 <code>dev</code> 环境，或者 <code>test</code>、<code>doc</code> 等等。而且 poetry 还可以更加自动化的分析出相关的依赖关系，在安装前提前排雷，并且对全部虚拟环境中的包可以一条命令全部升级（升级的方式也是按约定俗成的包版本管理方式来的）。</p>
<p>因为这次的项目还需要发布包，而 Poetry 在这个过程中又进一步简化的操作，让个人开发者进行 Python 包的开发门槛降到了最低。</p>
<h3 id="ruff-代码检查工具">ruff 代码检查工具</h3>
<p>配合 <code>pyproject.toml</code> 中进行的设置，和 IDE(VSCode) 的自动提示功能，ruff 让 Python 代码更加规范，避免一些容易出问题的情况发生。</p>
<p>关于 ruff 的更详细的介绍，这里也还是给上 <a href="https://docs.astral.sh/ruff/">链接</a>，大家自行学习。</p>
<p>我的项目设置如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-toml" data-lang="toml"><span class="line"><span class="cl"><span class="p">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="nx">tool</span><span class="p">.</span><span class="nx">ruff</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">line-length</span> <span class="p">=</span> <span class="mi">120</span> <span class="c"># YOLO</span>
</span></span><span class="line"><span class="cl"><span class="nx">target-version</span> <span class="p">=</span> <span class="s2">&#34;py311&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="nx">tool</span><span class="p">.</span><span class="nx">ruff</span><span class="p">.</span><span class="nx">lint</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nx">select</span> <span class="p">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;E&#34;</span><span class="p">,</span> <span class="c"># pycodestyle errors</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;W&#34;</span><span class="p">,</span> <span class="c"># pycodestyle warnings</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;F&#34;</span><span class="p">,</span> <span class="c"># pyflakes</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;I&#34;</span><span class="p">,</span> <span class="c"># isort</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;C&#34;</span><span class="p">,</span> <span class="c"># flake8-comprehensions</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;B&#34;</span><span class="p">,</span> <span class="c"># flake8-bugbear</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="github-的-pr-操作">Github 的 PR 操作</h3>
<p>这次虽然是个人开发，但为了让项目看起来更加正规，还是研究了一下 Github 的 PR 操作：</p>
<ol>
<li>创建新分支</li>
<li>开发代码，并发布和提交新分支</li>
<li>主分支下就会看到有新分支提交，可以选择创建 PR</li>
<li>创建 PR 请求后，对个人开发者而言，就可以进行合并了</li>
</ol>
<p>这个过程就会在 Github 中留下 PR 记录，这个流程一方面可以将不同的模块功能的开发相对独立的拆解出来，另一方面也在生成发行版时，可以更加直观的记录版本的新功能。</p>
<h3 id="mkdocs-创建项目文档页面">Mkdocs 创建项目文档页面</h3>
<p>Mkdocs 来创建和管理项目文档十分的方便，就体感来说，是比 Hugo 写博客要更加便捷的。</p>
<p>除了生成项目文档外，可以十分便捷的一键发布到 Github 的 page 中，mkdocs 会自动在你的项目下创建页面文件的分支，并且自动发布。</p>
<p>需要留意的是，如果想自动发布后能狗自定义页面网址，需要在项目根目录中保留一个 CNAME 文件，这个和 Hugo 是一样的。</p>
<h2 id="大模型应用开发的前景">大模型应用开发的前景</h2>
<p>这个领域也研究了大半年了。说一些自己的判断：</p>
<ol>
<li>大模型的能力还难以支撑复杂问题的解决，哪怕是 GPT4。所以当前 <code>Agent</code> 的解法可以解释成：将复杂问题变成简单问题，然后交给大模型来解决。所以才需要规划能力，使用工具的能力，<code>ReAct</code> 的能力等等。但是哪怕这样，最近的论文也表明，哪怕是问题已经很清晰的：旅行规划问题，大模型在 90% 的真实情况下没有办法给出很好的解法。类似的，<code>Text2SQL</code> 这个课题已经是大模型领域的经典问题了，但是面对真实复杂场景，大模型还是不堪重用的。它只能解决简单问题（复杂问题的样本大模型遇见的太少了）。</li>
<li>上面这一点基本上宣告了大模型在一段时间内，成为生产力工具的方向是十分困难的。哪怕能走通，那大概率主体也不是大模型。所以换个视角来看，大模型自己的舒适区——自动化信息处理很可能是一段时间内的突破点。包括不限于：更优秀的资料整理者（筛选信息），特定主题的资料汇总（学术场景或者新闻场景），大量文档的快速阅读工作（例如 Github 项目阅读、简历筛选之类）。</li>
<li>针对上面这种发展趋势，那 RAG 一定会是下个时期的最佳辅助——特定领域的信息整理，一定离不开领域下的信息搜索。当前的 RAG 还是做得太小了——针对个人或企业文档的对话问答搜索，这并没有那么强烈的真实需求，更像是拿着锤子找钉子。</li>
<li>虽然第一点带来的 Agent 能力不能很好的发挥，但是还是需要一些更加稳定可靠的 Agent 能力的出现，这些将成为未来大模型能力进一步解放的基础。当前我的开发框架中选取了四个核心能力：结构化输出、Code Interpreter、ReAct、RAG，希望未来会有更多的突破出现。</li>
</ol>
]]></content:encoded>
    </item>
    <item>
      <title>2023 年总结</title>
      <link>https://blog.uglyboy.cn/posts/2024/01/28/</link>
      <pubDate>Sun, 28 Jan 2024 17:39:14 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2024/01/28/</guid>
      <description>2023 年总结，也是过去十年的总结</description>
      <content:encoded><![CDATA[<blockquote>
<p>这是一份迟到的 2023 年总结，因为我一直在思考，如何总结这一年，想着想着，就成了十年总结了。</p>
<p>2023 年，我 38 岁——这是一个听起来让人彷徨的年纪，大部分这个年纪的人，这辈子能不能有伟大的成就，都早已成为了定数。回想起自己 18 岁时许下的人生愿景：我要改变这个世界！到了今天，也不清楚能否达成。</p></blockquote>
<h2 id="过去的-10-年">过去的 10 年</h2>
<p>恰好，我是从 13 年开始参加工作的。</p>
<p>过去这十年，我完成了从一个学生的身份向社会人的转换。这十年，很难说自己积累下了哪些丰富的工作经验——自己特例独行的工作方式，既没有在某个领域成为专业性的人才，也没有走向管理岗位，成为一个管理者。</p>
<p>所幸，这样的结果和我所期寄的人生是一样的：人生的每个阶段都不走重复的路，让自己的人生充满新的精彩。大学前的学生生涯，让自己满足了成为领域专家的奋斗过程；大学里的生活，则让自己完整的体会过了权利的悲喜。</p>
<p>所以这 10 年，我成为了一个探索者。而这个博客里，也承载了大部分自己的探索成果。我相信这些成果的价值，也有人认可这些成果的价值，可惜，这些终究是还未兑付的价值，依然谈不上改变世界。但这十年的探索已经足够我自己解答大部分人生或工作中遇到的疑问，和自己完成和解。</p>
<p>于是到了 2023 年的这个节点上，当我基本完成了系统理论的研究后，发现不再有那么多理不清的方向性问题了。也是时候跟这十年的自己做一个告别，走向新的十年了。</p>
<h2 id="大模型的世界">大模型的世界</h2>
<p>新的征程里，有别于过去，自己想扎下根来，实打实的做一些具体的事情。谈不好这个事情会是什么，或许是曾经在学校时的小愿望——成为一个独立开发者，赚些小钱，做让自己开心的工作。</p>
<p>还没想出来做什么，所以就跟着谷阿姨，先做些“擅长”的事情——基于大模型做点啥。最初的课题，是大模型相关的数学。</p>
<p>曾经一直遗憾自己本科数学没有学好，但这一年做大模型数学研究的经历，让自己发现，或许数学真的是一种天赋。哪怕高级的数学工具自己没能学好，没能掌握好，但数学的感觉并不完全依赖于数学工具。也就是说，一个数学博士，其数学领域的理解并不一定比一个数学爱好者更加深刻。</p>
<p>但<strong>深刻</strong>，才是数学最美好的东西。一个看起来平平无奇的定理，竟然在很遥远的地方得到了重要的应用，这才是数学之美。所以数学很严谨，但是数学并不是一门枯燥的学科，而是一门充满创意的学科。</p>
<p>或许如果当年的自己能够更加坚持，真的有可能在数学生取得更大的成就。当然，这件事从最初就不是自己的梦想罢了。</p>
<p>曾经自己梦想成为一名天文学家，探索这个宇宙最大的未知。现在，大模型的世界里，或许可以圆自己这个梦。</p>
<h2 id="生活">生活</h2>
<p>悠悠也 3 岁了。她可爱的样子，一定是我上辈子的恋人！</p>
<p>研究了种种理论后发现，快乐的生活才是人生的真谛！工作需要是这种快乐生活的拼板，而不是全部。</p>
<p>所以我依旧玩《原神》，依旧看爽文，看精品电视剧。只要依然让我快乐，这些就是生活中最重要的部分。</p>
<p>房贷还完后，又给爸妈在北京置办了新家，估计今年就可以装修入住了。感觉生活的一切都渐渐有了安稳的模样，或许这就是人生慢慢淡然下来的状态吧。</p>
<p>总觉得自己比常人早熟些，可能因此，更早的有了“四十不惑”的心境吧。这样挺好的。</p>
<h2 id="结尾">结尾</h2>
<p>工作后，许久许久都不再写日志了。感觉在这个变化的世界中，似乎没有太多的东西值得被记录下来，而自己的心境，也似乎早就有些宠辱不惊了。</p>
<p>与其用文字表达心情，似乎不如让事实记录一切。如果未能留下什么，那也都是不值一提的小事了。</p>
<p>于是这份 2023 年的总结就算完成了。感谢这十年的岁月，是充实和精彩的十年；也希望未来也将会继续充实和精彩。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Hugo &#43; Reveal.js 后续</title>
      <link>https://blog.uglyboy.cn/posts/2023/12/19/</link>
      <pubDate>Tue, 19 Dec 2023 08:56:27 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/12/19/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.uglyboy.cn/posts/2023/12/07/hugo-%E6%9B%B4%E6%8D%A2%E5%AD%97%E4%BD%93-%E4%BD%BF%E7%94%A8-reveal.js-%E5%81%9A%E5%9C%A8%E7%BA%BF%E5%B1%95%E7%A4%BA/&#34;&gt;上一篇&lt;/a&gt; 介绍了如何在 Hugo 中使用 Reveal.js 进行 Slide 分享后，这段时间把自己历史的分享逐一迁移到博客上了。这个过程中，又发现了上次方案的一些问题，并做了很多细节的调整，整理如下：&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p><a href="https://blog.uglyboy.cn/posts/2023/12/07/hugo-%E6%9B%B4%E6%8D%A2%E5%AD%97%E4%BD%93-%E4%BD%BF%E7%94%A8-reveal.js-%E5%81%9A%E5%9C%A8%E7%BA%BF%E5%B1%95%E7%A4%BA/">上一篇</a> 介绍了如何在 Hugo 中使用 Reveal.js 进行 Slide 分享后，这段时间把自己历史的分享逐一迁移到博客上了。这个过程中，又发现了上次方案的一些问题，并做了很多细节的调整，整理如下：</p></blockquote>
<h3 id="基础配置">基础配置</h3>
<p><code>layouts/partials/reveal.html</code> 文件是 Reveal.js 的核心配置文件，这里有一些调整：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/reveal.min.css&#34;</span> <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/highlight/monokai.min.css&#34;</span> <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/theme/dracula.min.css&#34;</span>  <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/reveal.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/markdown/markdown.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/highlight/highlight.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/math/math.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@2.0.0/plugin/mermaid/mermaid.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">{{ $js := resources.Get &#34;js/pdfexport.js&#34; | resources.Minify | resources.Fingerprint &#34;sha512&#34; }}
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;{{ $js.Permalink }}&#34;</span> <span class="na">integrity</span><span class="o">=</span><span class="s">&#34;{{ $js.Data.Integrity }}&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nb">document</span><span class="p">.</span><span class="nx">addEventListener</span><span class="p">(</span><span class="s1">&#39;DOMContentLoaded&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nx">print_pdf</span> <span class="o">=</span> <span class="kc">false</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="nb">window</span><span class="p">.</span><span class="nx">location</span><span class="p">.</span><span class="nx">search</span><span class="p">.</span><span class="nx">match</span><span class="p">(</span><span class="sr">/\?print-pdf.*$/</span><span class="p">))</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="nx">print_pdf</span> <span class="o">=</span> <span class="kc">true</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s1">&#39;.reveal&#39;</span><span class="p">).</span><span class="nx">removeAttribute</span><span class="p">(</span><span class="s1">&#39;style&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="kd">var</span> <span class="nx">body</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s1">&#39;body&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="nx">body</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">setProperty</span><span class="p">(</span><span class="s1">&#39;--gap&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;important&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="nx">body</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">setProperty</span><span class="p">(</span><span class="s1">&#39;--content-gap&#39;</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;important&#39;</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="s1">&#39;#top-link&#39;</span><span class="p">).</span><span class="nx">style</span><span class="p">.</span><span class="nx">display</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="nb">document</span><span class="p">.</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s1">&#39;header, #vcomments&#39;</span><span class="p">).</span><span class="nx">forEach</span><span class="p">(</span><span class="kd">function</span> <span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">x</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">height</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">            <span class="nx">x</span><span class="p">.</span><span class="nx">style</span><span class="p">.</span><span class="nx">margin</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">        <span class="p">});</span>
</span></span><span class="line"><span class="cl">    <span class="p">};</span>
</span></span><span class="line"><span class="cl">    <span class="nx">Reveal</span><span class="p">.</span><span class="nx">initialize</span><span class="p">({</span>
</span></span><span class="line"><span class="cl">        <span class="nx">mathjax3</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">mathjax</span><span class="o">:</span> <span class="s1">&#39;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">tex</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nx">inlineMath</span><span class="o">:</span> <span class="p">[</span> <span class="p">[</span> <span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;$&#39;</span> <span class="p">],</span> <span class="p">[</span> <span class="s1">&#39;\\(&#39;</span><span class="p">,</span> <span class="s1">&#39;\\)&#39;</span> <span class="p">]</span>  <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nx">displayMath</span><span class="o">:</span> <span class="p">[</span> <span class="p">[</span><span class="s2">&#34;$$&#34;</span><span class="p">,</span> <span class="s2">&#34;$$&#34;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&#34;\\[&#34;</span><span class="p">,</span> <span class="s2">&#34;\\]&#34;</span><span class="p">],</span> <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nx">processEscapes</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">processEnvironments</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="nx">options</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nx">enableExplorer</span><span class="o">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">skipHtmlTags</span><span class="o">:</span> <span class="p">[</span> <span class="s1">&#39;script&#39;</span><span class="p">,</span> <span class="s1">&#39;noscript&#39;</span><span class="p">,</span> <span class="s1">&#39;style&#39;</span><span class="p">,</span> <span class="s1">&#39;textarea&#39;</span><span class="p">,</span> <span class="s1">&#39;pre&#39;</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="nx">mermaid</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">theme</span><span class="o">:</span> <span class="s1">&#39;dark&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">darkMode</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">flowchart</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nx">useMaxWidth</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">htmlLabels</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="nx">transition</span><span class="o">:</span> <span class="s1">&#39;fade&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">progress</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">embedded</span><span class="o">:</span> <span class="o">!</span><span class="nx">print_pdf</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">pdfSeparateFragments</span><span class="o">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">plugins</span><span class="o">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="nx">RevealMarkdown</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">RevealHighlight</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">RevealMath</span><span class="p">.</span><span class="nx">MathJax3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">RevealMermaid</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">PdfExport</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">}).</span><span class="nx">then</span><span class="p">(()=&gt;{</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="p">(</span><span class="nx">print_pdf</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">setTimeout</span><span class="p">(</span><span class="kd">function</span><span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nb">window</span><span class="p">.</span><span class="nx">print</span><span class="p">();</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span> <span class="mi">400</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>跟之前的版本相比，主要做了以下调整：</p>
<ul>
<li>增加了 <code>Mermaid.js</code> 的支持，用于绘制流程图；</li>
<li>添加了 <code>assets/js/pdfexport.js</code> 这个文件，它是 Reveal.js 的一个插件，用于导出 PDF，这里使用了 <a href="https://github.com/McShelby/reveal-pdfexport">reveal-pdfexport</a>，在 slide 界面中使用 <code>E</code> 键即可自动跳转到导出 PDF 的界面，然后使用浏览器的打印功能即可导出 PDF。代码中的大部分调整都是为了适配 PDF 导出功能；</li>
</ul>
<h3 id="css-调整">CSS 调整</h3>
<p>添加 <code>assets/css/reveal.css</code> 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-css" data-lang="css"><span class="line"><span class="cl"><span class="p">.</span><span class="nc">mermaid</span> <span class="nt">svg</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">max-width</span><span class="p">:</span> <span class="mi">100</span><span class="kt">%</span> <span class="cp">!important</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">.</span><span class="nc">reveal</span><span class="p">.</span><span class="nc">embedded</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">height</span><span class="p">:</span><span class="mi">50</span><span class="kt">vh</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">max-height</span><span class="p">:</span><span class="mi">540</span><span class="kt">px</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">.</span><span class="nc">reveal</span> <span class="nt">h5</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">font-size</span><span class="p">:</span> <span class="mf">0.83</span><span class="kt">em</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">.</span><span class="nc">reveal</span> <span class="nt">h6</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">font-size</span><span class="p">:</span> <span class="mf">0.67</span><span class="kt">em</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">.</span><span class="nc">reveal</span> <span class="nt">code</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">background</span><span class="p">:</span> <span class="kc">unset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">.</span><span class="nc">reveal</span> <span class="p">.</span><span class="nc">controls</span> <span class="nt">button</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">margin</span><span class="p">:</span> <span class="kc">unset</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">background-color</span><span class="p">:</span> <span class="kc">transparent</span> <span class="cp">!important</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="使用-shortcode-生成-slide">使用 Shortcode 生成 Slide</h3>
<p><code>layouts/shortcodes/reveal.html</code> 文件是用于生成 Slide 的 Shortcode：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl">{{- $raw := ( .Inner | chomp) -}}
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;reveal&#34;</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;slides&#34;</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">&lt;</span><span class="nt">section</span> <span class="na">data-markdown</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">      <span class="p">&lt;</span><span class="nt">textarea</span> <span class="na">data-template</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">      {{ $raw }}
</span></span><span class="line"><span class="cl">      <span class="p">&lt;/</span><span class="nt">textarea</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">&lt;/</span><span class="nt">section</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>删除了 <code>layouts/_default/reveal.html</code> 文件。
这样，在文章中使用 <code>{{ &lt;reveal&gt; }}</code> 就可以生成一个 Slide 了，不需要在 Front Matter 中添加 <code>layout: slide</code> 了。</p>
<p>因为功能同 layout 解耦了，后续甚至可以将这个 shortcode 制作成一个 Hugo 的 theme，这样就可以直接使用了。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Hugo 更换字体 &amp; 使用 Reveal.js 做在线展示</title>
      <link>https://blog.uglyboy.cn/posts/2023/12/07/</link>
      <pubDate>Thu, 07 Dec 2023 10:09:58 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/12/07/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;又开始折腾博客了。&lt;/p&gt;
&lt;p&gt;基于 &lt;code&gt;PaperMod&lt;/code&gt; 主题，给博客增加了两个基本功能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;更换中文更加友好的 &lt;a href=&#34;https://github.com/lxgw/LxgwWenKai&#34;&gt;霞鹜文楷&lt;/a&gt; 字体，当然同样的方法也适用于其他任何字体&lt;/li&gt;
&lt;li&gt;因为我经常使用 &lt;code&gt;Markdown&lt;/code&gt; 做 PPT，所以希望博客也能支持这种类型的文档。&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;字体配置&#34;&gt;字体配置&lt;/h2&gt;
&lt;p&gt;首先，依然修改 &lt;code&gt;layouts/partials/extend_head.html&lt;/code&gt; 文件&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;：&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p>又开始折腾博客了。</p>
<p>基于 <code>PaperMod</code> 主题，给博客增加了两个基本功能：</p>
<ol>
<li>更换中文更加友好的 <a href="https://github.com/lxgw/LxgwWenKai">霞鹜文楷</a> 字体，当然同样的方法也适用于其他任何字体</li>
<li>因为我经常使用 <code>Markdown</code> 做 PPT，所以希望博客也能支持这种类型的文档。</li>
</ol></blockquote>
<h2 id="字体配置">字体配置</h2>
<p>首先，依然修改 <code>layouts/partials/extend_head.html</code> 文件<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-go" data-lang="go"><span class="line"><span class="cl"><span class="p">{{</span> <span class="k">if</span> <span class="nx">or</span> <span class="p">.</span><span class="nx">Params</span><span class="p">.</span><span class="nx">reveal</span> <span class="p">.</span><span class="nx">Site</span><span class="p">.</span><span class="nx">Params</span><span class="p">.</span><span class="nx">reveal</span> <span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">{{</span> <span class="nx">partial</span> <span class="s">&#34;reveal.html&#34;</span> <span class="p">.</span> <span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">{{</span> <span class="k">else</span> <span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">{{</span> <span class="k">if</span> <span class="nx">or</span> <span class="p">.</span><span class="nx">Params</span><span class="p">.</span><span class="nx">mathjax</span> <span class="p">.</span><span class="nx">Site</span><span class="p">.</span><span class="nx">Params</span><span class="p">.</span><span class="nx">mathjax</span> <span class="p">}}</span> <span class="p">{{</span> <span class="nx">partial</span> <span class="s">&#34;mathjax.html&#34;</span> <span class="p">.</span> <span class="p">}}</span> <span class="p">{{</span> <span class="nx">end</span> <span class="p">}}</span>
</span></span><span class="line"><span class="cl"><span class="p">{{</span> <span class="nx">end</span> <span class="p">}}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">{{</span> <span class="k">if</span> <span class="p">.</span><span class="nx">Site</span><span class="p">.</span><span class="nx">Params</span><span class="p">.</span><span class="nx">changeFont</span> <span class="p">}}</span> <span class="p">{{</span> <span class="nx">partial</span> <span class="s">&#34;font.html&#34;</span> <span class="p">.</span> <span class="p">}}</span> <span class="p">{{</span> <span class="nx">end</span> <span class="p">}}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Reveal.js 的配置中会用到 <code>mathjax</code>，但会与我们之前添加过的 <code>mathjax</code> 冲突，所以需要在这里做一下判断。</p></blockquote>
<p>然后在 <code>layouts/partials</code> 下新建 <code>font.html</code> 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdn.bootcdn.net/ajax/libs/lxgw-wenkai-webfont/1.6.0/style.min.css&#34;</span> <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">style</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="nt">body</span><span class="o">,</span><span class="nt">code</span><span class="o">,</span><span class="nt">section</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">font-family</span><span class="p">:</span> <span class="s2">&#34;LXGW WenKai&#34;</span><span class="p">,</span> <span class="kc">sans-serif</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;/</span><span class="nt">style</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这样博客的字体就生效了。</p>
<h2 id="revealjs-的配置">Reveal.js 的配置</h2>
<p>这个会稍微麻烦一些，先在 <code>layouts/partials</code> 下新建 <code>reveal.html</code>:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/reveal.min.css&#34;</span> <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/highlight/zenburn.min.css&#34;</span> <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">link</span> <span class="na">rel</span><span class="o">=</span><span class="s">&#34;stylesheet&#34;</span> <span class="na">href</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/theme/serif.min.css&#34;</span> <span class="p">/&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/reveal.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/markdown/markdown.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/highlight/highlight.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.0.2/plugin/math/math.min.js&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="nb">document</span><span class="p">.</span><span class="nx">addEventListener</span><span class="p">(</span><span class="s1">&#39;DOMContentLoaded&#39;</span><span class="p">,</span> <span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nx">Reveal</span><span class="p">.</span><span class="nx">initialize</span><span class="p">({</span>
</span></span><span class="line"><span class="cl">        <span class="nx">mathjax3</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="nx">mathjax</span><span class="o">:</span> <span class="s1">&#39;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="nx">tex</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nx">inlineMath</span><span class="o">:</span> <span class="p">[</span> <span class="p">[</span> <span class="s1">&#39;$&#39;</span><span class="p">,</span> <span class="s1">&#39;$&#39;</span> <span class="p">],</span> <span class="p">[</span> <span class="s1">&#39;\\(&#39;</span><span class="p">,</span> <span class="s1">&#39;\\)&#39;</span> <span class="p">]</span>  <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nx">displayMath</span><span class="o">:</span> <span class="p">[</span> <span class="p">[</span><span class="s2">&#34;$$&#34;</span><span class="p">,</span> <span class="s2">&#34;$$&#34;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&#34;\\[&#34;</span><span class="p">,</span> <span class="s2">&#34;\\]&#34;</span><span class="p">],</span> <span class="p">],</span>
</span></span><span class="line"><span class="cl">                <span class="nx">processEscapes</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">processEnvironments</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">            <span class="nx">options</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="nx">enableExplorer</span><span class="o">:</span> <span class="kc">false</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="nx">skipHtmlTags</span><span class="o">:</span> <span class="p">[</span> <span class="s1">&#39;script&#39;</span><span class="p">,</span> <span class="s1">&#39;noscript&#39;</span><span class="p">,</span> <span class="s1">&#39;style&#39;</span><span class="p">,</span> <span class="s1">&#39;textarea&#39;</span><span class="p">,</span> <span class="s1">&#39;pre&#39;</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="p">},</span>
</span></span><span class="line"><span class="cl">        <span class="nx">transition</span><span class="o">:</span> <span class="s1">&#39;fade&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">progress</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">embedded</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">minScale</span><span class="o">:</span> <span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="nx">plugins</span><span class="o">:</span> <span class="p">[</span> <span class="nx">RevealMarkdown</span><span class="p">,</span> <span class="nx">RevealHighlight</span><span class="p">,</span> <span class="nx">RevealMath</span><span class="p">.</span><span class="nx">MathJax3</span> <span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这样就可以在 <code>markdown</code> 的文件头中通过设置：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">reveal</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>来启动 <code>Reveal.js</code> 的支持。但是只是这样的话，还是没有办法在页面中演示出 PPT 文件。</p>
<p>还需要增加新的文档类型，在 <code>layouts/_default</code> 下增加 <code>slide.html</code>，其中最核心的功能是：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl">{{ define &#34;main&#34; }}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">article</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;post-single&#34;</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">  {{- if .Content }}
</span></span><span class="line"><span class="cl">  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;reveal&#34;</span> <span class="na">style</span><span class="o">=</span><span class="s">&#34;height: 50vh;&#34;</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&#34;slides&#34;</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">      <span class="p">&lt;</span><span class="nt">section</span> <span class="na">data-markdown</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        <span class="p">&lt;</span><span class="nt">textarea</span> <span class="na">data-template</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">        {{ .RawContent }}
</span></span><span class="line"><span class="cl">        <span class="p">&lt;/</span><span class="nt">textarea</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">      <span class="p">&lt;/</span><span class="nt">section</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  {{- end }}
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl"><span class="p">&lt;/</span><span class="nt">article</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">{{ end }}{{/* end main */}}
</span></span></code></pre></td></tr></table>
</div>
</div><p>这样，再在文章的文件头中添加：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">layout</span><span class="p">:</span><span class="w"> </span><span class="l">slide</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>就可以在页面中直接展示 PPT 了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>大部分的功能配置都需要修改这个文件，通过这个路由来添加具体的 <code>js</code> 和 <code>css</code> 文件。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>浮躁的时代</title>
      <link>https://blog.uglyboy.cn/posts/2023/12/01/</link>
      <pubDate>Fri, 01 Dec 2023 08:56:12 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/12/01/</guid>
      <description>浮躁的时代，浮躁的人们，浮躁的世界。</description>
      <content:encoded><![CDATA[<h2 id="从拼多多市值超过阿里说起">从拼多多市值超过阿里说起</h2>
<p>昨天，拼多多的市值超过阿里了。</p>
<p>随后，看到了诸多的自媒体和身边资本圈的朋友开始热议，然后是前同事们的讨论，然后是马老师讲话又炸出一波人跟着转发着各种评论。</p>
<p>大家都在认真的研究一个问题：<strong>拼多多做对了什么？</strong> 可是有一个前提却被大家选择性遗忘——<strong>拼多多真的做对了吗？</strong></p>
<p>当然，这个问题不是针对拼多多的，并不是想说这个企业有什么不对。我们类比一下，三年前阿里市值最辉煌的时候，似乎也没有人在批判阿里有什么不对？而今天评判拼多多的每一个“对”，其实都在反衬着三年前阿里的“错”。回顾三年前，大家觉得阿里做对了吗？为什么今天就会觉得拼多多对了？那三年后，又会如何评说拼多多今天的对错呢？</p>
<p>这是一个浮躁的时代——随波飘零的片片落叶，被浪花裹挟到高点时，就会被认为是对；反过来若恰好落到了波谷，则又会被认为是错。于是对错的标准也如同波浪般变幻莫测了。</p>
<p>身边总有些朋友在努力寻找对与错的规律，希望自己成为对的人，奈何苦寻不得。或许，根源不是他们不够努力不够聪颖，而是误信了对错的标准，失去了正确的方向。</p>
<h2 id="芒格去世了大家尊敬着他的长期主义却依然做着短视的判断">芒格去世了，大家尊敬着他的长期主义，却依然做着短视的判断</h2>
<p>也是在昨天，芒格去世了。</p>
<p>朋友圈里追捧着拼多多的朋友们也纷纷缅怀着芒格。讽刺的是，他们说：芒格教会了他们要坚持长期主义；他们又在宣扬着自己从事的长期事业——无一例外的都是当下最热闹的赛道：大模型、跨境电商、AGI 。。。</p>
<p>或许他们的观念中，未来是一成不变的，当下的热门可以持续一辈子；也或许他们的观念中，“长期”其实是短暂的，或许数年，或许更短，当热门变换时，就是下一个长期的到来。</p>
<h2 id="无一幸免">无一幸免</h2>
<p>这种浮躁，在整个世界中蔓延，无一幸免。</p>
<p>政治圈，中国会赢。不过是因为其他的人都太浮躁——我们可以愿意用 20 年改变什么事情，但是大部分的国家不愿意等，没有耐心等，甚至不相信等待的力量。</p>
<blockquote>
<p>芯片封锁了又能怎样？我们在芯片上再等 20 年又能怎样？这其实才是中国人真正的底气所在——我可以用 20 年的坚持去等待，去改变。而你们，不行！</p>
<p>“时间在我”，不是因为重要的节点临近，而是因为我们相信我们的坚持。时间永远会站在正确的方向上，短期的挫折，不过是时机未到罢了。</p></blockquote>
<p>科研圈，也充斥着急功近利的浮躁的人们——大模型领域的论文越来越像新闻 PR 稿件，唯恐标题不醒目，满篇 &ldquo;xxx is all you need&rdquo;。</p>
<p>娱乐圈。。。</p>
<p>或许体育圈反而成了最后的净土，最强的战士们依然是上个时代遗留下来的老将，他们用刻苦和努力告诉后辈们，流星确实可以闪亮一瞬，绚烂无比；<em>你可以选择成为流星，但我选择成为太阳</em>。</p>
<h2 id="像我这样的人">像我这样的人</h2>
<p>喜欢毛不易的《像我这样的人》，唱出浮躁的时代里，每个人的沉沦：</p>
<blockquote>
<p>像我这样优秀的人
本该灿烂过一生
怎么二十多年到头来
还在人海里浮沉</p>
<p>像我这样聪明的人
早就告别了单纯
怎么还是用了一段情
去换一身伤痕</p>
<p>像我这样迷茫的人
像我这样寻找的人
像我这样碌碌无为的人
你还见过多少人</p>
<p>像我这样庸俗的人
从不喜欢装深沉
怎么偶尔听到老歌时
忽然也晃了神
像我这样懦弱的人
凡事都要留几分
怎么曾经也会为了谁
想过奋不顾身</p>
<p>像我这样孤单的人
像我这样傻的人
像我这样不甘平凡的人
世界上有多少人</p></blockquote>
<p>身边很多的人，意气风发，拿着高薪，天天讲着行业里最热门的黑话，谈论着阿里衰落的必然性和拼多多的成功，努力在各种场合里抛头露脸，努力成为时代的“精英”，换取别人崇拜的目光。他们不是互联网上的网红，却努力活成了自己生活圈中的网红，小心经营着自己的人设，努力迎合着身边的一切“粉丝”的期盼。</p>
<p>可他们身上却似乎又都带着 BGM：“<em>像我这样迷茫的人/像我这样寻找的人/像我这样碌碌无为的人/你还见过多少人</em>”</p>
]]></content:encoded>
    </item>
    <item>
      <title>大语言模型的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2023/11/09/</link>
      <pubDate>Thu, 09 Nov 2023 12:08:22 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/11/09/</guid>
      <description>&lt;h2 id=&#34;大语言模型的基本逻辑&#34;&gt;大语言模型的基本逻辑&lt;/h2&gt;
&lt;p&gt;大语言模型的本质是一个 &lt;code&gt;N-GRAM&lt;/code&gt; 模型，即：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：&lt;/p&gt;
&lt;p&gt;$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$&lt;/p&gt;
&lt;p&gt;该模型是一个 $N-1$ 阶的马尔可夫链，称为 &lt;code&gt;N-GRAM&lt;/code&gt; 模型&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="大语言模型的基本逻辑">大语言模型的基本逻辑</h2>
<p>大语言模型的本质是一个 <code>N-GRAM</code> 模型，即：</p>
<p><strong>定义：</strong></p>
<p>假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：</p>
<p>$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$</p>
<p>该模型是一个 $N-1$ 阶的马尔可夫链，称为 <code>N-GRAM</code> 模型</p>
<p><strong>推论：</strong> 有限马尔可夫链（或 <code>N-GRAM</code> 模型）背后的「语法」是有穷自动机，也就是正则表达式。是 <code>乔姆斯基体系</code> 最底级的文法。</p>
<h3 id="agent--llm-可以成为完备图灵机">Agent + LLM 可以成为完备图灵机</h3>
<p>一般来说，希望将有穷自动机扩充成完备图灵机，朴素的想法就是添加外部存储，如 <a href="http://arxiv.org/abs/2301.04589">Schuurmans et al(2023)</a> 就证明了使用外部存储的大模型是图灵完备的。但这种图灵完备性的实现依然需要大量的人工介入。所以我们希望找到一种更加自然的，可以自我学习的具有图灵完备性的模式。</p>
<h4 id="while-循环的图灵完备性">While 循环的图灵完备性</h4>
<p>编程语言 WHILE 语义 (Semnatik):</p>
<ul>
<li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li>
<li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li>
<li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li>
<li>对于一个 WHILE 程序,有三种运行模式:
<ul>
<li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li>
<li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li>
<li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li>
</ul>
</li>
</ul>
<p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p>
<p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD, STORE, CLOAD, CADD, CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p>
<h4 id="agent-流程都是-while-循环">Agent 流程都是 While 循环</h4>
<p>典型的几个 Agent 流程：</p>
<ol>
<li><a href="http://arxiv.org/abs/2210.03629">ReAct</a> 获得反思推理能力</li>
<li><a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> 基础的计划任务 Agent</li>
<li><a href="http://arxiv.org/abs/2303.11366">Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> 第一个全能 Agent</li>
</ol>
<p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p>
<blockquote>
<p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p></blockquote>
<h2 id="大语言模型的泛化性">大语言模型的泛化性</h2>
<p>从机器学习的角度看，大语言模型是一个生成式模型——学习原始数据的概率分布。这里有一个基础的问题：用哪种机器学习的方法来学习这个生成式模型。</p>
<p>我们也逐步来分析这个问题，首先是传统统计学习的学习方法和深度神经网络之间的选择。从结果上看，我们选择了深度神经网络，因为我们不可能见过所有人说过的所有话，所以我们希望我们训练的模型在我们未见过的样本上也能取得很好的效果，这就是<strong>模型的泛化能力</strong>。而实验表明，深度神经网络的泛化性更好。为什么呢？</p>
<h3 id="过参数化是泛化性好的本质原因">过参数化是泛化性好的本质原因</h3>
<p>在传统统计学习中，我们希望使用的参数尽可能的少（奥卡姆剃刀原理），这样才能带来更好的泛化效果。另一方面，我们又希望我们的模型的表达能力尽可能的强，这样才能更好的拟合真实的概率空间。所以会有经典的微笑曲线：</p>
<img alt="2qe95" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/caba559c77a8265e9f2d8e80df97e666.png"><p>假设空间的大小不能太小，也不能太大（否则会过拟合）。</p>
<p>但当时的人们都没有尝试一件事情，就是如果进一步增大参数空间（已经发生过拟合之后），会发生什么？</p>
<p>下图是实际发生的事情：</p>
<img alt="6kj97" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/70ad342e678188e5f5b011264353d943.png"><p>随着参数空间的继续增大，泛化性又逐步的提升了，而且比过拟合之前的最优值还要好了。</p>
<p>这件事是深度学习拥有良好泛化性的本质原因——<strong>过参数化</strong>。如 <a href="http://arxiv.org/abs/1802.01396">Belkin et al(2018)</a> 中描述的，其实这种能力也并不是深度神经网络所独有的，而是一切过参数化的机器学习方法都能具备的性质。</p>
<p>深度神经网络一方面可以通过网络结构学习任意形状的可积函数的分布，另一方面，又可以通过过参数化获得良好的泛化性，于是就成为了真实世界大部分问题的最优机器学习方法——我们可以从猜测真实问题的函数结构中解脱出来，也不用担心样本量少无法遍历全部解空间。</p>
<h3 id="过参数化带来的思考">过参数化带来的思考</h3>
<p>过参数化的机器学习过程有无穷多最优解（训练数据上 Loss 为零），所以一定是一个非凸优化问题。但是不同的解对应的泛化性是不同的。而至今为止，我们也没有一个关于解的泛化性的指导性优化理论。所以深度学习能否获得良好的泛化性是一个随机事件。</p>
<p>但另一方面，从实践的角度我们能得到，深度学习获得良好泛化性又是一个大概率的事件。</p>
<p>结合深度学习中已经获得的大量实验结果，我们可以形成这样的物理认知：泛化性好的解空间应该是空间范围比较大（或者是梯度变化更平缓）的区域；而泛化性不好的空间则反之。从而自然会有结论：解落入更大空间的概率会更大，所以解能大概率是泛化性好的。</p>
<p>而基于上面这个未被证实认知也会带来一些推论：</p>
<ul>
<li><strong>收敛速度快的算法，可能其泛化性不如收敛速度慢的算法</strong>；</li>
<li><strong>增加收敛时的随机扰动可以提升泛化性</strong>；</li>
</ul>
<p>这些结论与已知的实验结果是相符的：Adam 收敛速度好于 SGD，但泛化性很多时候不如 SGD；而 SGD 的泛化性好于 GD。</p>
<p>以及，当下的一些研究，例如尝试将已经训练好的模型中的部分参数扣掉——“因为这些参数的变化不会影响训练集上的 Loss，或者我们已知的测试集上的 Loss”……这些尝试是危险的，很可能损失掉良好的泛化性。</p>
<p>过参数化的泛化性问题，现在还没有很好的数学解释，从而也没有合适的理论来衡量一个解的泛化性效果。一段时间之内，这个问题都会是大模型的“阿喀琉斯之踵”，考验大部分的深度学习优化算法——<strong>当你带来计算效率的提升时，是不是能确保泛化性不下降</strong>？</p>
<h2 id="大语言模型的-transformer-算子">大语言模型的 Transformer 算子</h2>
<p>当我们确定了使用 <code>N-GRAM</code> 作为语言模型，以及利用深度神经网络作为机器学习的方法，以获得模型良好的泛化能力。下一步就需要进一步研究模型更细节的结构上是否为大语言模型带来的新的能力，亦或者是限制了什么能力。</p>
<p>这里首先引入一个结论：</p>
<p>当前所有的深度学习中的算子，都可以展开成全链接网络。也就是说，当前的各种深度学习的算子，并不能获得全链接网络获得不了的能力。所以如果是作为基础能力的研究，例如“网络层深是如何带来更强的表达能力的”这种研究课题，是可以将任意算子都抽象成全链接网络来进行探索。这也是 <code>NTK</code> 理论的重要价值。</p>
<p>于是，各种具体算子带来的好处，是在于使用时效率的提升。这种提升等价于——给网络带来良好的先验知识。所以深度学习中的算子不存在优劣之分，只有不同的算子对于不同的数据，先验知识的匹配程度的差别。</p>
<p>所以下面我们即将讨论的 Transformer 算子，研究的重点是它带来了哪些先验知识（或者可以说它舍弃了哪些信息，而只关注哪些知识）。</p>
<h3 id="transformer-算子的位置编码">Transformer 算子的位置编码</h3>
<p><code>N-GRAM</code> 模型是时不变的，具体来说，就是一句话的分布，不会因为它前后位置的小变化而改变。例如一个文章中一句话前面多打了一两个空格，并不会影响将要说的这句话。</p>
<p>更具体来说，就是 <code>N-GRAM</code> 中的信息只与相对位置信息有关，而与绝对位置信息无关。基于这个信息，就可以优化全链接网络，设计出算子结构，使得其只与相对位置信息有关，而与绝对位置信息无关。</p>
<p>放到 Transformer 算子中来说，就是位置编码的设计应该满足：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,m−n)
$$</p>
<p>只与 $m-n$ 有关，而跟 $m,n$ 的具体数值无关。从这条性质就能比较容易地得到 <code>RoPE</code> 旋转位置编码。</p>
<h4 id="位置编码的内差">位置编码的内差</h4>
<p>大模型当前研究的重点之一是上下文窗口的大小，我们希望这个窗口可以进一步的扩大以捕获更多的输入信息。</p>
<p>但因为训练数据有限，以及模型本身需要有一个明确的形状，所以训练时的数据基本上还是要维持差不多在 4k 的水平上，但希望能对更长的文本进行预测。这时，从位置编码的性质来看，是与上下文窗口的大小无关的，所以是可以合理外推到无限大的。但是受限于训练样本的数量，当上下文窗口更大时，基本上还是只能有效捕获到训练窗口大小的信息，对更多的信息是无法利用的。</p>
<p>这时自然的想法时，如果我对信息内差（将更长的文本挤成短文本窗口大小的样子），就可以利用已经训练的信息来推测更多的信息了。</p>
<p>可以理解成，将位置编码设计成：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,\frac{m−n}{s})
$$</p>
<p>其中，$s$ 是窗口长度。这表达的是位置编码与相对位置的绝对大小无关，而只与相对位置的相对大小有关。</p>
<p>但是这样的问题是，在更常用的场景下，相对位置的绝对大小是更重要的，例如比较短的句子中，两个 token 究竟是相隔几个位置是十分重要的。这意味着，无法直接使用这样的位置编码获得任意的窗口能力。</p>
<p>所以，当下流行的位置编码内差的方法是：</p>
<ol>
<li>通过 <code>RoPE</code> 算法训练一个 $s$ 长的窗口</li>
<li>然后再用内差的办法，重新扩张了窗口大小，此时低频（长文本部分）通过内差获得了还不错的训练性能。但高频（短文本）部分却被严重破坏了。</li>
<li>此时重新微调模型，将高频部分调整到合适的位置，可以理解成只需要训练高频部分的信息（这部分信息其实也已经有了一些合理的先验知识了），所以可以更快的将短窗口扩展到长窗口。</li>
</ol>
<p><a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-aware Scaled RoPE</a> 方法，使用的是高频内推，低频外差的办法，做到了不需要额外训练即可扩大上下文窗口，即：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,l(m−n))
$$</p>
<p>其中：</p>
<p>$$
\begin{equation}
l(m-n) \approx
\left \{
\begin{array}{ll}
m-n &amp; \text{当}(m-n)\text{较小时} \\
\frac{m-n}{s} &amp; \text{当}(m-n)\text{较大时}
\end{array}
\right.
\end{equation}
$$</p>
<p>但这个方法还是会在很多位置上失去原本训练时学到的信息，使得外推时有性能损失。</p>
<p>类似的，其实我们还可以用这样的思考方式，重新设计位置编码，使得模型可以更好的利用现有的训练数据获得合理的外推能力。例如 <code>ReRoPE</code> 算法的位置编码的设计是这样的：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,l(m−n))
$$</p>
<p>其中：</p>
<p>$$
\begin{equation}
l(m-n) =
\left \{
\begin{array}{ll}
m-n &amp; m-n &lt; s\\
s &amp; m-n \geq s
\end{array}
\right.
\end{equation}
$$</p>
<p>这是因为，训练样本中，我们从未见过 $m-n&gt;s$ 的样本，所以更长程的样本都用 $s$ 来替代，近似的获得信息的利用。类似这样的编码设计，可以保证训练集上无性能损失，并且具备了一定的扩展能力，最大化的利用了训练信息。</p>
<p>基于这个思想，还可以扩展出很多的位置编码的设计，尽最大可能性来挖掘训练样本中的信息。</p>
<h4 id="位置编码是否需要时间衰减">位置编码是否需要时间衰减？</h4>
<p>包括 <code>RoPE</code> 在内的各种位置编码，都增加了时间衰减的先验。而这部分信息其实是可以通过训练来学习到的。所以是否真的需要时间衰减这个先验信息，它是否能更有效的帮助我们训练？是一个值得研究和思考的问题。</p>
<h3 id="transformer-算子的信息编码">Transformer 算子的信息编码</h3>
<p>类似于上面位置编码的分析，我们知道分析算子的核心，是考虑它保留了什么信息（或者说舍弃了什么信息，是否有不应当舍弃的信息被舍弃了）。</p>
<blockquote>
<p>Transformer 信息编码的设计表达的是：某个位置所蕴含的信息，只与这个位置之前的所有文本间两两的相似度信息有关。</p></blockquote>
<p>其中极为重要的信息是如下公式：</p>
<p>$$
Attention(Query,Source) = \sum_{i=1}Similarity(Query,Key_i)*Value_i
$$</p>
<p>于是也可以将 Attention 机制看作一种软寻址（Soft Addressing）。</p>
<p>至于注意力模型中是否丢失了什么重要的信息？是否有更加合理的选择？是进一步分析 Transformer 算子的核心。但这一部分同样也没有什么更加基础的数学依据，所以就没有什么进一步讨论的余地了。</p>
<p>稍值得留意的是，具体的 $Similarity$ 算法的选取，还是可以从一切其他不变量中获得部分更加有意义的约束的。例如，<a href="https://spaces.ac.cn/archives/8823">从熵不变性看Attention的Scale操作</a>，还是可以从提升上下文窗口外推能力的角度，获得一个更有效的系数项。</p>
<h2 id="大语言模型的对齐">大语言模型的对齐</h2>
<p>这部分其实在数学上值得分析的内容不多，因为对齐的操作本质上是一个偏应用的操作，是让预训练模型更加符合人类的使用场景的操作。所以对齐之后，模型能力层面是没有本质提升的，更多的是在方便人类使用的层面获得了提升。这部分从应用和工程角度是需要而且极为重要的，但没有额外的数学信息。</p>
<p>其中只有一个话题值得探索，即为什么对齐的操作选择了强化学习而不是继续用传统的模式识别的方法训练？</p>
<blockquote>
<p>坊间的笑谈是，当时 OpenAI 负责对齐的团队恰好手边有现成的 RL 的算法，所以就用它搞出了 RLHF。</p></blockquote>
<p>网上关于这个问题有一些解释，大体上就是表达 RL 的调整效率是高于传统的模式识别的。这部分内容我还没有仔细的研究，就先不胡扯了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>证明细节请看：·<a href="https://zhuanlan.zhihu.com/p/343107128">while循环</a> ，源自 <a href="https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf">Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>大语言模型的计算能力</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/30/</link>
      <pubDate>Mon, 30 Oct 2023 07:50:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/30/</guid>
      <description>&lt;h2 id=&#34;大模型是有穷自动机&#34;&gt;大模型是有穷自动机&lt;/h2&gt;
&lt;h3 id=&#34;非确定型有穷自动机nfa的定义&#34;&gt;非确定型有穷自动机（NFA）的定义&lt;/h3&gt;
&lt;p&gt;非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Q$ 是一个有穷集合，称为&lt;strong&gt;状态集&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$\Sigma$ 是一个有穷集合，称为&lt;strong&gt;字母表&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是&lt;strong&gt;转移函数&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$q_0\in Q$ 是&lt;strong&gt;起始状态&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;$F \subseteq Q$ 是&lt;strong&gt;接受状态集&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;大模型是-nfa-的证明&#34;&gt;大模型是 NFA 的证明&lt;/h3&gt;
&lt;p&gt;令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="大模型是有穷自动机">大模型是有穷自动机</h2>
<h3 id="非确定型有穷自动机nfa的定义">非确定型有穷自动机（NFA）的定义</h3>
<p>非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中</p>
<ol>
<li>$Q$ 是一个有穷集合，称为<strong>状态集</strong>。</li>
<li>$\Sigma$ 是一个有穷集合，称为<strong>字母表</strong>。</li>
<li>$\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是<strong>转移函数</strong>。</li>
<li>$q_0\in Q$ 是<strong>起始状态</strong>。</li>
<li>$F \subseteq Q$ 是<strong>接受状态集</strong>。</li>
</ol>
<h3 id="大模型是-nfa-的证明">大模型是 NFA 的证明</h3>
<p>令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为</p>
<p>$$
\phi:{s_0s_1s_2&hellip;s_{n-1}}\rightarrow s_n,s_i \in \Sigma
$$</p>
<p>取 $\delta$ 为：</p>
<p>$$
\begin{equation}
\delta(q,\sigma) = \left \{
\begin{array}{ll}
q \circ\sigma &amp; \sigma \neq \varepsilon \\
q\circ\phi(q) &amp; \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$</p>
<p>也就是将 $Q$ 设置为已经拥有的上文，连续预测下一个字符（若当前是输入过程，则只需要简单的叠加到状态集，不需要预测的过程）。这描述了大语言模型下的“<em>Next Token Prediction</em>” 范式。也就是说这个范式下的一切模型（无论是 Transformer 还是 其他的什么算子进行这种模式的预测），都跳不出这个基本的范式。</p>
<p>即当前的大模型无论如何提升自己的能力，其计算能力也不过是一个有穷自动机。</p>
<blockquote>
<p>也就是说，类似于 $\{0^n\#1^n\}$ 这个模式是无法被有穷自动机学习和预测出来的。换句话说，大模型的智能在这个例子上直接会被锁死，注定达不成所谓的**“AGI”**。</p>
<p>以这个例子泛化来说，我们仅通过构造正负样本和机器学习做概率预测的方式，永远也无法对上面的模式做完美的判定。这个结论正是上面的推理想表达的意思。</p>
<p>这件事可以拿 ChatGPT 来测试，对于 <code>0#1</code>，<code>00#11</code>，<code>000#111</code>，&hellip;，这个序列，让 ChatGPT 续写，它可以继续写下去且不出错（但这只是假象），而且也会明确的说出这个序列是 $\{0^n\#1^n\}$ 这个模式的产物。但当你要求它输出 n=100 时的输出，或者你拿 n=100 时的输入让 ChatGPT 判定时，它就会出错了。</p></blockquote>
<p>直接得到的重要启示是：</p>
<p>除了大模型，我们还需要新的范式来解决 <strong>AGI</strong> 问题。<strong>仅靠提升模型规模，注定有很多事情做不到</strong>。</p>
<h3 id="额外的说明">额外的说明</h3>
<p>有穷自动机是做不出基础四则运算的计算器 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 的。这也意味着大模型的推理能力是不存在的。</p>
<p>我们认为的推理能力，不过是在有限状态空间下的穷举，例如上文中的 $\{0^n\#1^n\}$ 这个例子。更大的模型可以通过训练模拟出更长的匹配，但是从“<strong>压缩比</strong>”的角度看，终究是没有能掌握这个规律，而是通过空间换时间的方式将更多的答案在训练的过程中记住。</p>
<p>所以可能又回到了最初的问题——大模型是不是必须要足够大？继续增加大模型的规模还可以进一步提升泛化性，在类似这样的原本有穷自动机解决不了的问题上缓存更多的答案，“<strong>假装</strong>”大模型是可以解决它的。但这不是我们想要的答案。</p>
<h2 id="agent--llm-可以成为完备图灵机">Agent + LLM 可以成为完备图灵机</h2>
<h3 id="while-循环的图灵完备性">While 循环的图灵完备性</h3>
<p>编程语言 WHILE 语义 (Semnatik):</p>
<ul>
<li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li>
<li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li>
<li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li>
<li>对于一个 WHILE 程序,有三种运行模式:
<ul>
<li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li>
<li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li>
<li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li>
</ul>
</li>
</ul>
<p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p>
<p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p>
<blockquote>
<p>需要注意的是，循环中 ${x_i}$ 的个数对其是否是图灵完备的有影响。具体来说，<strong>任意图灵机可以被拥有 $8$ 个变量的 WHILE 程序模拟计算</strong>。</p>
<p>这里的大部分变量其实是用来操控 RAM 或者用来操控图灵机的。真实使用时，不需要这么多的掣肘。</p></blockquote>
<h3 id="agent-的基本范式">Agent 的基本范式</h3>
<p>Agent 的基本范式恰好就是一个 While 程序，其 <code>Python</code> 描述如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nd">@dataclass</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ReAct</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">thought</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">    <span class="n">action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">obs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">()</span> <span class="c1"># 获取执行 Action 的结果</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        执行Action
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@classmethod</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&#34;ReAct&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        从大模型返回的文本解析成 ReAct 的实例
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@property</span>
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">done</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        终止条件
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nd">@abstractmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        从 ReAct 中抽取信息形成新的 Prompt
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">act</span> <span class="o">=</span> <span class="n">ReAct</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">acts</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">while</span> <span class="ow">not</span> <span class="n">act</span><span class="o">.</span><span class="n">done</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">acts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">prompt</span> <span class="o">=</span> <span class="n">get_prompt</span><span class="p">(</span><span class="n">acts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">act</span><span class="p">:</span> <span class="n">ReAct</span> <span class="o">=</span> <span class="n">ReAct</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">llm</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span> <span class="c1"># 调用大模型，并将 response 解析成 ReAct 的实例</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>其中，存储和变量有两种选择：可以保存在函数 <code>get_prompt</code> 中（这意味着更多的人工控制设定），也可以保存在 <code>ReAct</code> 中（这意味着让大模型在上下文中自行决定保存哪些信息）。</p>
<p>所以，<strong>Agent 的基本范式是图灵完备的</strong>。</p>
<p>典型的几个 Agent 流程：</p>
<ol>
<li><a href="http://arxiv.org/abs/2210.03629">ReAct</a> 获得反思推理能力</li>
<li><a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> 基础的计划任务 Agent</li>
<li><a href="http://arxiv.org/abs/2303.11366">Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> 第一个全能 Agent</li>
</ol>
<p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p>
<blockquote>
<p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p></blockquote>
<h3 id="突破的方向可训练的-agent">突破的方向——可训练的 Agent</h3>
<p>如果想获得更强的计算能力，需要提升的不仅仅是 LLM，而是结合了 Agent 后的整体系统。所以微调（fine tuning）和对齐（Alignment）更应该在整合了一个可学习的 Agent 之后进行。</p>
<p>另外，基础的预训练模型或许并不需要特别的大（当然，越大性能越好的结论不变，但与其记更多的数据不如记更多的规律），而需要把更多的训练工作后置到集成了 Agent 之后进行，这样才有可能将有穷自动机无法识别的模式学习出来。</p>
<blockquote>
<p>Agent 的 While 程序模式，其实也恰好符合一个强化学习的学习过程，这里确实是可以做很多工作的。</p></blockquote>
<h2 id="这是通往-agi-之路吗">这是通往 AGI 之路吗</h2>
<p>到今天为止，其实我们也没有一个关于智能的合理定义。</p>
<blockquote>
<p>学会了人的技能就算是智能了吗？会不会千百万年后的未来人回头看，会觉得人类太傻，并不具有智能呢？所以大模型学习人这件事是不是就是最好的选择？</p></blockquote>
<p>但至少今天人能够完成的一切，都没有可以超出图灵机范式的计算能力，所以图灵机的计算能力可以当作今天人类的极限。</p>
<p>AGI 可以定义为:</p>
<blockquote>
<p>无需人类的介入，实现任意的图灵机能力。</p></blockquote>
<p>如果以这个定义来看，那么当下的 Agent + LLM 在理论上已经可以到达人类能够触达的一切天空了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>有穷计算机无法模拟括号的匹配和乘除法的运算优先级。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>证明细节请看：·<a href="https://zhuanlan.zhihu.com/p/343107128">while循环</a>，源自 <a href="https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf">Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>所谓“压缩即是智慧”毫无意义</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/25/</link>
      <pubDate>Wed, 25 Oct 2023 10:57:47 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/25/</guid>
      <description>&lt;h2 id=&#34;算数编码才是压缩的本质&#34;&gt;算数编码才是压缩的本质&lt;/h2&gt;
&lt;p&gt;一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 &lt;a href=&#34;https://www.youtube.com/watch?v=dO4TPJkeaaU&amp;amp;t=247s&#34;&gt;Compression for AGI - Jack Rae | Stanford MLSys #76&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;里面核心模式只有一个：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;假定我有一个程序 f，我将 f 的代码传输给另一端；&lt;/li&gt;
&lt;li&gt;我有一个序列需要传输，我通过 f 对逐个字符出现的概率进行了预测；&lt;/li&gt;
&lt;li&gt;我根据算数编码，将结果编码后，传输给了另一端；&lt;/li&gt;
&lt;li&gt;最后传输的信息量最小。&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;这不过是算数编码的定义好不好！！！&lt;/strong&gt; 哪里有什么神奇的地方。。。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="算数编码才是压缩的本质">算数编码才是压缩的本质</h2>
<p>一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 <a href="https://www.youtube.com/watch?v=dO4TPJkeaaU&amp;t=247s">Compression for AGI - Jack Rae | Stanford MLSys #76</a></p>
<p>里面核心模式只有一个：</p>
<blockquote>
<ol>
<li>假定我有一个程序 f，我将 f 的代码传输给另一端；</li>
<li>我有一个序列需要传输，我通过 f 对逐个字符出现的概率进行了预测；</li>
<li>我根据算数编码，将结果编码后，传输给了另一端；</li>
<li>最后传输的信息量最小。</li>
</ol></blockquote>
<p><strong>这不过是算数编码的定义好不好！！！</strong> 哪里有什么神奇的地方。。。</p>
<p>如果非说细节，也不过就是说明了为什么不用传输参数，将大模型的训练跟编码合到了一起而已。这完全证明不了大模型为什么有效果，以及为什么更大的模型效果更好。说出来的道理仅仅是：<strong>概率预测得越准，使用算数编码的压缩率越高</strong>。这件事结合算数编码的定义，不就是显然的问题吗？</p>
<p>而且它原始的流程中，也没有能体现出“<em>Next Token Prediction</em>”的优越性和必要性。</p>
<ol>
<li>如果序列很小，那么压缩效率的核心是 f 的代码量。此时使用 <code>lambda:x=x</code> 达到的效果最好。</li>
<li>如果序列很大，那么传参也不会是压缩算法优劣的核心差别。那么其他模式训练出来的能对文本做良好概率预测的模型都可以达到好的压缩效果。</li>
<li>如果序列中等，我们需要的是是否存在一个方法，一次传输了多个算数编码和多个残差，能否通过这些信息还原出初始编码？针对这个问题，我们单独开一章来分析</li>
</ol>
<h2 id="是否只能用-ntp-做压缩">是否只能用 NTP 做压缩？</h2>
<p>由自然归纳法，如果一次传输两个编码和两个残差，能还原出原始信息，那么，一次传输 $n$ 个算数编码和 $n$ 个残差就一定可以还原出原始编码。</p>
<p>假设我们使用的算法的过程是先用除第一个字符以外的所有字符来预测第一个字符的概率，同时梯度下降；然后再用除第二个字符以外的其他字符预测第二个字符的概率，同时梯度下降。这样可以得到两个算数编码和两个残差，应该如何用这些信息还原初始的字符呢？</p>
<p>方法和不确定型自动机的原理类似，或者用更土的办法来理解算法：</p>
<blockquote>
<p>我们用词表中的所有字符，重试这个过程，看哪个字符可以匹配上。虽然计算效率相比原版的 $\mathcal{O}(1)$，这个方法的复杂度是 $\mathcal{O}(n^2)$，但至少从压缩率的角度来看，我们对算法的要求没有计算速度方面的考量，更不用提这个算法一定是可以被优化的。</p></blockquote>
<p>以此推广，也就是对于任何模式的文本预测算法，都可以用同样的方法进行信息解压缩。于是不同方法之间在压缩率方面的差距还是会回归到对概率预测的精度上。甚至理论上看，使用了更多上下文的算法，应当可以比只做 &ldquo;Next Token Prediction&rdquo; 的算法精度更高。</p>
<h3 id="其他的无效解读">其他的无效解读</h3>
<p>至于残差究竟是不是用信息熵，其实对这个压缩算法没有什么核心的影响，无论哪种残差该反向传播依旧按原本的方式传播，无所谓其物理意义。因为所有的意义都只体现在传递的残差能否还原原来的编码。残差能对应上什么物理意义的各种解释其实对压缩率和计算都没有帮助。</p>
<h2 id="结论">结论</h2>
<p>所以那个演讲其实不过是个披着数学魔术的神奇表演，本质上不过是说：大模型谁的性能好，谁就是更好的大模型——典型的废话文学新版本了。</p>
]]></content:encoded>
    </item>
    <item>
      <title>Hugo 搭建流程</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/18/</link>
      <pubDate>Wed, 18 Oct 2023 11:57:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/18/</guid>
      <description>&lt;h2 id=&#34;基本安装&#34;&gt;基本安装&lt;/h2&gt;
&lt;h3 id=&#34;搭建站点&#34;&gt;搭建站点&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;hugo new site &amp;lt;name of site&amp;gt; -f yml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;初始化-git-仓库&#34;&gt;初始化 Git 仓库&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git init
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git branch -m main &lt;span class=&#34;c1&#34;&gt;# 兼容 Github 的设置&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;安装主题&#34;&gt;安装主题&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git submodule add --depth&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;若是已经安装过主题的，需要下面的命令激活&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git submodule update --init --recursive
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;本地调试&#34;&gt;本地调试&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;hugo server
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;添加新文章&#34;&gt;添加新文章&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;hugo new posts/my-first-post.md
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;配置&#34;&gt;配置&lt;/h2&gt;
&lt;h3 id=&#34;配置-configyml&#34;&gt;配置 &lt;code&gt;config.yml&lt;/code&gt;&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;50
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yml&#34; data-lang=&#34;yml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;baseURL&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;https://examplesite.com/&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;languageCode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;zh-cn&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ExampleSite&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;theme&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PaperMod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;timeZone&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Asia/Shanghai&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enableInlineShortcodes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enableGitInfo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enableRobotsTXT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enableEmoji&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;hasCJKLanguage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;home&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;HTML, RSS, JSON]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Params&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ExampleSite&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ExampleSite description&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;author&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;xxx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;homeInfoParams&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Hi there wave&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Content&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Can be Info, links, about...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;socialIcons&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# optional&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;rss&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/index.xml&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ShowFullTextinRSS&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ShowReadingTime&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;ShowCodeCopyButtons&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;DateFormat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;2006-01-02&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 日期格式化&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;menu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;identifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;home&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;主页&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;identifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;搜索&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/search&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;identifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;tags&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;标签&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/tags&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;identifier&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;archives&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;时间轴&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;url&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/archives&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;40&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;配置-contentarchivesmd&#34;&gt;配置 &lt;code&gt;content/archives.md&lt;/code&gt;&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;title: &amp;#34;时间轴&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;layout: &amp;#34;archives&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;summary: archives
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;配置-contentsearchmd&#34;&gt;配置 &lt;code&gt;content/search.md&lt;/code&gt;&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;title: &amp;#34;搜索&amp;#34; # in any language you want
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;layout: &amp;#34;search&amp;#34; # is necessary
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;summary: &amp;#34;search&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;placeholder: &amp;#34;Typing something...&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;增加-latex-数学公式的支持&#34;&gt;增加 Latex 数学公式的支持&lt;/h3&gt;
&lt;p&gt;在 &lt;code&gt;layouts/partials&lt;/code&gt; 路径下新建文件 &lt;code&gt;extend_head.html&lt;/code&gt;：&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="基本安装">基本安装</h2>
<h3 id="搭建站点">搭建站点</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hugo new site &lt;name of site&gt; -f yml
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="初始化-git-仓库">初始化 Git 仓库</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git init
</span></span><span class="line"><span class="cl">git branch -m main <span class="c1"># 兼容 Github 的设置</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="安装主题">安装主题</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git submodule add --depth<span class="o">=</span><span class="m">1</span> https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod
</span></span></code></pre></td></tr></table>
</div>
</div><p>若是已经安装过主题的，需要下面的命令激活</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git submodule update --init --recursive
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="本地调试">本地调试</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hugo server
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="添加新文章">添加新文章</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hugo new posts/my-first-post.md
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="配置">配置</h2>
<h3 id="配置-configyml">配置 <code>config.yml</code></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">baseURL</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;https://examplesite.com/&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">languageCode</span><span class="p">:</span><span class="w"> </span><span class="l">zh-cn</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">title</span><span class="p">:</span><span class="w"> </span><span class="l">ExampleSite</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">theme</span><span class="p">:</span><span class="w"> </span><span class="l">PaperMod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">timeZone</span><span class="p">:</span><span class="w"> </span><span class="l">Asia/Shanghai</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">enableInlineShortcodes</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">enableGitInfo</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">enableRobotsTXT</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">enableEmoji</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">hasCJKLanguage</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">outputs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">home</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">HTML, RSS, JSON]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">Params</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">title</span><span class="p">:</span><span class="w"> </span><span class="l">ExampleSite</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;ExampleSite description&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">author</span><span class="p">:</span><span class="w"> </span><span class="l">xxx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">homeInfoParams</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Title</span><span class="p">:</span><span class="w"> </span><span class="l">Hi there wave</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">Content</span><span class="p">:</span><span class="w"> </span><span class="l">Can be Info, links, about...</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">socialIcons</span><span class="p">:</span><span class="w"> </span><span class="c"># optional</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">rss</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/index.xml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ShowFullTextinRSS</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ShowReadingTime</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ShowCodeCopyButtons</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">DateFormat</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;2006-01-02&#34;</span><span class="w">  </span><span class="c"># 日期格式化</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">menu</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">main</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">identifier</span><span class="p">:</span><span class="w"> </span><span class="l">home</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">主页</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">10</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">identifier</span><span class="p">:</span><span class="w"> </span><span class="l">search</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">搜索</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/search</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">20</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">identifier</span><span class="p">:</span><span class="w"> </span><span class="l">tags</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">标签</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/tags</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">identifier</span><span class="p">:</span><span class="w"> </span><span class="l">archives</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">时间轴</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l">/archives</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">40</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="配置-contentarchivesmd">配置 <code>content/archives.md</code></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">title: &#34;时间轴&#34;
</span></span><span class="line"><span class="cl">layout: &#34;archives&#34;
</span></span><span class="line"><span class="cl">summary: archives
</span></span><span class="line"><span class="cl">---
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="配置-contentsearchmd">配置 <code>content/search.md</code></h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">title: &#34;搜索&#34; # in any language you want
</span></span><span class="line"><span class="cl">layout: &#34;search&#34; # is necessary
</span></span><span class="line"><span class="cl">summary: &#34;search&#34;
</span></span><span class="line"><span class="cl">placeholder: &#34;Typing something...&#34;
</span></span><span class="line"><span class="cl">---
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="增加-latex-数学公式的支持">增加 Latex 数学公式的支持</h3>
<p>在 <code>layouts/partials</code> 路径下新建文件 <code>extend_head.html</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl">{{ if or .Params.math .Site.Params.math }} {{ partial &#34;math.html&#34; . }} {{ end }}
</span></span></code></pre></td></tr></table>
</div>
</div><p>和 <code>math.html</code> 文件：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-html" data-lang="html"><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">  <span class="nx">MathJax</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nx">tex</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nx">inlineMath</span><span class="o">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="s2">&#34;$&#34;</span><span class="p">,</span> <span class="s2">&#34;$&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="s2">&#34;\\(&#34;</span><span class="p">,</span> <span class="s2">&#34;\\)&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="nx">displayMath</span><span class="o">:</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="s2">&#34;$$&#34;</span><span class="p">,</span> <span class="s2">&#34;$$&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="s2">&#34;\\[&#34;</span><span class="p">,</span> <span class="s2">&#34;\\]&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="p">],</span>
</span></span><span class="line"><span class="cl">      <span class="nx">processEscapes</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nx">processEnvironments</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">    <span class="nx">options</span><span class="o">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nx">skipHtmlTags</span><span class="o">:</span> <span class="p">[</span><span class="s2">&#34;script&#34;</span><span class="p">,</span> <span class="s2">&#34;noscript&#34;</span><span class="p">,</span> <span class="s2">&#34;style&#34;</span><span class="p">,</span> <span class="s2">&#34;textarea&#34;</span><span class="p">,</span> <span class="s2">&#34;pre&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="nb">window</span><span class="p">.</span><span class="nx">addEventListener</span><span class="p">(</span><span class="s2">&#34;load&#34;</span><span class="p">,</span> <span class="p">(</span><span class="nx">event</span><span class="p">)</span> <span class="p">=&gt;</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">document</span><span class="p">.</span><span class="nx">querySelectorAll</span><span class="p">(</span><span class="s2">&#34;mjx-container&#34;</span><span class="p">).</span><span class="nx">forEach</span><span class="p">(</span><span class="kd">function</span> <span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nx">x</span><span class="p">.</span><span class="nx">parentElement</span><span class="p">.</span><span class="nx">classList</span> <span class="o">+=</span> <span class="s2">&#34;has-jax&#34;</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">});</span>
</span></span><span class="line"><span class="cl">  <span class="p">});</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span> <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;</span><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">script</span>
</span></span><span class="line"><span class="cl">  <span class="na">type</span><span class="o">=</span><span class="s">&#34;text/javascript&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="na">id</span><span class="o">=</span><span class="s">&#34;MathJax-script&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="na">async</span>
</span></span><span class="line"><span class="cl">  <span class="na">src</span><span class="o">=</span><span class="s">&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">&gt;&lt;/</span><span class="nt">script</span><span class="p">&gt;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="利用-github-actions-自动发布">利用 Github Actions 自动发布</h2>
<h3 id="编写-github-actions-脚本">编写 Github Actions 脚本</h3>
<p>在 <code>.github/workflows</code> 下新建文件 <code>build.yml</code>：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="c"># This is a basic workflow to help you get started with Actions</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Auto Deploy Hugo</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c"># Controls when the workflow will run</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">on</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c"># Triggers the workflow on push or pull request events but only for the main branch</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">push</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="l">main ]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">pull_request</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c"># A workflow run is made up of one or more jobs that can run sequentially or in parallel</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="c"># This workflow contains a single job called &#34;build&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">build</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="c"># The type of runner that the job will run on</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="c"># Steps represent a sequence of tasks that will be executed as part of the job</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="c"># Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">actions/checkout@v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">submodules</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">  </span><span class="c"># Fetch Hugo themes (true OR recursive)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">fetch-depth</span><span class="p">:</span><span class="w"> </span><span class="m">0</span><span class="w">    </span><span class="c"># Fetch all history for .GitInfo and .Lastmod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Setup Hugo</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">peaceiris/actions-hugo@v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">hugo-version</span><span class="p">:</span><span class="w"> </span><span class="l">latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">extended</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Build</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l">hugo --minify</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">peaceiris/actions-gh-pages@v3</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">if</span><span class="p">:</span><span class="w"> </span><span class="l">${{ github.ref == &#39;refs/heads/main&#39; }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">deploy_key</span><span class="p">:</span><span class="w"> </span><span class="l">${{ secrets.ACTIONS_DEPLOY_KEY }}</span><span class="w"> </span><span class="c"># secret 中设置好私钥</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">external_repository</span><span class="p">:</span><span class="w"> </span><span class="l">your-repo/your-repo.github.io </span><span class="w"> </span><span class="c"># Page 仓库</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">publish_branch</span><span class="p">:</span><span class="w"> </span><span class="l">main </span><span class="w"> </span><span class="c"># Page 仓库的分支</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">publish_dir</span><span class="p">:</span><span class="w"> </span><span class="l">./public</span><span class="w"> </span><span class="c"># 静态网页路径</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">commit_message</span><span class="p">:</span><span class="w"> </span><span class="l">${{ github.event.head_commit.message }}</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>记得在 Page 仓库的设置中开启 Github Pages，选择 <code>main</code> 分支，用你的仓库名替换 <code>your-repo/your-repo.github.io</code>。</p></blockquote>
<h3 id="生成私钥">生成私钥</h3>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ssh-keygen -t rsa -b <span class="m">4096</span> -C <span class="s2">&#34;</span><span class="k">$(</span>git config user.email<span class="k">)</span><span class="s2">&#34;</span> -f gh-pages -N <span class="s2">&#34;&#34;</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>你将得到两个文件：</p>
<ul>
<li><code>gh-pages.pub</code> 是 Public Key</li>
<li><code>gh-pages</code> 是 Private Key</li>
</ul>
<h3 id="在-github-中设置信息">在 Github 中设置信息</h3>
<ul>
<li>在本项目目录下设置 <strong>Sectets</strong> 的 <code>ACTIONS_DEPLOY_KEY</code> 信息，填入 Private Key</li>
<li>在 Pages 项目下设置 <strong>Deploy Keys</strong>，填入 Public Key，记得选中 <strong>Allow write access</strong></li>
</ul>
<table>
  <thead>
      <tr>
          <th>添加 public key</th>
          <th>Success</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/deploy-keys-1.jpg"><img alt="deploy-keys-1" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/a94d37f0dd668df3f707654313a5e949.jpg"></a></td>
          <td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/deploy-keys-2.jpg"><img alt="deploy-keys-2" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/dd59f719ce2905d9aeb8ecab1bb1e2af.jpg"></a></td>
      </tr>
  </tbody>
</table>
<table>
  <thead>
      <tr>
          <th>添加 private key</th>
          <th>Success</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/secrets-1.jpg"><img alt="secrets-1" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/d47a214db6716738b60e199cb523c4b1.jpg"></a></td>
          <td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/secrets-2.jpg"><img alt="secrets-2" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/117c9fb649ea9562b4c281e0b2142b14.jpg"></a></td>
      </tr>
  </tbody>
</table>
]]></content:encoded>
    </item>
    <item>
      <title>Scaling Law 的数学解读</title>
      <link>https://blog.uglyboy.cn/posts/2023/10/10/</link>
      <pubDate>Tue, 10 Oct 2023 11:50:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2023/10/10/</guid>
      <description>&lt;h2 id=&#34;dataset-size-和-loss-的关系&#34;&gt;Dataset Size 和 Loss 的关系&lt;/h2&gt;
&lt;h3 id=&#34;最大似然估计mle&#34;&gt;最大似然估计（MLE）&lt;/h3&gt;
&lt;p&gt;一切机器学习的本质都是最大似然估计：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;模型下的理想真实世界的概率分布：$p(x|\theta)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\theta|x)$&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="dataset-size-和-loss-的关系">Dataset Size 和 Loss 的关系</h2>
<h3 id="最大似然估计mle">最大似然估计（MLE）</h3>
<p>一切机器学习的本质都是最大似然估计：</p>
<ol>
<li>
<p>模型下的理想真实世界的概率分布：$p(x|\theta)$</p>
</li>
<li>
<p>我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\theta|x)$</p>
</li>
<li>
<p>现在 $x$ 已知，$\theta$ 未知，若对于两个参数 $\theta_1$ 和 $\theta_2$ 有</p>
<p>$$
L(\theta_1|x) = p(x|\theta_1) &gt; p(x|\theta_2) = L(\theta_2|x)
$$</p>
<p>那么意味着 $\theta=\theta_1$ 时，随机变量 $\theta_1$ 生成 $x$ 的概率大于当参数 $\theta=\theta_2$ 时。这也正是似然的意义所在，若观测数据为 $x$，那么 $\theta_1$ 是比 $\theta_2$ 更有可能为分布函数的参数。</p>
</li>
<li>
<p>在给定观测数据集 $X={x_n},n \in \mathbb{N}$ 时，真实世界最有可能的概率分布对应的参数 $\hat\theta$ 应该满足：</p>
<p>$$
L(\hat\theta|x) = p(x|\hat\theta) &gt; p(x|\theta) = L(\theta|x), \theta \in \mathbb{\Theta} 且 \theta \ne \hat\theta
$$</p>
<p>即：</p>
<p>$$
\hat\theta = \arg\max\limits_\theta L(\theta|x)
$$</p>
</li>
<li>
<p>求解最大似然函数：</p>
<p>$$
\frac{\mathrm{d}}{\mathrm{d}\theta} L(\theta|x) = 0
$$</p>
</li>
</ol>
<p>对这个方程数值求解的过程，对应的就是绝大部分机器学习算法中的梯度下降过程。</p>
<p>在测试集上评估的结果，我们预想的误差应当包含两部分：</p>
<ol>
<li>似然函数 $L(\theta|x)$ 对真实世界概率分布描述能力不足，带来的误差；</li>
<li>通过 $X$ 估计 $\theta$ 时，样本本身的误差；</li>
</ol>
<p>若假定我们可以通过梯度下降收敛（即上面最大似然函数的导数在 0 的一个很小的临域中），那么至少就是我们相信在观测数据集 $X$ 上，模型是正确的，那么评估的误差就更加明确的指向 $X$ 本身带来的误差。</p>
<h3 id="fisher-信息量">Fisher 信息量</h3>
<p>为了求解最大似然估计，我们常用的数值手段是：</p>
<p>假定观测数据集 $X$ 的真实世界概率对应的概率密度函数是 $f(x_i;\theta)$，定义似然函数：</p>
<p>$$
L(X;\theta) = \prod \limits^{n}_{i=1} f(x_i;\theta)
$$</p>
<p>求解时，先对 $L(X|\theta)$ 取对数，再求导，这个函数定义为 Score function：</p>
<p>$$
S(X;\theta) = \sum \limits^n_{i=1} \frac{\partial \ln f(x_i;\theta)}{\partial\theta}
$$</p>
<p>则 Fisher 信息量的定义就是这个 Score function 的二阶矩（second moment）</p>
<p>$$
I(\theta) = E[S(X;\theta)^2]
$$</p>
<p>Fisher 信息量最重要的意义是：通过中心极限定理，弱大数定律，依概率一致收敛，以及 Slutsky 定理，可以证明 MLE 的渐进分布是正态分布 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，即：</p>
<ol>
<li>$\hat \theta \stackrel{P}{\longrightarrow} \theta_0$，其中 $\theta_0$ 是参数的真实值；</li>
<li>$\sqrt{n}(\hat\theta - \theta_0) \stackrel{L}{\longrightarrow} N(0,I^{-1}(\theta))$ ;</li>
</ol>
<h3 id="数据量与误差的关系">数据量与误差的关系</h3>
<p>花了大量篇幅描述了 最大似然 和 Fisher 信息量后，最终真正值得我们关注的结论却异常的简单：</p>
<p>$$
L(D) \propto D^{-0.5}
$$</p>
<p>这个结论同计算均值时，数据样本带来的误差是完全一样的。</p>
<p>真实的机器学习条件下，我们的样本量的质量并不均匀，所以往往会优先使用更好的样本（小样本集不是大样本集的随机采样，而是精选），会导致观测数据集 $X$ 不能满足概率同分布，所以带来的结果是上述幂律关系中，实际的幂律值会小于 $0.5$。</p>
<p>理论上来说，如果我们能做到样本集随机采样，那样这个幂律就会更加接近 $0.5$，而如果样本集不能随机采样，某种意义上说，能否保持这种幂律关系是值得怀疑的。所以对于 OpenAI 和 Google 的 Scaling Law 的论文，在样本量同 Loss 的关系上，Google 的结果是更可信的。</p>
<p>哪怕依旧能维持幂律关系（维持幂律关系的数学基础是不存在的。。。），具体的数值也只能通过实际拟合来估计。因为这件事不是通用规律，只跟具体的训练数据集的分布有关，跟模型无关（前提条件是模型能在<strong>大数据</strong>下<strong>收敛</strong>，即满足大数定律、中心极限定律，并且模型可以拟合真实分布）。</p>
<h2 id="compute-和-loss-的关系">Compute 和 Loss 的关系</h2>
<h3 id="控制论和-pid-算法">控制论和 PID 算法</h3>
<p>梯度下降法的数值计算过程，某种视角下可以理解成就是控制论下的控制算法——我如何根据真实信息来控制我的预期值离目标值更近。</p>
<p>直观而好用的方法就是 PID 算法：</p>
<p>$$
u(t) = K_pe(t) + K_i\int^t_0 e(\tau)\mathrm{d}\tau + K_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}
$$</p>
<p>当然，我们的梯度下降法原没有 PID 算法如此之精密，实际流程大概率只使用了 P 的部分，也就是对误差做补偿。在深度学习中，被称为反向传播。</p>
<h3 id="单参数计算量与误差的关系">单参数计算量与误差的关系</h3>
<p>单目标的 PID（只省 P 过程了）算法，误差与计算量（迭代次数）之间的关系：</p>
<p>$$
L(C) \propto K_p^{C}=e^{\lambda C}
$$</p>
<p>即，误差同计算量之间的关系是指数关系，不是幂律关系。</p>
<p>这一点在 <a href="https://arxiv.org/abs/2206.14486">Sorscher et al. (2022)</a> 中有所体现，它的结论是：至少对于某些任务，损失可以随着数据集 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 大小呈指数级增长，而不是作为幂律。</p>
<h3 id="总计算量与误差的关系">总计算量与误差的关系</h3>
<p>不同于优化问题中，我们会通过反复迭代的方式增加计算量，深度学习的计算量基本上是同模型规模和数据量正相关的。反过来意味着对单参数的优化迭代很少的固定步数就可以收敛，所以在通常数据量规模下，可以将单参数计算量带来的优化效果视作常数（都能优化到收敛）。</p>
<p>单参数计算量带来的优化效果视为常数（不会随计算量、节点数、数据量变化而变化），意味着计算本身同误差之间没有直接关联，总计算量与误差之间的关联体现的是数据量与误差的关系和节点数（结构）与误差的关系。</p>
<p>总计算量与数据量成正比，而数据量同优化效果之间的关联我们已经在前文完成了论述。下一步我们将分析节点数和误差之间的关系，或者其实更加精确的说，应当是在单参数误差不变的条件下，节点数的变化与总计算量之间的关系，是这个关系蕴含了总计算量与误差之间的关联。</p>
<h2 id="compute-和-parameters-的关系">Compute 和 Parameters 的关系</h2>
<h3 id="分形维度">分形维度</h3>
<p>具有自相似性的结构就是分形。而我们的深度学习计算就是典型的分形结构——当模型规模扩大时，主流的扩大的方式就是增加层数 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，这带来的就是自相似性。</p>
<p>而自相似性带来的重要性质就是，系统会具有分形维度，分形维度会使得系统规模扩大时，对应的全局属性并不是等比增加，而是幂律增加，幂律的指数就是其分形维度。</p>
<blockquote>
<p>生物学中有重要的 $\frac{3}{4}$ 定律——生物随着重量的变大（原子数量的规模扩大），其相关的很多生物学特征，例如新陈代谢能力、血管长度、心跳、呼吸等等，并不与重量成正比，而是按照 $\frac{3}{4}$ 的幂律进行增长。
一个直观的理解，随着生物体长度增长，其体重会以幂律 $3$ 进行提升，而腿部的横截面则是幂律 $2$ 增长。所以生物的规模变大，就会带来腿部承受的压力不断变大，所以老鼠体型的动物的腿都很细，但大象规模的动物，腿都很粗；蚂蚁可以举起自身体重百倍的物品，但人只能举起和体重相仿的物品。这些都是因为规模变化带来的非线性，要求生物的动力学模型必须发生变化，而不能与小规模时一样。
类似的，在城市规模同城市中加油站、小超市、医院之类的城市核心建设之间，也存在着幂律增长的关系——相关幂律大约是 $0.85$。</p></blockquote>
<p>对应的，深度学习模型中，在保证单参数误差不变的条件下，Parameters 规模的增加所需要的 Compute 计算量的增加不是等比的，而是幂律的，而且这个幂律应当是小于 $1$ 的。</p>
<p>换句话说，计算量同损失之间的关系是伴生关系——计算量本身同损失是没有直接关联的。带来损失变化的根本原因不是计算不足，而是模型表达能力以及数据本身蕴含的信息带来的。</p>
<p>但因为这里的结论中，计算量与参数数量也是幂律关系，由前文，数据同损失也是幂律关系，如果参数数量同损失同样是幂律关系的化，那么计算量与损失也可以用幂律关系来表示。</p>
<h2 id="parameters-与-loss-之间的关系">Parameters 与 Loss 之间的关系</h2>
<p>这里要分析的是参数量增加为何能带来 Loss 的降低。这是因为 Parameters 的增加，可以提升模型的表达能力，可以更好的拟合目标函数。也就是说，一个模型距离真值的误差（Loss），除了因为 Dataset 自身的误差外，还有一部分是模型距离 Dataset 所描述的最大似然函数的误差。</p>
<p>这部分要是详尽分析起来会很复杂，幸好已经有一些这方面的研究：<a href="https://arxiv.org/abs/2004.10802">Sharma et al. (2020)</a> 和 <a href="https://arxiv.org/pdf/2102.06701.pdf">Bahri et al. (2021)</a> 都对这个问题进行了很好的分析，其结果也有对应的实验支撑。</p>
<blockquote>
<p>文章假定深度模型将数据映射到一个 $d$ 维数据流形上，增加的模型参数（无限数据的条件下）都会被模型用来将数据流形分割成更小的组件，然后模型将在数据流形的每个分量上进行独立的预测，以优化训练损失。</p>
<p>这样自然的，如果我们想让子区域的大小缩小 $2$ 倍，就需要增加 $2^d$ 倍的数据量或模型参数。进而就是直观的结论：</p>
<p>$$
L(P) \propto P^{-\frac{1}{d}}
$$</p>
<p>即 Loss 与 参数量之间是幂律关系，其幂律值小于 $1$（因为有 $d&gt;1$）。</p></blockquote>
<h2 id="总结">总结</h2>
<p>至此，关于 Scaling Law 的数学含义就已经基本都解释清楚了。</p>
<p>更重要的问题是，有了相关的理论支撑后，我们能做什么？哪些事情做不了。</p>
<h3 id="基于多份数据融合的实验结果预测">基于多份数据融合的实验结果预测</h3>
<p><strong>这件事是不可行的</strong>。</p>
<p>一切机器学习的基础都是最大似然估计，而最大似然估计的基础假设就是独立同分布。两组分布不同的数据融合，一定会破坏原有的分布，至于不同比例下融合后形成怎样的分布，具有怎样的特性，在两份数据的分布都已知的条件下，是可以计算的。但对于我们自己的机器学习任务，原本就是要去学习数据的分布，这就决定了，不可能在不了解数据分布的条件下，估计融合后的数据分布。</p>
<p>类似的，多分不同分布的数据集怎么融合能更贴近测试集也是不可知的，只能试出来。由于测试集也不是真值，甚至测试集对真实世界的表达很可能还不如训练集，所以针对测试集做针对性调优是不值得的。</p>
<p>这部分的定量分析，其实可以借鉴 OpenAI 关于 Scaling Laws 的经典文章 <a href="https://arxiv.org/abs/2001.08361">Kaplan et al. (2020)</a> 中尝试的方法：</p>
<blockquote>
<p>迁移学习与测试效果的提升：
当我们在与训练集分布不同的文本上评估模型时，其结果与训练验证集上的结果强烈相关，损失函数中有一个大致恒定的偏移量。换句话说，转移到不同的分布会带来一定的固定惩罚，但除此之外，其提升程度大致与训练集上的表现一致。</p></blockquote>
<p>可以用类似这样的方法，通过多份不同分布的测试集效果打分情况，评估模型表现。</p>
<p>当然，实操方面其实也不复杂，就是多看几个测试集的结果，记录下来。如果模型优化后，在各个测试集上的提升是基本一致的，那就说明这次改进不是因为数据分布变化带来的，而是因为模型能力带来的。</p>
<h3 id="判断最优的参数和模型数据量配比">判断最优的参数和模型数据量配比</h3>
<p>这件事不是特别值得做。因为我们当前模型的优质数据不够多。所以提供的数据质量是不稳定的。小模型上得到的预测数据值，在大模型上操作时，肯定不能按预测量来操作，而是还需要进一步增加数据量。但是具体增加多少，因为我们对数据质量无法在训练前得到评估，所以是不可预测的。</p>
<p>这件事值得做的条件是：我们已经用一份数据训练了一个很大的模型，然后我们可以通过抽样的方法构建小模型，用大模型预测小模型需要多少数据量，这件事是可行的。</p>
<p>当然，如果只是一个预估值做参考，这件事倒是可以做一下。</p>
<p>注：这件事值得做的数学理论基础是：我们需要找到样本的精度和模型训练的流型精度一致的对应比例。这件事的前提条件是：模型得到充分训练，且 Loss 与 样本、模型精度是同一个数量级（Loss 就是当前的精度）。如果这个精度不一致，loss 会被更大的精度所制约。带来的影响是会增加一定的无效计算量。</p>
<p>理论上，这件事更应该用适合的停机算法来避免冗余的计算，而不是需要精准的预估精度。</p>
<h3 id="尝试用更小的模型达到更优的效果">尝试用更小的模型达到更优的效果</h3>
<p>这件事价值不是特别大。</p>
<ol>
<li>不需要知道具体的比例，我们也知道，哪怕对于小模型，喂更多的数据可以达到更好的效果。</li>
<li>小模型的表达能力是有限的，所以也不是喂更多的数据就一定可以提升效果。</li>
</ol>
<p>于是哪怕做出了预估，也需要加好多限制条件，而实际应用场景也不多。</p>
<h3 id="其他">其他？</h3>
<p>昨天看完后，原本想说 Scaling Law 是个显然的结果，其规律并不蕴含更深层次的信息。但后来仔细想了想，可能还是有很多细节值得仔细的表述一下，以免遗漏什么可能性，所以写了这个文档。</p>
<p>总得来说，我对于 Scaling Law 并没有想到更深的应用场景，它所能表达的大概也只是：更多的数据、更大的模型（更多的模型参数）可以更好的拟合真实的概率分布。这件事对于机器学习来说，是自然的结论。这个规律几乎不涉及具体的模型形式——几乎只要是机器学习都符合这个规律。</p>
<p>所以从第一性原理角度出发，它算是一个数学上给出定性的存在性定理：我们的机器学习是可以不断优化的。但它不蕴含如何能更好地做优化的信息。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>这个定理的前置条件和证明过程这里就不赘述了，需要的话自己查一下。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>大部分的训练，增加迭代次数的方式都伴随着提供更多的训练样本，若模型距离收敛所需要的迭代次数比较多，例如如果 <code>学习律</code>（本质上就是 PID 中的 $K_p$）比较小，模型距离理论上限比较远，这时误差项主要不是来源于数据自身的误差，而是来自梯度下降逼近的误差，那么这个指数关系就会比较显著，对应的表象就是误差同数据量之间是指数关系。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>还可以增加每层的神经元数量（宽度），这种增加模式就不属于分层。类似于 Scaling Law 的规律，这种扩大的方式（形状变化）对于结果的影响不显著。当然，这件事是值得做实验，试一试少层数多神经元和多层数少神经元（参数总数一致）训练的结果是否一致。盲猜会有显著性能差异。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>Autorestic</title>
      <link>https://blog.uglyboy.cn/posts/2022/02/22/</link>
      <pubDate>Tue, 22 Feb 2022 06:44:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2022/02/22/</guid>
      <description>完善的数据自动备份系统</description>
      <content:encoded><![CDATA[<blockquote>
<p>原本是打算不再在这里写技术类博客了，结果最近的一个小成果确实没啥合适的地方存放，所以还是留在这里吧。</p></blockquote>
<p>自从开始使用树莓派，忧虑的一个重要问题就是：万一我的硬盘坏了可怎么办？通常的方案是硬盘组磁盘列阵，例如 raid1，raid5，raid10。可惜这套方案价格太高，不符合我用树莓派做 NAS 的风格。所以还是追求一套通常且通用的数据备份方案。</p>
<p>其间尝试了一些手段，例如：rclone，Duplicati，rsync，自己写 shell 脚本等等，但是如何构建一个完善的数据备份方案还是比较复杂的，需要考虑包括备份的可靠性，备份文件的大小（冗余程度），备份的版本管理，按不同时效留档等很多的要素。</p>
<p>最终，终于遇到了 <a href="https://autorestic.vercel.app/">automation</a> ，并依此打造了一套完善的备份方案。</p>
<h2 id="备份方案的基本原则">备份方案的基本原则</h2>
<ul>
<li>备份数据要保证本地非源数据的硬盘保存一份，云端保存一份；</li>
<li>云端备份需要有数据加密机制；</li>
<li>备份数据需要有类似 git 的版本管理机制，保证冗余数据不被重复存储，且按版本标签可方便的管理；</li>
<li>云端保存需要支持各种云端数据源；</li>
<li>本地需要有旧版本文件清理机制；</li>
<li>可以对不同的数据源进行不同的备份机制设定；</li>
<li>自动化管理备份，无需过多的人工干预和介入；</li>
</ul>
<h2 id="数据备份方案的基本组件">数据备份方案的基本组件</h2>
<h3 id="rclone">rclone</h3>
<p>最早我是使用 rclone + shell 进行备份的，但是这只能解决云端备份和支持数据源的部分，而且设定异常的复杂。根本原因是在于，rclone 其实是一款同步数据应用，而不是数据备份应用。</p>
<p>但现在有了一个良好的开端：可以将数据同步到任何云端网盘中了。</p>
<h3 id="restichttpsresticnet"><a href="https://restic.net/">restic</a></h3>
<p>这是一款类似于 rclone 的软件，但是不同的是，restic 是专注于备份的软件，支持加密传输，增量备份，快照记录等等，而且还可以同 rclone 联动，利用 rclone 支持多种云端的能力，将数据备份到各种网盘中。</p>
<p>另外，restic 也可以非常便捷的还原任何一个版本的数据，总得来说，是一个很简单便捷的备份工具。但它是一个命令行工具，也就是说，并不是一个服务，无法提供自动备份的功能（定时备份），而且每一项操作都需要运行相关命令加参数。</p>
<p>如此一来，关于备份这件事，就只剩下自动化版本管理这个问题需要解决了。</p>
<h3 id="autorestichttpsautoresticvercelapp"><a href="https://autorestic.vercel.app/">autorestic</a></h3>
<p>autorestic 是 restic 的一个「包装器」，通过自动调用 restic 的方法，加上了配置文件、定时执行（伪）等功能。将命令行程序扩展成了一个基于固定配置可重复运行的应用。</p>
<p>相关的命令说明还是需要自己看一下官方的文档。</p>
<p>但 autorestic 依然是一个命令行，不是服务，虽然提供了配置文件的方式可重复操作，但依然无法实现定时自动备份功能</p>
<h3 id="crontab">crontab</h3>
<p>autorestic 的官方文档推荐的方式即配合 crontab 每 5 分钟执行一次的方式 将 autorestic 配置成一个伪服务，进而提供定时自动备份功能。</p>
<h3 id="docker">docker</h3>
<p>最后，终极的解决方案，是需要将这些工具组合起来，形成一套完整的工具链。于是将对应的工具打包进 docker image，就可以便捷的部署和使用对应的自动化备份方案了。</p>
<h3 id="2023-12-18-更新">2023-12-18 更新</h3>
<ul>
<li>之前一直没有能自动进行 forget 操作，导致备份的数据越来越多。这个问题需要在 autorestic 的配置文件中添加 forget 的配置。</li>
<li>之前的镜像的 hostname 每次都是随机生成的，导致每次都会重新初始化，现在已经解决了这个问题，通过在 docker-compose 中添加 hostname，可以保证每次都是同一个 hostname 了。</li>
</ul>
<h2 id="最终的数据自动化备份解决方案">最终的数据自动化备份解决方案</h2>
<p>我通过 <a href="https://github.com/uglyboy-tl/autorestic-docker">Github</a> 的自动化流程，构建了实现上述 autorestic 服务的 <a href="https://hub.docker.com/r/guixi/autorestic">镜像</a> ，使用说明如下：</p>
<h3 id="features">Features</h3>
<p>Often it is usefully to trigger backups automatically. So in this image, it would be trigger the command every 5min.</p>
<h3 id="install">Install</h3>
<ol>
<li>Create an initial config file (<code>autorestic.yml</code>) such as:</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yml" data-lang="yml"><span class="line"><span class="cl"><span class="nt">locations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">my-location</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l">/data</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">to</span><span class="p">:</span><span class="w"> </span><span class="l">my-backend</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">forget</span><span class="p">:</span><span class="w"> </span><span class="l">prune</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cron</span><span class="p">:</span><span class="w"> </span><span class="s1">&#39;0 3 * * 0&#39;</span><span class="w"> </span><span class="c"># Every Sunday at 3:00</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>You can read full <a href="https://autorestic.vercel.app/config">docs</a> to configure it.</p>
<ol start="2">
<li>Create an empty file (<code>autorestic.lock.yml</code>)</li>
<li>run [[docker-compose]] as below:</li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;3&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">services</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">autorestic</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">guixi/autorestic</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="l">autorestic</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l">unless-stopped</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">hostname</span><span class="p">:</span><span class="w"> </span><span class="l">autorestic</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">/etc/localtime:/etc/localtime:ro</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">$(pwd)/autorestic.yml:/root/.autorestic.yml:ro</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">$(pwd)/autorestic.lock.yml:/root/.autorestic.lock.yml</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro </span><span class="w"> </span><span class="c">#optional</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="l">my-volume:/data</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><h3 id="usage">Usage</h3>
<p>you can use autorestic to show all buckups such as</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker <span class="nb">exec</span> -it autorestic autorestic <span class="nb">exec</span> -av -- snapshots
</span></span></code></pre></td></tr></table>
</div>
</div><p>and also use restic directly such as</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">docker <span class="nb">exec</span> -it autorestic restic
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="license">License</h3>
<p><a href="https://github.com/uglyboy-tl/autorestic-docker/blob/main/LICENSE">MIT</a> © Uglyboy</p>
]]></content:encoded>
    </item>
    <item>
      <title>组织理论</title>
      <link>https://blog.uglyboy.cn/posts/2021/11/11/</link>
      <pubDate>Thu, 11 Nov 2021 17:30:18 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2021/11/11/</guid>
      <description>&lt;h2 id=&#34;组织的基本构成&#34;&gt;组织的基本构成&lt;/h2&gt;
&lt;h3 id=&#34;非正式组织&#34;&gt;非正式组织&lt;/h3&gt;
&lt;p&gt;组织是可以自然产生和存在的，但往往短期存在的组织方式，会被人们用“&lt;strong&gt;合作&lt;/strong&gt;”这一说法取代。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假如有两个司机都要通过一条狭窄的山路，但路被一块巨石挡住了，至少两个人才搬得动。可以预想，前面的司机一定得把车停下来，等第二个人来，以解除力量上的限制。第二个司机同样也需要这样的合作。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<h2 id="组织的基本构成">组织的基本构成</h2>
<h3 id="非正式组织">非正式组织</h3>
<p>组织是可以自然产生和存在的，但往往短期存在的组织方式，会被人们用“<strong>合作</strong>”这一说法取代。</p>
<blockquote>
<p>假如有两个司机都要通过一条狭窄的山路，但路被一块巨石挡住了，至少两个人才搬得动。可以预想，前面的司机一定得把车停下来，等第二个人来，以解除力量上的限制。第二个司机同样也需要这样的合作。</p></blockquote>
<p>只要合作是“<strong>有效率</strong><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>”的，那么合作就天然存在持续的可能性。如果存在一个适合的自由市场，这种合作就可以通过交易的方式继续延续。</p>
<p>但天然就<strong>有效率</strong>的合作方式 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，往往意味着标准化的能力模型，和可替代的合作方 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。这样的合作模式就如同现在的劳动市场招工，进而可以形成一段时间内合作的组织。</p>
<p>以交易形成的合作方式，自然会非常容易以交易的形式被终结。例如当其中一个合作者找到了另一个更有效率的合作方时，就会终止与原有合作者的交易，转向新的合作方。例如跳槽就是典型的更换合作者的方式之一，类似的，当今的职业运动场上的各类交易，都充满着这种变化。</p>
<h3 id="正式组织">正式组织</h3>
<blockquote>
<p>上一节中的合作方式也可以形成组织，但仅以交易形成的组织依然被笔者归类到了非正式组织这一概念中，原因是这种合作有效率的组织模式依然缺少了一些通常人们想象中的组织应蕴含的一些灵魂。例如我们总是觉得组织是需要有组织文化的，组织是一定程度稳定的，组织是需要长效的，等等。所以在本章中笔者依然先以这样的名词来分别对待这两类组织，进而产出清楚组织的核心要点，后续的章节中，则会回避这种主观性很强的定义。</p></blockquote>
<p>生活中还有很常见的一类组织，大家搭伙一起做事情，却几乎不考虑<strong>效率</strong>，例如经常一起组团逛街的小伙伴，一起追星、聊八卦的小圈子，一起旅行的驴友等等。从这些组织的存在来看，效率并不是组织的必要条件，这也是为什么笔者不把恰当形式“合作”等同于正式组织的本质原因。</p>
<p>回到非正式组织的案例，从非正式向正式过渡的过程，一个核心变化是：向<strong>持续性</strong>转变。而以这个标准来考察上面提到的这类非功利组织，会发现依然是其之所以成为组织的核心要点。</p>
<p>所以组织的本质是“长效性”，而不是效率。</p>
<h4 id="长效性的经济学特点">长效性的经济学特点</h4>
<p>产业组织理论中认为，企业（组织）的价值是将交易内部化，进而降低了交易成本。</p>
<blockquote>
<p>为什么交易内部化就可以降低交易成本呢？甚至在一些大公司呆过的人会更有体会，内部交易反而会走向双寡头交易，相比于自由市场，交易成本甚至会提升。但更多的时候，我们还是觉得交易内部化才能解决更多的效率问题。</p>
<p>类似的，产业组织理论中也描述了企业扩张的路径：横向一体化和纵向一体化，其中纵向一体化就是希望上下游的产业都收归于组织内部，进而获得更好的效率。背后的原因，是企业内部才能获得更定制化的解决方案，进而获得效率的提升。或者用长效性来解读的话，就是稳定的长效性合作存在，才会带来针对性的改进，进而形成更好的<strong>社会分工</strong>，于是提升了效率，变相降低了交易成本。</p></blockquote>
<p>交易内部化，本质上是指组织的长效性，带来了组织内分工的针对性改变。这个结论也揭示了组织的核心特点：</p>
<ol>
<li>从最终表现来说，组织是追求长效性的合作方式；</li>
<li>从执行层面的特点来说，组织有能力在组织内驱动组织成员为之进行改变；</li>
</ol>
<blockquote>
<p>有了上面的第二点，就比较容易理解为什么大部分的组织的组织形式并不是制定一个自由市场让大家都能很低的交易成本的方式进行交易。因为对于自由市场中的角色，追求价值最大化是他们的目标，最优解决方案是找到更匹配当下自身产品特点的需求进行交易，而不是为了某种特定的目标改变自己的产品。尤其是当一个组织陷入困境时，需要组织中的人站出来拯救这个组织，而不是组织中的人都去寻找一个能获得更多利益的组织。这件事的发生，不是基于利益和效率的。</p></blockquote>
<p>换成另一个视角，更能让我们直观的体会到什么是组织（长效性）：</p>
<p><strong>组织成员愿意为一个“组织”而主动（不是被迫）改变的，这才是真正的组织。</strong></p>
<p>回到非正式组织的话题，如果为了完成某个商业目标，你雇佣了很多的人，通过经济利益达成了目标，这种组织，只是形式上的组织（合同），并不是真正的组织。这样的组织就会缺乏长效性，任何不稳定的扰动产生，都会使得合同失效。</p>
<blockquote>
<p>一个好的组织的案例，可以参考《海贼王》中三年之约，路飞的伙伴们为了未来能在伟大航路的后半段走得更远，都好好利用这三年的时间锻炼和提升自己。甚至更主观的一种感觉，当你视其他人为伙伴（而非雇员）时，那么你们这就是一个真实的组织了。</p>
<p>所以真实的组织中，不会有明确的边界，大家都会互相补位，去学习自己不擅长的事情，彼此改变以获得更好的配合。</p>
<p>再回归到第一节中提到的标准化能力模型这个案例，标准化的能力模型的产生：是先有了社会分工，才会逐渐形成每个分工的能力模型，然后才会进一步形成相同能力模型下的自由市场，标准能力成为了可替代性的部分。这部分的案例，后面的章节中讲到流程部分时也还会再次讨论。</p></blockquote>
<h4 id="长效性与效率">长效性与效率</h4>
<p>从前文的推演中，刻意区别了：长效性不代表效率。但是长效性在真实世界中，很多时候也都会带来对效率的正向影响。当人们追求长效性对效率的正向影响时，这种行为被成为“<strong>长期主义</strong>”。</p>
<p>最简单的长期主义的理解：人们需要通过学习来提升自己的认知水平，进而才有可能完成一些更高深的事情。所以很多有门槛的事情，是需要长期主义的坚持，最终“守得云开见月明”。所以生活中我们可以听到很多长期主义带来的共情的案例：谁谁谁坚持怎样的事情坚持了很久，最终得到回报。例如“卞和献璧”的故事：“吾非悲刖也，悲夫宝玉而题之以石，贞士而名之以诳，此吾所以悲也。”</p>
<p>长期主义似乎是人类共同的一个共情点，一种生来的追求。</p>
<p>但长期主义本真并不意味着效率。例如最近的新闻，一个考生为了上清华大学，已经复读和重考了 12 年。</p>
<p>正确方向的长期主义，最终可以带来效率的提升。但错误方向的长期主义，最终只能是向着相反的方向越走越远，这种长期主义，可以称为之固执、偏执等等另一类人们并不喜欢的品质特点。</p>
<p>幸好人们是可以通过历史的经验学习和进化的，所以对于长效如何能成长为长期主义，积累了一定的经验和认知，让长期主义的可靠性更强。但反过来，在组织这件事情上，尤其要留意长效性同长期主义的区别，组织是天然追求长效性的，但这并不意味着长期主义。</p>
<blockquote>
<p>比较典型的案例，宗教是有非常强的长效性的，但是宗教的长效性并不能像我们理解的那样，称之为长期主义。类似的，中国传统文明也是有非常强的长效性，但是在特定时期，这种长效性反而变成了进步的阻力，不仅没能形成长期主义，反而变成了固步自封，所以才需要打破传统文明中阻碍效率的共情点，寻找可以兼顾效率的共情，或者新的值得坚持的长期主义。</p></blockquote>
<h3 id="长效性的本质共情">长效性的本质：共情</h3>
<blockquote>
<p>长效性时组织的最终表现，是果而不是因。这一节要研究的，就是：究竟是什么因素，带来了长效性？</p></blockquote>
<p>回归到组织的成员的视角，什么会驱动一个人形成长效？</p>
<p>人的两类基本动机：功利动机和共情动机。其中功利动机是有目的的，于是更好的解决方案是如何能更有效的解决其目的。这类的解决方案，不会形成长效性，因为如果发现更好的解决方案，一定会抛弃原有解决方案。而共情的解决方案，是人基于已有的共识，去寻找新的共识的过程，这个过程本身就是长效的。长效性的本质，应当是 gong&rsquo;qing。</p>
<p>更具体的，我们透过<strong>共情</strong>来考察组织的形成，以及组织执行层面带来的特点：</p>
<p>大家愿不愿意合伙干事情，很大的因素不是能力，而是“是否知根知底”，或者“彼此相互认同”。如果都用共情来考虑的话，会很直观的概括成：彼此有共情；而反过来，如果一群人彼此有共情，则它们也很容易基于共情一起交流信息（创造新共情），这种形式是长效的，所以自然就会形成组织（例如一起逛街、一起八卦、一起旅行）。</p>
<p>另一个角度来看，共情是具有很强的对个体的改变能力的：</p>
<ol>
<li>人是有意愿因为共情而影响功利动机的选择的；</li>
<li>影响功利动机的改变，最终需要带来更好的共情体验；</li>
</ol>
<h4 id="案例-1婚姻和爱情">案例 1：婚姻和爱情</h4>
<p>爱情的出发点，大概率是人的动物性：荷尔蒙带来的冲动，吸引两个人彼此靠近。但荷尔蒙应当是典型的功利动机，带来的结果就是，理论上爱情应当不被所谓的伴侣所约束，是自由的；以及恋爱中的人不时会形成的灵魂拷问：“你是不是我的百分百女孩（男孩）？<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>”</p>
<p>但当爱情中的男女，超越了荷尔蒙，进而追求共情带来的快乐时，彼此一起经历过的事，形成的两个人才懂的编码，以及共同的价值观，就成为了他们最宝贵的财富。而这个过程中，双方会愿意改变自己，以获得更好的共情体验，当有了这种动力并最终形成了相当程度的共情后，他们两个人就真正成为了一个组织，走向了婚姻，组成了家庭。</p>
<p>所以能长久的爱情，是彼此愿意为对方改变，并且这种改变或妥协可以带来快乐的状态。这也是社会发展，总是能从动物性逐步走向稳定的家庭关系的原因之一。</p>
<p>而当今社会，越来越多的人更加自我，认为要找到一个无限包容自己的人，这样的选择是典型的功利动机，这样的人最终很容易就会发生类似于出轨，觉得厌烦现在的伴侣，追求新鲜和刺激等等的行为。但这种选择的结局也已经注定：他很难获得一个长效的组织，获得共情的体验。也就是有些很自我的人常抱怨的：为什么找不到一个懂我的人。</p>
<h4 id="案例-2组织中的加班文化">案例 2：组织中的加班文化</h4>
<p>当大家聊起 996，聊起企业的加班文化时，会深恶痛绝，觉得反人性，甚至举出很多企业的“无良”做法：提供晚餐、提供 9 点的班车、提供晚上的健身设施等等。但真正回归到自己身上时，往往加班的最直接原因是：周围的同事都没有走。是共情的因素导致了因此人们改变了自己的行为，为了更好的融入这个组织。</p>
<p>回归到组织长效性的角度来聊加班文化，加班本身如果不具有长效性的话，那么这样的组织动作也是没有实质的长期价值的。具体来讲，重要的不是加班与否，而是加班的人是否因此而感觉到更快乐。</p>
<p>例如大部分的创业企业，都要面临加班的问题。但是大家觉得是在为了自己的梦想奋斗，而且这种改变带来了更好的共情（离梦想更近），那么加班就不那么痛苦了。类似的，很多新员工在第一次参加双十一时，有种特别的荣誉感，并不会因此觉得痛苦，而是获得了更多的共情，这样的加班就是长效的，就是合适的。但如果大家对于加班这个话题已经变成了第一段中描述的那种心情，虽然还是共情带来的加班的结果，但这样的加班已经在损伤这种共情了，这就是对组织有伤害的事情了。</p>
<h3 id="业务组织的三个管理维度">业务组织的三个管理维度</h3>
<p>有共情就可以形成组织，婚姻是组织，家庭是组织，小团队也是组织。这类的组织更偏向于自组织形式——不需要很明显的组织力就会产生的组织。还有一类组织，例如商业组织（企业、工厂），职能组织（政府、奥运会），有时甚至是以具体价值为目的 <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> 而存在的组织。</p>
<p>这样的组织，共情点或着共识点中，一定离不开具体价值，所以产生价值的能力会显著影响组织的共情体验，进而这样的组织是需要追求效率的。这种组织我们称为业务组织。</p>
<p>针对业务组织，因为效率是其重要的共情点，所以这样的组织会非常具象的形成三个管理维度：</p>
<ol>
<li>面向过去，将过去的有效率的手段有效沉淀并应用于当下的，是流程管理；</li>
<li>面向现在，针对组织中各个角色信息不对称的情况，更好的调整整体的资源分配模式的，是制度管理；</li>
<li>面向未来，希望组织中的每个人都为了组织（共识）进行一些更有益的改变的，是组织文化管理；</li>
</ol>
<p>当然，针对到实操的对象时，往往三个维度会有重叠，典型的例子：对于企业中的最基本个体，员工，对他的管理，既要求其遵守流程，保证基本的价值贡献；又要求其在企业的制度下，可以更有效的发挥他的多方面能力，物尽其用；同时又会从组织文化的维度，希望他愿意为了企业的目标不断改善自我，更好的实现企业的目标。</p>
<h4 id="流程管理">流程管理</h4>
<p>流程管理是最传统的企业管理手段，而且是非常可量化（效果）的组织手段，例如节约了多少人力成本，提升了多少产能等等。而流程管理本身，是历史上的成体系的任务分工：每个工种的专业化，信息环节的高效化等等，最终共同作用的结果。</p>
<p>所以通常来说，一个领域中流程管理的方法是可以被沉淀，被大规模复制的。甚至可以部分跨行业进行复制。在组织管理者的维度来看，可以简单的通过交易的方式，直接获取更有效率的流程管理经验在组织中大规模复制；</p>
<h4 id="制度管理">制度管理</h4>
<p>制度管理有别于流程管理的重要点在于，制度管理试图解决的不是历史上曾经遇到过的已知问题，而是需要面向未来发展的过程中，待解决的问题。所以制度要解决的是很多信息不对称的问题（每个人都没有办法快速获得全局的变化信息），以及在这种条件下，大家行为的激励相容问题。</p>
<p>制度管理的最终目标依然是效率，但针对具体情况的不同，所需要的具体执行方式也不同。所以不同于流程管理，制度管理方面需要匹配组织自身的信息分布情况，以及因为制度本身是需要一定强制力的，但这种强制力是源于组织（也就是共情），所以具体制度在执行层面上也还需要契合组织的共情点，或者说组织的价值观。</p>
<p>这两点构成了：不同的组织会有不同的管理制度，而往往外来的制度无法非常有效的解决自己组织的问题。</p>
<h4 id="组织文化管理">组织文化管理</h4>
<p>组织文化管理这部分是当今最被误解的部分。</p>
<p>首先，组织文化不是解决效率问题的工具。如果尝试解决效率，要么是流程，要么是制度，所以有部分人对于组织文化的理解往往会等同于企业制度，认为文化是制度的衍生品，这种想法是错误的，甚至是相反的。是制度需要依照文化进行调整，而不是文化因制度而被塑造。</p>
<p>其次，组织文化解决的是组织的共识，解决的是组织中的人改变的意愿，更进一步，组织文化才真正解决组织的长效性问题，组织文化（共情）才是组织能否长期生存的最关键问题。一个没有效率的组织，只要组织文化还有活力，不一定哪天就会诞生一个合适的人帮助组织解决效率问题；但反过来一个有效率的组织，缺乏组织文化，那么任何扰动（例如增速变缓）都会导致组织中的人才的流失，进而丧失组织效率方面的竞争力，最终组织垮塌。</p>
<p>通常的组织管理类书籍，如果不能将组织文化部分的目标和制度部分的目标进行区别，以及无法正确的理解共情对功利的单向影响，其讨论中就会存在着一些难以逻辑自洽的部分。例如总也说不清制度（例如组织的结构）对组织文化的影响，以及组织文化这件事到底怎么带来效率？当组织文化和效率存在冲突的时候，应当如何选择？</p>
<blockquote>
<p>本文后续的内容，将更加专注地在组织文化方面进行更进一步的讨论，来理解怎样的组织文化才能带来更好的长效性，以及带来对效率的正向影响。</p></blockquote>
<h2 id="组织文化">组织文化</h2>
<h3 id="更好的共情">更好的共情</h3>
<p>前面花了大量的篇幅得到了结论：组织最本质的形成的驱动力是共情。有了共情基础，就可以自发的形成组织。</p>
<p>但形成组织，并不意味着这就是一个好的组织，高效的组织。甚至还存在着套用组织框架，但本质只是雇佣关系（很弱的共情）的非自然组织。</p>
<p>从长效性出发，可以用组织成员为组织改变的意愿，也就是贡献的意愿来衡量组织的好坏。于是基于 &ldquo;<strong>共情理论</strong>&rdquo; 也就非常容易的得到结论：更好的共情可以带来更好的组织成员贡献的意愿。</p>
<p>这里需要进一步将共情理论中的要点跟组织结合，拆解出：更具体的，哪些因素可以带来更好的共情。</p>
<h4 id="组织需要有共识">组织需要有共识</h4>
<p>首先，组织的共情基础，是组织中的这一群人有相同的共情（共识）。同时，这个共同的共识，也是组织成员产生贡献的意愿的方向。</p>
<p>如果一个人同组织的整体共识不相容，那么他其实只是组织的雇员，当有更好的利益机会存在时，他很自然就会离开组织。同时，在没有利益刺激的条件下，他也没有任何为组织贡献的意愿。如果他加班，大概率只是为了更好的绩效，而不是为了组织而改变自我。一切行为都是可以用价钱来衡量和影响的。</p>
<blockquote>
<p>进一步来讲，如果组织存在一些有一定自由度，涉及到选择，且不容易被量化的工作，这样的雇员会找到一种个人利益最大化的方式来工作，例如在更有自由度的工作中偷懒，或者谋私利。这件事就是传统经济学中一直较难讨论清楚的，职业经理人的激励相融的机制设计问题。尤其是，这样的工作，在目标已经很明确的项目中，还可以用共同利益来解决，但对于目标不仅仅是功利结果的项目，非常容易带来跟原本组织不一致的共识。这也是很多案例中，外来的和尚没能拯救企业，很多伟大的企业需要自己培养起来的领导者带领企业前进的原因。</p></blockquote>
<p>对于业务组织来说，业务方向必须是组织的共识之一，因为需要组织成员为业务目标贡献。所以对于业务组织而言，所谓的战略必须是大部分组织成员可以达成共识的方向，否则如果只是领导拍板的方向，在执行层面上也一定会遇到各种不认同，各种挑战，反而会带来大量的沟通成本和效率的降低，以及在需要执行者的贡献意愿帮助业务时，得不到助力。而且这个问题不是依靠组织架构、制度设计能解决的问题。</p>
<p>极端情况，如果领导者认为正确的战略方向和组织的共识是不同的，那么宁可选择一个和组织共识更相融的不那么正确的方向，也不能强行推进一个无法达成共识的方向。关于这一点，在后文 &quot; 人设不能垮 &quot; 部分还会进一步阐述。</p>
<p>组织的共情，只要有共情，就能带来好的效果。例如，聚焦到组织中的小组织——某个具体的业务部门，有明确的业务目标，这样的小部门里，只要部门的同事之间的共情非常的好，哪怕都是基于内容的共情（曾经一起扛过枪，一起参加了团建，或者只是互相聊得来），对部门贡献的意愿就会非常的强。这一点也重新验证了共情本身是不区分究竟是编码的共情，还是内容的共情，抑或是观点的共情。</p>
<p>但扩大到组织维度，尤其是很多大公司，甚至国家这种超级大组织，基于内容的共情往往就有些力不从心——太容易被新的内容覆盖，导致内部共情出现分歧，影响共情效率。所以构建共情的编码，和基于观点的共情，可以带来更好的效果。</p>
<p>上升到国家维度，共同的语言就是最最重要的共情编码；而价值观，就是基于观点维度的共情。</p>
<blockquote>
<p>在阿里，通过花名文化，武侠文化，其实构筑了很好的共情编码；以及后续的阿里价值观的搭建，也是非常契合于共情的观点的。阿里主要的问题，是组织没有能将这种编码和观点维持成组织的共情。当人员也会发生变化时，共情是有偏移的可能性的，这也是在《社交与传播》的研究中探讨的，关于社区较难扩大的原因。</p>
<p>更进一步来聊阿里的一些具体的做法，招聘时，寻找有阿里味的人的做法是契合和恰当的。但这件事其实并没有那么的重要，更重要的是，新加入阿里的人，如何逐渐让他们都具有阿里味，以及大家都对于阿里的价值观有共情。当这种共情需要用制度来约束时，已经不再是共情了，类似的做法只会让阿里在共情这件事上进一步下滑。</p>
<p>就现在的阿里来说，大家对阿里的共情可能只剩下基于内容的共情了。这样的企业，可能只有创始人重新回归，才能带来曾经的共情的回归和重现？当然，马老师自己大概也立不住人设了。</p></blockquote>
<h5 id="基于内容的共情可能带来共情的分化">基于内容的共情可能带来共情的分化</h5>
<p>对于很多的创业公司，合伙人往往是曾经的同事。同事彼此聚集在大组织的组织共情下，逐步又会形成小团体的共情，而且小团体的共情在内容共情的维度上随着时间的推进会越来越强（当然也会有边际效应）。所以这种情况就可能带来小团体的共情强过大组织的共情，小团体内部的人员会对小团体有更强的组织感。</p>
<blockquote>
<p>组织内部希望杜绝小团体主义，但小团体的形成是无法避免的——毕竟小团体的这群人才有可能产生越来越多的内容共情。所以对于组织而言，尤为重要的就是在共情的编码和观点层面做建设。</p>
<p>当然，我们也看到一些组织为了避免小团体，对于各种部门经常进行拆解、重组，对部门负责人进行轮换管理。但这样的作法一方面本身在组织共情方面就是减分的操作，另一方面，如果小团体已经形成一定的良好的共情，这种拆解治标不治本，甚至会让小团体的共情对组织更加抵触，进一步激发小团体的共情同组织共情的分裂。</p></blockquote>
<h4 id="更高的共情的频率">更高的共情的频率</h4>
<p>像阿里这样的企业，曾经都形成过很好的组织共情，但依然在扩张和追逐利益的过程中，丢失了优质的组织共情。所以对于组织而言，如何维持共情也是十分重要的环节。</p>
<p>有了共情理论，这件事变得不再那么虚无缥缈——在组织成员的主要信息渠道里，不断提升组织共情消息的共情效率即可。简单说，就是更高的共情频率。</p>
<p>按渠道来区分，大体可以分为三类：</p>
<ol>
<li>组织自己控制的信息渠道下，提供更多的组织共情（企业价值观）的信息；</li>
<li>组织内部的人际传播中，激发更多的符合组织共情的信息出现和传播；</li>
<li>组织外部的社会，对于组织的信息反馈中，是否能提供符合组织共情的信息；</li>
</ol>
<p>这里拿两个具体的例子来加深理解：</p>
<blockquote>
<p>组织中进行的评优活动，其实是很好的提升组织共情频率的活动，其目标不是为了奖励绩效高的员工（这种利益激励，通过制度和绩效考核就可以完整覆盖了），而是通过这样的形式，带来组织共情的更好的传播：</p>
<ol>
<li>评优活动，可以让更多的符合组织共情的信息（事件）出现，创造更多的信息；</li>
<li>评优活动本身的宣传，就是企业在自己控制的信息渠道下，提供更多的组织共情；</li>
<li>评优活动中需要群众参与的部分，就是激发群众更好的人际传播组织共情的方式。</li>
</ol>
<p>所以对于组织的评优活动本身，奖励和公正等因素不是最重要的，最重要的是组织共情的传播。所以如果恰当的时机，扩大评优的获奖数量，是更合适的做法。当然，这个过程也需要慎重，因为如果存在不符合组织共情的事件被当作组织共情传播，或者未来其中有人人设崩塌，这件事带来的危害性就极大了。</p></blockquote>
<blockquote>
<p>如果一个组织的组织共情具有外部性，带来的是对外部社会的价值。那么外部社会对组织的信息反馈就会不断的贡献符合组织共情的信息。这样的组织，哪怕内部并没有做很多提升组织共情效率的工作，也可能具有很强的组织共情效率。</p>
<p>例如一些公益组织。</p>
<p>另一方面，有很多高速发展的企业，给社会或者被企业的客户带来了巨大的真实价值。他们的组织成员可以很容易的从外部获得组织共情，所以往往对于高速发展的企业，组织共情问题可以很好的被掩盖，因为外部传播部分解决了这个问题。但这种状况不是长效可持续的——当组织发展速度变缓，或者业务模式成为了常态，外部反馈的组织共情的效率就会急速的下降，组织的成员就会产生巨大的心理反差，会觉得组织变了，不再是曾经的那个组织了。</p>
<p>这也是想说明，组织文化的建设不能等，尤其不能因为组织的高速发展、问题不体现就忽视。这样的话，在组织放慢脚步时，问题会被更突出的爆发出来，造成严重的影响。</p></blockquote>
<p>其中，对于激发组织成员贡献组织共情的信息，理论上来说应当是最有价值的事情之一。所以如果一个公司的员工在脉脉上经常聊的是在组织共情带来的快乐，那这样的组织大概率是非常有长效性的，组织内的员工贡献的意愿会尤其强烈；反过来，如果脉脉上吐槽的都是组织共情的表里不一，那这个组织其实已经病入膏肓了。</p>
<p>最终组织成员认同的是组织共情，甚至大家会用组织共情来指代组织；但做组织相关工作的人千万不可以将组织成员认同的主体等同于组织自身，组织自身有可能成为组织共情 ，但没有必然性。只有做到了相当程度的组织共情，将组织自身发展成了共情编码后，这种等同关系才能初步建立。</p>
<h4 id="人设不能垮哪怕影响效率">人设不能垮，哪怕影响效率</h4>
<blockquote>
<p>从共情理论出发，一个重要的启示就是，人设不能垮。这一点无论是放到明星身上，放到社交类 APP 上，还是线下商业实体、品牌、IP 等等，都无一不应验。而现在，它又出现在了组织文化方面。</p></blockquote>
<p>这里说的人设就是组织共情。组织共情包含组织的价值观、组织的战略方向、组织的业务目标，以及组织的历史，组织中人和人的羁绊（彼此共同的经历）、组织的黑话等等。这些共同组成了组织共情，形成了组织的人设，这些一切构成了让组织成员为之贡献的动机。</p>
<p>所以为了这种动机，已经形成的人设不能垮塌，更具体来说就是需要有前后的一致性，不能说改就改。哪怕调整，也要沿着组织共情的精神进行改良和调整，不能破而后立。</p>
<blockquote>
<p>回到阿里的案例，月饼事件就是阿里人设崩塌的典型案例，虽然阿里的做法是试图保人设，哪怕牺牲效率。但全程没有任何共情的信息传递，只是流程式地说违反了价值观，而且价值观的衡量也不应该是规则式的，更应当是道德式的。这部分下一章会讨论。</p>
<p>但重点是，月饼事件让大部分阿里人（和外面的人）不再相信阿里的价值观，后面组织内就快速滋生了各种没有组织共识的现象，大家都开始用利益的方式来指导行为，所谓的组织共情只剩下空壳了。在这种情况下，不是去重新梳理和引导新的组织共识，而是用强制的手段，用制度来约束共识，只会让组织成员愈发没有组织共识，对组织的价值观感到困惑。</p>
<p>所以阿里根上的问题是人设垮塌了，这种情况挽救起来就非常困难了。</p></blockquote>
<p>当人设和组织目标有冲突，必须二选一时，对组织而言，人设才是必须要保的部分。</p>
<blockquote>
<p>谷歌曾经的价值观是“不作恶”，所以当谷歌被爆出和美国军方合作，利用人工智能执行军事行为时，哪怕有再大的经济利益，这种行为都是对人设的极大颠覆。对组织而言就是危险的。</p></blockquote>
<blockquote>
<p>暴雪的人设是：“暴雪出品，必出精品”，这种人设意味着消费者可以对暴雪跳票的行为和高售价等都有非常大的包容性，但是绝对无法容忍暴雪尝试出一款追求潮流的，平庸的作品。当然，这种人设很难保持，也是暴雪今天面临困境的重要原因。</p>
<p>但与之同时，很多暴雪出来的员工，依然坚持着暴雪的组织共情，他们的作品也同样得到了消费者的包容和认可。所以换个角度说，暴雪这家公司虽然已经不太成了，甚至丢掉了原本的组织共情，但只要原来的这些人还坚持着这种组织共情，其实这个组织就还活着，只是换了种形式，换了个名称而已。</p></blockquote>
<p>组织从来都不是某个资本下的框架，也不是特定的几个人，而是由组织共情聚拢的一群人，这群人可能会变，但只要组织共情没有变，这个组织就依然存在着。写到这里突然让我很激动的想喊一句九州世界里跨越了无数个时代依然存在的组织——“天驱”的口号：<strong>铁甲依然在</strong>！</p>
<p>组织共情，经历无数演化后，可能会留下很多故事（内容的共情），但最终最精髓的部分，一定会凝汇成共情的编码，以及共情的观点，可以超脱于时代流传，哪怕曾经的共情的人们都不在了，共同信仰的人们也可以穿越时空产生共情，让组织延续。这大概就是组织传承中常用的说法：“后继有人”吧。</p>
<h3 id="法律与道德">法律与道德</h3>
<p>法律要追寻的是每一次的正确执行；道德追寻的是整体的共情效率；所以社会并不要求每个人都是道德上的完人，当道德和利益冲突的时候，并不是必须道德优先的，而是在一个良好的平衡下，寻找更大概率可以实现道德的途径。</p>
<p>法律是国家这个组织的制度管理手段；道德是国家这个组织的组织文化管理。类似的道理，对于任何一个组织都是成立的。</p>
<p>当组织有具体的业务目标时，需要规矩（制度管理）来保证效率。所以组织的规矩就是规矩，是不可以被打破的。但是回到组织共情，它不是一个考核标准，不是一个可以判断一切行为的准则。它尝试平衡的是每个人做事后，获得共情的体验，这件事本身就会因人而异，不应该用统一标准衡量。另一方面，它并不是业务结果导向的，而是共情结果导向的，所以可以理解成，组织共情是主观的，不存在完美的客观标准。当然，组织共情也是一直在变化的，这更加增加了客观衡量的难度。</p>
<p>回归到具体问题来说，如果我们希望组织共情能够更好的影响行为（某冲程度上也可以表述成，提高组织成员的贡献意愿），那最重要的点，还是当用户的行为产生后，是否获得了更多的共情反馈，而不是用行为规范准则进行强制的约束。</p>
<blockquote>
<p>所以，如果一个组织的价值观开始用绩效的方式考核，这个组织的组织共情估计离破灭就不远了。这也是阿里加速崩塌的原因之一。</p></blockquote>
<p>如果把组织共情用“价值观”这个说法来描述，那价值观是动机，是行为，但未必能带来结果。但无论其是否带来结果，只要是契合企业价值观的行为发生，就值得被给与认可和更多共情的反馈</p>
<h4 id="给功利以物质奖励给共情以共情奖励">给功利以物质奖励，给共情以共情奖励</h4>
<p>游戏化理论中已经有过论述，错误的激励会改变动机。所以针对组织管理中的制度管理部分和组织文化管理部分，解决的是不同的问题，也应当对不同的行为进行不同的奖励。</p>
<p>所以对于组织共情方面表现优秀的人，应当给予他更多的共情奖励，例如更多的荣誉，更多的事迹宣传，更多的与其他人进行价值观方面交流的机会。但是这件事和他带来的物质结果是需要区别对待的，不应当因此给他更好的绩效，更多的奖金，更多的物质福利。</p>
<blockquote>
<p>当你用奖金的方式激励敬业，那最终加班的人只会为了加班费加班，在没有加班费的时候，绝对不会再来加班的。</p></blockquote>
<p>而且，对于组织共情方面优秀的人而言，物质的奖励有时甚至会让他们觉得变味儿，觉得你在用钱侮辱他们的情怀。很多组织中都有道德楷模，他们需要的是被认可，而不是试图通过道德楷模的做法牟利，否则，他们就不会选择走道德楷模这条路。</p>
<p>进一步，回到“贡献的意愿”这个维度，我们每个人都可能在某个特定的节点上产生贡献的意愿，这个时候，这种意愿背后蕴含的往往是不在意物质方面的得失，更在意的是自我的精神满足。</p>
<blockquote>
<p>鸿星尔克的“野性消费”事件，就是消费者对于鸿星尔克长期坚持的行为的共情反馈，而鸿星尔克的后续行为也非常的恰当，不是打折回馈消费者（利益刺激），而是通过直播间的主播劝消费者要理性消费，从消费者视角出发给与共情的反馈。</p></blockquote>
<h4 id="组织共情不需要惩罚">组织共情不需要惩罚</h4>
<p>组织共情中的奖励，一方面是为了给与更多的共情，激发员工贡献的意愿；另一方面也是通过奖励来宣传组织共情，形成更高的共情频率；</p>
<p>反过来，组织共情如果需要惩罚，惩罚的是什么？或者这种惩罚能带来什么共情效率的提升吗？</p>
<p>对于组织而言，组织共情是提供给组织成员的，所以不存在组织成员在组织共情方面做得不好，只存在组织没有能给组织成员提供好的组织共情，亦或者这个成员跟组织原本就没有共情。所以如果组织共情需要惩罚，那被惩罚的主体也应该是组织自身，而不是任何组织中的一员。这也回归到了本章节的对比：法律同道德，法律是要有惩罚的，但道德没有。</p>
<p>具体员工身上确实也有可能出现对于不符合组织共情的事情，或者把这称之为创造了不符合组织共情的信息。这种信息本身就不应该频繁出现在组织成员的信息体系中，会降低组织共情的频率。所以对于这样的信息，应当尽量避免其转播，或者创造逆向传播（传播这样的信息是不对的）。所以总得来说，惩罚这种动作本身可能还会带来不恰当的信息传播降低共情，组织共情维度并不需要惩罚。</p>
<p>如果真的遇到无法与组织共情的员工，那么直接好聚好散就可以了（这只能说不适合，没有对错），也没有必要进行惩罚。</p>
<p>当组织共情足够强大时，无法共情的人其实在这里存在着，就相当于一直接受着酷刑了。类似于道德标准对人的道德批判，效果是远大于所谓的物质惩罚的。</p>
<h4 id="组织共情是需要培养的">组织共情是需要培养的</h4>
<p>组织尝试找出契合组织共情的员工，这件事是合理的。但对于组织而言，更重要的是让新员工逐渐成长和改变，成为契合组织共情的员工。这种促使人成长的能力，是组织共情中最重要的能力。</p>
<p>手段上，共情理论已经有了很多的解法：可以通过编码、内容的共情，逐渐形成观点的共情。因为组织中的伙伴彼此都是共事的关系，所以在内容的共情方面会很容易积累（例如多搞点团建，内容的共情就会非常丰富），之后如果组织原本就有良好的组织共情，新员工就会从老员工那里获得组织共情（观点）方面的影响，逐渐就会成长为具有相同组织共情的老员工。</p>
<blockquote>
<p>足球俱乐部的球迷就具有类似的特点——一个球迷会极大的影响自己的圈子都成为这个俱乐部的球迷，甚至让自己的后代也都持续的喜欢同一个俱乐部。</p></blockquote>
<p>总结一下：组织的共情对于加入组织的人来说，并不是天然就会形成的，而是需要经历一个成长的过程。这一点也是前文中提到的，往往企业的优秀接班人是成长于企业内部，而不是外来者，就是这样的原因。</p>
<h3 id="组织的领导者">组织的领导者</h3>
<p>共情理论中推演过，任何一群有群体共情的人，都会自发产生群体的意见领袖。也就是彼此的共情关系会从网状结构自发过渡到星状结构，并且这种星状结构，以及星点的 KOL ，都是稳定的。将这样的结论带入到组织中来，星状结构的星点，自然就是组织的领导者。</p>
<blockquote>
<p>这一点也很有意思——世界上形形色色的自发形成的组织（团体），几乎从来都没有平权的，甚至是分权的，都是明显的星状结构。这一点背后是由共情的特性决定的，跟集权、政治、管理效率等等都没有直接的关系。</p></blockquote>
<p>因为组织的结构最终一定会成为一个星状结构，这件事也决定了，组织的领导者，对于组织共情、组织的长效性等等各方面，起着决定性的因素。而在组织效率、组织能力等方面，反而并不完全依赖组织的领导者。也许一个效率能力方面比较弱的领导，其带领的组织并不能快速发展成一个引人注目的业务组织，但只要领导有足够的组织共情能力，这个组织可以跨越时间的壁垒，用时间来慢慢积累所需要的业务技能，最终或许会在业务深度上，抛开其他的追求效率的组织，成为最后的赢家（这件事就是前面《长效性与效率》章节中描述的事情）。在外界看来，也许这是长期主义的胜利，也有可能是这个组织领导者不得已的选择。</p>
<h4 id="领导者核心能力是共情">领导者核心能力是共情</h4>
<p>一些企业招聘管培生时，往往有一个重要的环节，叫做群面，就是让一群应聘者在一起准备同一个问题，然后通过大家讨论的表现，最终来判断谁更有领导力。关于领导力，其实就是在一群笔记都不熟悉的人坐在一起时，看谁更能跟群体中的每一个人形成良好的共情，然后推动集体共情的形成。这件事，并不完全依赖这个人的业务水平，他可以推动大家沿着同一个共情去贡献新共情，然后由他自己作为信息交流的枢纽，带动群体共情。</p>
<p>类似的，任何一个组织的领导，最核心的就是塑造和持续维护共情的能力。极端情况甚至这个领导都不需要拥有自己产生共情的能力，只要能发挥好星点的传导作用，就可以很好的维持这个组织的长效性，和贡献的意愿。</p>
<p>当然，如果这个领导者拥有产生共情的能力，甚至他自己就能成为共情，那么这个组织的组织共情就更容易维护。</p>
<p>很多的组织（尤其是小组织），其组织的领导者就是组织的共情——员工并不是因为信某个理念而聚集在一起，而是因为信某个人而聚集在一起。这样的组织在初期会很顺利——因为领导者就是共情本身，领导者的想法就会化为组织的想法，组织的执行力会非常的强；但当组织发展到一定程度，领导者自身能力就会显著成为组织的瓶颈，并且比较难调整，例如引入外部技术或思想，如果不能变成领导者自己可以理解的内容，就无法被组织所接受，组织的上界就会被领导者所束缚。</p>
<p>但如果领导者可以将组织共情升华，上升为某种组织共情的观点，不再是领导者自身（编码），那么组织就会有更好的宽容度，尝试与组织共情的观点有契合点的方法，不再拘泥于领导者自身的理解和认识。</p>
<blockquote>
<p>马老师是个非常好的组织领导者，其长处就是在于组织共情方面。而关于组织效率的问题，是可以通过其他的种种方式来解决的。但老逍直到今天，大概都还不能成为阿里巴巴的共情 KOL，所以马老师”退位“后，某种意义上，组织是缺少领导者的。哪怕从制度维度，从效率维度，老逍都做得很好，但是这件事并不是非得在组织领导者这个位置上完成的。</p>
<p>至于快手，所谓的双合伙人模式，其实就是彻头彻尾的失败。哪怕曾经的效率方面可以实现组织的前进，但是最终依然会败倒在组织无法形成合适的 KOL，进一步完全无法形成恰当的组织共情上。</p></blockquote>
<p>反过来，现在很多企业中，部门的领导是由业绩更好的员工来担任。这件事有一点点的契合度——能为业务企业带来更好的业务的人，更可能拥有比较好的企业共情，但这件事并没有任何的确定性，而且很多业绩好的人，没不具备合适的共情能力，甚至作为组织领导者的共情能力，也不是通过培养就可以形成的。这就会导致大部分的企业在中层管理者的环节会出现问题。</p>
<blockquote>
<p>通过业绩来选择管理者的制度，在管理类的组织是天然契合的。例如政府、例如管理层的人员的晋升变迁。但这件事对于强业务导向的部门是不合适的。</p>
<p>过去中国的国企是通过双轨的方式进行相关的管理，来解决业务部门业务决策需要业务性强的人，但组织管理需要真正的组织领导者的问题：部门的领导是业务领导，但同时会配备党政的书记进行组织共情管理。这种结构下，熟悉中国体制的人都知道，真正的一号位是书记，而不是部门负责人。</p>
<p>这种模式，不仅仅是加强党对企业的领导，更多的是通过一套相对成熟的组织共情体系，帮助这些企业解决长效性的问题。</p>
<p>所以反过来，这些国企哪怕一段时间内的效率可能比不上社会上的私企，但是企业的稳定性、韧性是更强的。当然，谈到国企的问题，无法避免的会牵扯太多其他的因素进来，可能就不是很容易说清楚背后的原理了。所以这个话题也不再深究。</p></blockquote>
<h4 id="领导者的共情能力">领导者的共情能力</h4>
<p>领导者的核心能力是共情。进一步，领导者之间也有”能力“的差别，所以这一节会通过共情理论，来梳理一下领导者的共情能力，会在哪些环节体现，以及如何进一步发挥领导者的共情能力。</p>
<h5 id="维护领导者地位">维护领导者地位</h5>
<p>维护领导者地位，目的不是为了权力，而是为了避免组织共情的分裂。所以如果一个领导者的领导力不足，组织内就会自发出现新的星点，并且带着这个组织的很多人沿着新的组织共情前进，逐渐的就会与原有的组织分道扬镳。所以如何维护好领导者地位首先就是领导者必须要解决的基本问题。</p>
<p>回归到 KOL 形成的方式和演化的路径来说，<strong>领导者需要是所有组织成员基于组织共情最优秀的共情渠道</strong>。把这个描述细化，就拆解成如下三个核心点：</p>
<ol>
<li>领导者需要深刻的了解和理解组织的组织共情；</li>
<li>领导者可以通过种种手段创造出组织共情的新信息（哪怕是转述）；</li>
<li>对于任何一个组织成员而言，领导者需要有更合适的渠道跟组织成员进行高频率的共情，并且要保证共情效率是组织中最高的；</li>
</ol>
<blockquote>
<p>从结果来说，愿意跟每一个员工聊天的领导，一定是更好的组织共情的领导；但有时业务领导可能无法有足够的时间跟组织中的每一个人构成足够高频的一一链接，那么就需要用一些其他的方式来扩展自己同组织成员沟通的渠道，提升共情效率。例如国家领导人可以通过”语录“、”选集“、”讲话“等方式，跟全国的人民进行共情；政府和企业会有领导人办公室，以领导者的身份，帮助领导者同更多的人建立共情。</p>
<p>特朗普的 twitter 执政，其实也是十分有效的手段：他自己与某一个组织成员的共情，通过 twitter ，就变成了与组织所有成员的共情。</p></blockquote>
<p>大多的业务领导者，是通过上述的第一点——对组织共情有更深的理解和了解，进而可以产生更契合组织共情的内容的方式，来提升自身在组织中的共情效率的。换句话说，是通过共情质量提升共情效率。但伴随而来的，就是很多业务领导者对于频率的忽视，甚至是傲慢——”只有我最懂组织共情“。</p>
<p>但组织共情本身就是组织中所有人共同形成和营造出来的结果，虽然组织领导者（KOL）有一定的能力影响组织共情，但是当组织领导者脱离客观的共情，自以为是的构想新共情，那他很快就不具备组织领导者的信息中心地位，起不到应有的作用了。</p>
<p>换句话说，其实组织领导者需要是一个好的学习者，认真的学习组织共情的变化，并能够同新的组织共情产生共鸣，这样才能更好的维护领导者地位。</p>
<blockquote>
<p>当某个明星的粉丝圈子逐渐形成了一些圈子文化，圈子内部黑话（编码）时，如果明星也能跟粉丝互动时，使用这种编码，会带来粉丝对明星的更剧烈的共情，为组织奉献的意愿也会更强。类似的情况其实放到企业里也是一样的。</p></blockquote>
<h5 id="发挥领导者作用">发挥领导者作用</h5>
<p>这里重点时两方面作用：</p>
<ol>
<li>提升组织共情效率；</li>
<li>对于业务组织而言，引导更有业务价值的新组织共情；</li>
</ol>
<p>其中的第一点其实在上面好多个章节中都反复探讨过了，无论是组织共情自身的需要，还是领导者维护领导者地位的需要，都会自然带来更好的组织共情效率，提升组织的长效性。</p>
<p>而第二点，则是业务组织特有的需求——当组织的目标时追求业务目标时，这个目标除了共情（感性部分）以外，必须辅以理性的部分，因为业务问题是有更明确的对错，更明显的效率差异的。纯基于业务讨论，或许可以找到非常理性的业务新方向，但是这种业务新方向如何变成组织共情的一部分，让组织成员都有贡献的意愿，帮助完成更合理的业务新方向，这需要的不是科学的解释，而是需要组织领导者（KOL）进行共情的引导。</p>
<p>这种引导几乎只能依靠组织领导者完成（这是 KOL 的拓扑结构决定的），否则，那其实意味着名义上的组织领导者并不是组织真正的 KOL。</p>
<p>组织的领导者，需要基于已有的共情点，去延伸出新的共情点，并通过自己的影响力，让组织的每一个人都相信新共情，最终通过人际传播，强化新共情。</p>
<blockquote>
<p>如果一个部门的领导，一个组织的领导，还需要借助很多其他的力量才能说服自己组织内部，统一思想，那么要么他想说的事情几乎跟组织共情不相容，这样的事情到了执行层面也会出问题；要么这个领导其实就没有 KOL 的影响力，这样的组织很难在领导这里形成组织共情，或者说组织共识，无论什么样的决策，都会受到大家的挑战的。</p></blockquote>
<blockquote>
<p>这里顺便可以聊一下组织基因的问题。看过很多的组织，我们会形成一种认识：组织的基因不同，做不同事情的结果就会不一样。逻辑上，明明大家都是相信科学的，都试图用更效率的方式解决问题，可为什么会受一个叫做”组织基因“的非理性因素影响，导致结果发生变化呢？从组织共情来看，就是符合组织共情的方向，会显著降低沟通成本，不容易形成沟通中信息传递的失真，最终就可以更顺利的将理性部分发挥出来；但不符合组织共情的方向，后验的理解会显著收到组织共情这种先验的影响，最终和所谓理性的解决方案形成巨大的差异。</p>
<p>所以对于组织决策而言，不存在绝对理性正确的选择，必须寻找基于组织共情下的理性解决方案。否则，就只能重建组织了。</p></blockquote>
<p>只有组织的领导者有可能调整组织共情；同时组织领导者调整组织共情的能力也不是无穷无尽的。</p>
<p>当然，组织的组织共情自身也是一直在演化的，而且这种演化也会收到例如流程、制度等管理维度的影响。但总得来说，这种演化是属于复杂系统的，是不完全可控的（尤其相比与组织领导者的影响力来说）。</p>
<blockquote>
<p>曾经笔者尝试从制度的角度去推演组织共情的演化，最终倒在了复杂系统上。</p>
<p>是否存在完全不依赖领导者，只需要有合适的制度，便能长效繁荣的组织？现在看来，只要是组织，就会出现领导者，就会更大程度的受领导者影响，而不是自演化的影响。</p>
<p>也许在未来的某一天，复杂系统的科学解读更加完善，并且可以拿来推演制度带来的组织共情变迁，届时可以尝试一种全新的制度——要求组织没有领导者的制度，并且这种制度可以更好的带来组织自演化和效率的提升。但在今时今日，还看不到这样的方式的逻辑自洽性。</p></blockquote>
<h4 id="领导者的接班人问题">领导者的接班人问题</h4>
<p>领导者的接班人问题，是每个组织幸福并痛苦的烦恼。因为如果一个组织涉及接班人问题，一定意味着组织的长效性已经得到了验证和保证；但另一方面，接班人则意味着一定程度上需要打破原有的组织的组织共情。</p>
<p>因为组织的共情结构，决定了组织领导者的星状结构，进而也决定了想要顺利的从组织中的一个星状结构，平滑的过渡为另一个星为核心的星状结构是不自然的。所以大部分组织的接班人过渡都可能成为组织崩塌或者分裂的根源。</p>
<p>这里我们依然考察自然的组织可能发生的情况：当组织拥有很好的组织共情，并且组织中有一个恰当合适的组织中心（领导者）时，这个组织的结构是稳定的。如果此时将组织中的中心点（领导者）拿掉，只要组织共情在一段时间内没有变化，那么新的无中心的组织，依然会自发的构建成网状结构后，重新进化为星状结构。当然，新的星状结构的中心点与被拿掉的中心点相比，可能在刚形成的一小段时间里，共情能力是不如原有中心点的，但随着新的组织中心进一步演化，终究是可以成长为具有高度共情能力的中心。</p>
<p>于是乎，合理的组织领导者接班人计划的全部要点就自然浮现了：</p>
<ol>
<li>原有的中心点退位（并且不要再试图发挥共情作用了）；</li>
<li>新的潜在中心点需要是在平均共情能力上，相较于其他节点，有更强的能力；或者说，他要更懂得更理解这个组织的组织共情；不能脱离原有共情发展新共情（至少在还没能成为组织真正的共情节点前，不要做这样的尝试）；</li>
<li>新的潜在的中心点，可以直接被放在更靠近中心的位置，便于其更高频的与其他节点共情，加速其成为新的中心节点的进程；</li>
<li>不要有其他的潜在中心节点竞争，否则可能会导致组织分化。</li>
</ol>
<p>把相关的问题梳理成更实操的表述：</p>
<ol>
<li>老领导需要退位，而且要退的彻底；</li>
<li>接班人需要被很好的培养，需要是非常认同组织共情的人；</li>
<li>需要一定的制度确保接班人可以快速承担原有的共情职责（但不需要快速承担效率职责）；</li>
<li>明确的接班人对于接班人过渡很重要。</li>
</ol>
<blockquote>
<p>中国传统文化的礼，其中比较重要的部分就是通过流程来规范化接班人问题，包括接班人的教育，接班人的选择，接班人的过渡，以及接班人和效率之间的取舍问题。</p>
<p>国家的皇帝承担的是国家的组织共情，所以对皇帝的要求，最重要的是合乎礼，而不是治理能力；而国家的治理这件事情，是通过朝堂，通过制度的方式实现的。皇帝不需要是执行方面最有效率的人，甚至不需要用效率来考量皇帝；皇帝需要的是聚拢包括朝堂，包括全国人民的意志，让每个人都有为国贡献的意愿，然后百官各司其职，在合适的制度下，每个人都能为国家的发展贡献自己的力量。</p></blockquote>
<h3 id="案例">案例</h3>
<h4 id="如何看待老员工">如何看待老员工</h4>
<p>如果只有效率的维度，如何处理企业的老员工总是无解的难题——很多行业，老员工无可避免的会走向效率的下降，进而就会成为马云所描述的”老白兔“：做人好，但是却不干活的人。从效率出发，这样的人一定是不值得存在于组织里的，于是互联网行业就会出现”35 岁危机“。</p>
<p>但是，对于一个组织而言，不够效率的人就是没有价值的人吗？上一章的讨论，可以看到，领导者自己甚至都可以是没有”业务效率“的人（但领导者必须是最高共情效率的人），那为什么组织里其他位置就不能容纳不够业务效率的人？</p>
<p>于是进一步的深究，老员工，一定不是指年纪老，或者经验丰富（这其实是指效率更高了），而是指在一个企业中积累了更多共情素材的人。尤其是具有良好组织共情的企业，老员工应当是经历了更多组织共情，更懂组织共情的人，他们反过来也是能供给更高质量的组织共情的人。</p>
<p>所以对于老员工，第一种适合的发展路径就是成为部门的领导者，因为他们更契合大组织的组织共情，可以更有效的将小组织共情同大组织共情相结合。但是不是每个人都有成为领导者的潜质和能力，所以如果不能发展成部门领导者的老员工，他们在非领导岗位依然可以贡献组织共情，但这样的效果会小很多。所以解决方案分两个路径：</p>
<ol>
<li>让老员工的比例提升，从而让更多的老员工可以覆盖更多的新员工，老带新的方式提升共情效率；</li>
<li>让老员工的具体共情信息得到组织维度的传播，老员工成为组织共情的信息提供者和创造者；</li>
</ol>
<p>而因为老员工的价值是在共情维度上，结合前文的内容，给功利以物质奖励，给共情以共情奖励，所以老员工的业务价值依然用业务效率指标来衡量，只要可以保证老员工依然可以生存于组织中（对于业务价值小的老员工，给予其高于其他企业给与的业务价值薪资即可，但并不需要特别高），但给予其更多的机会去表达和输出自己对组织共情的理解和认知，一方面是在组织共情维度上的资历的认可，另一方面是理应获得其他人的尊重和尊敬（这两点都是给与老员工的共情奖励）。</p>
<blockquote>
<p>以部队为例，老兵是部队最宝贵的财富！这句话不单单指老兵的经验，更多的是老兵在信念上的坚定，和老兵对新兵的影响。所以太多的案例中，大家都愿意以老兵为荣：“Old soldiers never die,they just fade away”，老兵不死，只是凋零。</p></blockquote>
<p>所以如何处理老员工，不是一个制度、效率的问题，而是一个组织共情的问题。它代表了组织共情中，人们相信什么。如果更有组织共情的人都被组织所抛弃，那么凭什么让大家相信组织的共情呢？</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>合作的结果可以给每个合作者都带来增量价值。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>这样的合作方式不是社会发展中最先出现的组织形式，而是随着最早的组织出现后，进一步进化形成的组织形式。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>存在可替代的合作方，这里才形成一个适合的自由市场。当然，这个市场依然有可能是垄断的，不效率的。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>村上春树《遇上百分百女孩》&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>解决功利动机的组织&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>庖丁解牛|究竟什么是“战略”？</title>
      <link>https://blog.uglyboy.cn/posts/2021/07/15/</link>
      <pubDate>Thu, 15 Jul 2021 08:00:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2021/07/15/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;前几日，在朋友圈看到前同事分享某大厂高管在 XX 创业营上开了一课《如何制定战略》，内容里有“使命、愿景、价值观”、还有“一颗心，一张图，一场仗”等行业黑话，而这两个话题凑一起，竟成了“从战略到执行的方法论”了。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p>前几日，在朋友圈看到前同事分享某大厂高管在 XX 创业营上开了一课《如何制定战略》，内容里有“使命、愿景、价值观”、还有“一颗心，一张图，一场仗”等行业黑话，而这两个话题凑一起，竟成了“从战略到执行的方法论”了。</p>
<p>什么是战略？战略到底是一门玄学还是一门科学？回想在老东家经历过的各个时期的“战略”，心有戚戚焉。</p>
<p>所以今天咱们就来硬核拆解一下：究竟什么是战略。</p></blockquote>
<h2 id="大而无当的战略">大而无当的“战略”</h2>
<p>先讲三个关于战略的小故事：</p>
<blockquote>
<p>当初老东家收编某视频巨头时，后者的市场份额跌至第三，于是许多本部的精兵强将前去支援。<!-- raw HTML omitted --></p>
<p>当时便面临着一个战略问题：在不差钱的情况下，如何重新夺回市场份额第一的宝座？</p>
<p>就我参与阶段的了解，给出的答案是非常有该厂特色的解决方案：</p>
<ol>
<li>技术上提升用户体验，将 crash 率、秒播率、带宽压缩率等等各项指标都做到行业领先；</li>
<li>通过个性化算法优化匹配效率；</li>
</ol>
<p>这个战略有一个宏大（但我记不住）的项目名称。一年之后，各项指标都在大家 996 的奋斗下达成了，项目也召开了一个隆重地表彰大会，获奖的团队笑脸盈盈。</p>
<p>故事的结果大家都知道。</p>
<p>它的市场份额，相比于前两家竞对，又跌了不少。</p></blockquote>
<blockquote>
<p>之后不久，恰赶上了“猫狗大战”最激烈的那两年，于是老东家内部酝酿出一个核心战略，具体名称不重要，我姑且简单粗暴地称之为“打狗棒”。所有部门都在这个大战略下指定各自的战略，于是可以看到各个行业关于“打狗棒”的战略方案。</p>
<p>但令人意外的是，这个战略似乎只在 PPT 里彰显存在感。我们难以找到它在业务执行层面的体现。也就是说，有它没它，业务都在沿着既定的方向走。</p></blockquote>
<blockquote>
<p>去年，跟快递公司有一些业务合作。某通的几个同学经常问我：菜鸟又出了什么什么战略，这个背后有什么思考？</p>
<p>这个问题让我想起了刚到菜鸟时，跟菜鸟的同学聊天，大家都在感慨：每年都在谈战略，甚至一年一个战略，但真心没啥战略。</p></blockquote>
<p>这三个小故事可以让我们对大厂“祛魅”：哪怕是大公司，哪怕是高管，也不一定能制定出正确的战略，甚至，以个人经验来看，他们中真的懂战略的，不多。</p>
<p>正因如此，“打工人”才常常有一种误解：一说“战略”，就是大而无当、虚头巴脑、不知所云。与其关心高层的“战略”，不如关心关心业务的 KPI，毕竟这才真正关乎自己的钱包。</p>
<blockquote>
<p>这里的三个故事，其实并不是想抨击大公司病或者揭露所谓的高管内幕。更明确地说，没有想表达对高管不敬的意味（认真脸）。<!-- raw HTML omitted --></p>
<p>高管的定义是高级管理者，是能够协同一个部门可能上百号人共同实现某个业务目标的领航人。但是，在战略这件事情上，执行能力强并不一定总能导向正确的方向，甚至会更容易形成偏见——我们擅长的部分就是最值得做的事情。</p>
<p>这可能也是老人们时常会谈，“将才”和“帅才”的区别吧。</p></blockquote>
<h2 id="战略是一门科学">战略是一门科学</h2>
<p>公司的战略，几乎都是领导们开会商量出来的。</p>
<p>是不是战略就应该由领导来决定呢？从权力的角度出发，是的。但从逻辑的角度来看，这样的战略决策会导致一个主观性极强的决定。就好像你爱吃甜，我爱吃辣一样，无所谓对错。</p>
<p>可是回到战略执行层面来看，这种做法会出现这样或那样的情况：执行后发现和想象的不一样只能重新规划的；换了领导于是调整战略的；也有执行了一阵子终于松口承认战略决策失误的……</p>
<p>如果战略就是这样一个既不可靠又不能延续的东西，那何必还要考虑什么“战略”呢？</p>
<blockquote>
<p>战略不是收入证明，不是只有高管或者大公司才配谈战略，也不存在说大公司的战略就一定是对的、好的，或者高管看问题就一定高瞻远瞩，高屋建瓴；战略也不是玄学，不应该有那么多的拍脑袋，“我觉得”，或者所谓的经验之谈。</p></blockquote>
<p>毛主席说：“战略问题是研究（战争）全局的规律性的东西。”战略应当是一门科学，它背后是有规律的，是可被描述和验证的。</p>
<p>在《营销的黑暗森林法则》一文中我做了一个铺垫：一定时期内，企业的竞争背后是一个零和游戏，企业发展是以抢占其他企业市场份额实现。</p>
<p>从而可以相对容易地理解战略的定义：</p>
<p><strong>战略是指在相同投入的条件下，能够获得更大的市场份额的业务方向。</strong></p>
<p>这个定义和我们直觉上对战略的理解是基本一致的，但是，其中有几个很核心的要点，却往往会被人们所忽视。</p>
<h3 id="你搞清楚自己的定位没有">“你搞清楚自己的定位没有？”</h3>
<blockquote>
<p>（段子）“灵魂六问”：</p>
<p>配钥匙师傅：你配吗？</p>
<p>算命先生：你算什么东西？</p>
<p>食堂阿姨：你要饭吗？</p>
<p>快递小哥：你是什么东西？</p>
<p>上海垃圾分拣阿姨：你是什么垃圾？</p>
<p>最有文化的还是网约车司机：你搞清楚自己的定位没有？</p></blockquote>
<p>关于战略，最核心的一个要点就是要看清自己所处的市场。不同的市场定位，战略打法会截然不同。</p>
<p>例如北京市和平里地区的餐饮市场，同全北京的餐饮市场，不同的内涵与外延形成的战略打法自然不同，最终达成的效果也会很不一样。</p>
<p>尤其是，不能自欺欺人地杜撰一个自以为是的市场。</p>
<blockquote>
<p>前段时间拜访了一家“无代码”的企业，企业的业务同学侃侃而谈“无代码”的美好前景，以及，“在‘无代码’的市场中，我们没有对手”。</p>
<p>可是，“无代码”这个概念自己就是一个市场吗？难不成发明一个不放鸡蛋的煎饼果子，就可以独霸“无鸡蛋煎饼果子市场”吗？哦，似乎说来也对，只是这样的“市场”很难说有什么价值。</p></blockquote>
<p>正经地说，市场需要一个科学的定义以便于我们理解：<strong>具有需求可替代性的产品的集合，才构成同一个市场。</strong></p>
<p>市场的范畴可小可大，例如火锅市场必然也可以延展到饮食市场。这个范畴的选择，需要根据企业自身的发展情况来抉择——不断成长的企业必然是不断扩大自己所在市场范畴的企业，让自己的产品可以在更大的范畴里解占据更多的市场份额，解决更多的用户需求。</p>
<h3 id="同样的资源为什么要给到你">“同样的资源，为什么要给到你？”</h3>
<p>战略的价值衡量不是简单地拿结果说话，而要看投入产出比。</p>
<p>曾经参加的很多项目决策会，项目方会卖力地说：“我的项目具有什么什么价值，只要资源到位就好”。但上位决策者考虑问题的方式是：相同的资源，给到这个项目还是其他项目，哪一个能够获得更大的价值。</p>
<p>有价值的事情很多，但不代表有价值就值得（当下）做。或者说，战略决策，某种意义上讲就是对各种选择进行优先级排序。</p>
<p>资源要优化配置的道理已是老生常谈，但放到自己身上，可能就成了“灯下黑”。</p>
<p>所以脱离投入产出比谈业务价值，就是耍流氓。</p>
<h3 id="大盘涨了你的市场份额涨了吗">“大盘涨了，你的市场份额涨了吗？”</h3>
<p>战略中值得在意的不是绝对价值，而是市场份额。</p>
<p>在一个不断发展的市场中，赚钱了，证明不了你的本事，也许你就是风口上的猪呢。至少你的增长速度要超过大盘的增长，才是真的增长。换句话说，就是需要有市场份额的提升。</p>
<p>所以谈战略的时候，份额才是真实的价值。</p>
<p>淘宝内部很多项目都要赶着双十一上线，接着双十一的流量大爆发，就可以容易地拿到一个很好看的数字。但是这样的结果能证明项目的价值吗？谈战略时，自欺欺人真心没什么意思。</p>
<p>尤为重要的一点是，企业自身当下的市场份额对如何选择战略方向起着决定性作用。</p>
<p>小企业自然需要立足于发展，而大企业则需要考虑扩张自己的势力范围。企业在不同阶段一定需要截然不同的战略选择。这也是很多成功的创业者，企业做大，就难以更进一步；而很多大厂的高管进入了初创企业也种种水土不服。</p>
<p>遗憾的是，很多优秀的企业都会将自己曾经的成功经验教条般地搬到新的阶段，造成战略方向的失误。这可能就是大部分企业最难的地方——让体验过成功的团队否定现状，这一定是很不愉快的感受。所以对未能跨过这一步的“成功过”的企业，只能对他们的曾经表示“respect”，然后摇一摇头，用前同事的话来美化一下这种失败——<strong>“优秀是卓越最大的敌人”</strong>。</p>
<h2 id="结语">结语</h2>
<p>战略是一门科学，而不是什么形而上的东西。所以战略不是高管或大厂的专属，而是值得每个人去思考和掌握的方法。</p>
<p>作为一门学科，给出“战略”的定义，正是开启科学发展方向的基础——有了定义，我们才知道战略不能空谈，<strong>必须要在看清自身所处的市场、自身的市场份额、自身的投入情况的条件下，去考虑各个业务方向的投入产出比</strong>。</p>
<p>这句话，也是今天想分享给大家的道理。</p>
<p>至于怎样制定战略？我还有一个关于“<strong>竞争策略</strong>”的研究，相对深入地分析了这个问题，以后有机会可以继续庖丁解牛。</p>
]]></content:encoded>
    </item>
    <item>
      <title>黑暗森林法则和营销</title>
      <link>https://blog.uglyboy.cn/posts/2021/07/01/</link>
      <pubDate>Thu, 01 Jul 2021 08:00:00 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2021/07/01/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，能做的只有一件事：开枪消灭之！&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p>宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，能做的只有一件事：开枪消灭之！</p>
<p>在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭。这就是宇宙文明的图景，这就是对费米悖论的解释。<!-- raw HTML omitted --></p>
<p>——刘慈欣 《黑暗森林》</p></blockquote>
<h2 id="营销社会学">营销社会学</h2>
<p>《三体》是大部分技术人都喜欢的作品，至少也是技术人彰显自己文艺的重要共情素材——它用一种很理科的方式推演了一个理论：<strong>宇宙社会学</strong>，并以此延伸出了一个荡气回肠的故事。对于技术人（理科生）而言，这种快感大概也只有《生活大爆炸（The Big Bang Theory）》可以相提并论了。</p>
<blockquote>
<p>宇宙社会学基本公理：</p>
<ol>
<li>生存是文明的第一需求；</li>
<li>文明不断增长和扩张，但宇宙中物质总量基本保持不变；</li>
</ol></blockquote>
<p>从这两条基本的假设出发，加上 &ldquo;<strong>猜疑链</strong>&rdquo; 和 &ldquo;<strong>技术爆炸</strong>&rdquo; 的概念，就可以得到黑暗森林法则（即本文引言），进而构建出罗辑这样的执剑人角色，让明明处于绝对劣势的地球得以和三体星人平等共生 62 年。</p>
<p>有意思的是，出现在科幻小说中的宇宙社会学基本公理，正是从现实生活中归纳和升华出来的假设，放到熟悉的领域中，也依然有效：</p>
<blockquote>
<p>企业社会学基本公理：</p>
<ol>
<li>发展是企业的第一需求；</li>
<li>企业不断尝试增长和扩张，但全社会的总需求在一定时期内是基本保持不变的，或者说全社会可支配收入的总额是基本保持不变的；</li>
</ol></blockquote>
<p>直接的结论是：企业的发展是以抢占其他企业的市场份额实现的<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>而营销，是帮助企业抢占其他企业份额的具体手段的总称，所以当行业发展到一定阶段，营销就会成为企业间最激烈竞争的战场。</p>
<p>营销是如此的重要，以至于企业生产了一个产品后，愿意将其中的大部分利润让利给营销者，帮助自己抢占更大的市场份额。从而出现了大量的形形色色的营销型企业，包括：品牌商、代理商、渠道商、电商平台、直播带货、广告行业、门店销售等等。</p>
<p>而这些营销型的企业也依然适用上面的企业社会学基本公理，这里将之单独提炼成<strong>营销社会学基本公理</strong>：</p>
<blockquote>
<p>营销社会学基本公理:</p>
<ol>
<li>赚取更多的营销费用是营销型企业的第一需求；</li>
<li>在一定时期内，社会的总营销费用是保持不变的；</li>
</ol></blockquote>
<p>有了营销社会学的基本公理，很容易就能理解这样的推论：<strong>所有营销型企业之间都是互相竞争和取代的关系；营销型企业之间比较的是（一定时期内）帮助生产型企业抢占更大市场份额的能力</strong>。</p>
<h2 id="最好的时代--最坏的时代">最好的时代  最坏的时代</h2>
<p>通常技术的发展都是渐进式的，甚至哪怕是《三体》中在一个比较长的时间周期里，可能出现的“<strong>技术爆炸</strong>”，描述的也是技术线性推进过程中，在时间轴上的爆发。</p>
<p>或者说，技术的发展大体上都是有脉络的，是旧的技术不断积累和沉淀，为新的技术打下的良好的地基。</p>
<blockquote>
<p>关于技术如何演进这件事，推荐一本书——来自圣塔菲研究所的布莱恩·阿瑟撰写的《技术的本质》</p></blockquote>
<p>技术的渐进式发展，直接影响生产型企业也呈现渐进式发展——生产效率总是以一种相对稳定的方式进行提升，所以大多数情况下，同一时期内一个行业里的生产效率总是相近的，生产型企业之间的能力差异更多的是在资本层面（规模型经济）、管理层面，或者营销层面。</p>
<p>不过，这个规律放到营销这件事情上则大为不同了。</p>
<p>是的，营销的发展并不是营销手段的渐进提升，而是近乎完全不同的营销手段之间的更替的过程，这里把这种趋势称为“<strong>突变演进</strong>”。</p>
<p>例如，营销的重心从最早的报纸广告，快速演进到电视广告、综艺植入，再到互联网广告、新媒体营销；在渠道的营销层面，则是从层层代理商，到大型连锁超市，再到依赖电商平台，以及最近两年正大火的直播电商。</p>
<p>虽然营销领域在“突变演进”下也有一定的积累沉淀，而且在“突变”的间隔里，营销型企业也是以经验的积累努力提升营销效率，但纵观全局，起主导作用的营销手段依然都是“突变演进”的方式进化的——新的营销方式总能对旧有的营销方式形成绝对的优势。</p>
<p>幸或不幸，媒体行业、广告行业以及大部分的互联网企业，都属于营销型企业（赚的是生产型企业的营销费用）。所以这些行业里“眼见他起高楼，眼见他宴宾客，眼见他楼塌了”，都是时代的眼泪。纸媒转型成了新媒体，传统广告让位于互联网广告；互联网领域，谷歌百度一时风头正盛，到了移动互联网时代又让位给了 Facebook、微信；亚马逊和淘宝看似屹立不倒，抢的是线下传统渠道的营销费用；而短视频的崛起，让曾经的任何巨头都不敢轻视……这样的故事仍在这些领域中续写着。</p>
<p>这是最好的时代，也是最坏的时代。</p>
<p>营销的“突变演进”，带来了传统行业突变的可能性。一个企业（生产型）被另一个企业打败，可能不是因为它的技术能力不足，而是没能跟进到最新最“先进”的营销手段，导致不小心在历史舞台上落幕。</p>
<blockquote>
<p>我自己非常有感触的一件事情：我心中洽洽瓜子一直是第一名的休闲零嘴，也是大学期间常年的必备零食。但最近几年，要么是买了各路的网红零食，什么三只松鼠、良品铺子，要么是逛了周边的网红蛋糕店，买脏脏包、奶茶，甚至在网易严选之类的平台上屯糕点零食啥的，竟然都没机会想起来买一袋洽洽瓜子。</p></blockquote>
<p>打败恰恰瓜子的未必是更好吃的瓜子，或者更好吃的零食，只不过在流淌的时间里，不经意间被人遗忘，再回想起来时，已经成为了情怀。就好像儿时的北冰洋、熊猫雪糕、大大泡泡糖。</p>
<p>在这样的时代里，我们可以亲眼见证：两岁的完美日记打败了 112 年的巴黎欧莱雅和 73 年的雅诗兰黛；五岁的元气森林超过了不可一世的可口可乐和百事可乐；比旺旺年轻十几岁的三只松鼠线上市场占有率为 11.2%，而旺旺则不足 1%……</p>
<h2 id="营销的黑暗森林法则">营销的黑暗森林法则</h2>
<p>营销型企业唯一的价值在于其营销水平能帮助客户（生产型企业）比其他的营销者抢占到更多的市场份额，不论手段。</p>
<p>曾经的各地晚报上的广告段子再火，也挡不住纸媒的衰落；一个广告代理商哪怕把百度凤巢、淘宝直通车等优化到极点，但直播带货兴起之时，企业还是会把营销费用都网红，转向直播带货，减少广告平台的投入。</p>
<p>那直播带货会是终局吗？我们可以笑一笑，五年之后再来看这个话题。</p>
<p>营销世界的黑暗森林法则就是：<strong>没有“正确”的营销，只有更好的营销</strong>。</p>
<p>曾几何时，当大家拿着营销学的红宝书——科特勒的《营销管理》，学习宝洁的品牌战略，搭建和完善线下的供应商体系时，悄然崛起的并不是另一个宝洁，而是跟着电商平台一起成长起来的阿芙精油、韩都衣舍、三只松鼠、御泥坊、芳草集。</p>
<p>它们没有做曾经认为最正确的营销：打传统媒体广告、树立传统的品牌战略、构建完整的供应商体系，但相比传统营销，线上营销玩法是更好的营销，所以它们最终获得了更多的市场份额。</p>
<p>近些年，线上营销成为了主流，哪怕是传统的品牌商也都纷纷转型线上，都知道要新媒体营销， 三微一抖<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，烧车钻展等等，但反倒是热闹的搞起线下营销的喜茶、瑞幸、泡泡玛特们脱颖而出。</p>
<p>甚至还有像拼多多这种，硬生生从不能做电商的社交流量上，搞出了一套“社交电商”的玩法，成为国内电商的第三极；vivo 和 oppo 则重金砸各类网络热门综艺节目，以异军突起之姿，顺利替代了“中华酷联”，已经将苹果挤出中国手机市场的前三；而已经有些由盛转衰的海底捞，则是靠着极致的服务成为餐饮行业的龙头。</p>
<p>这里似乎总在发生着“不讲武德”“乱拳打死老师傅”的故事——品牌们不停地学习新的营销玩法，但还是无可避免地被更新的一拨人挥舞着你没见过也看不懂的武器伤害着。</p>
<p>营销世界（营销型企业间的竞争）里，大厂们也都在焦虑着：因为短视频，腾讯在焦虑；因为拼多多，淘宝在焦虑；百度倒是可以佛了，正在焦虑的已经换成了今日头条和抖音。年轻的美团和拼多多也在焦虑，他们都急火火地进军了社区团购业务——他们是看明白了社区团购将是下一代的营销？未必。但是万一是呢？</p>
<p>年长者不得不杞人忧天，年轻者则做着异想天开一夜成名的梦。所以在营销的世界里，和《三体》中一片寂静的黑暗森林不同，这里的每个参与者都是一个发光体，并试图闪耀出更耀眼的光芒，沟通一片异常耀眼的黑暗森林，而且还将继续闪耀下去。</p>
<p>最后，也送给每个想在这片森林里闪耀的读者朋友一个问题：<strong>你正在为之奋斗的，是“正确”的营销，还是“更好”的营销</strong>？</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>严谨地说，这种情况发生在<strong>产能不是瓶颈，供给大于需求</strong>时，这是当下大部分行业的现状。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>三微一抖是指：微信、微博、微淘，以及抖音&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    <item>
      <title>社交与传播</title>
      <link>https://blog.uglyboy.cn/posts/2018/08/16/</link>
      <pubDate>Thu, 16 Aug 2018 21:23:33 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2018/08/16/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本文是笔者近年的一项研究，主要在探究人类社会在经济行为以外的时间里，进行行为的动机，也就是文中提到的 &lt;strong&gt;共情&lt;/strong&gt;。通过这项研究，基本解释了生活中大部分经济行为之外的现象背后的原理，也提供了一种新的创造价值的有效路径。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote>
<p>本文是笔者近年的一项研究，主要在探究人类社会在经济行为以外的时间里，进行行为的动机，也就是文中提到的 <strong>共情</strong>。通过这项研究，基本解释了生活中大部分经济行为之外的现象背后的原理，也提供了一种新的创造价值的有效路径。</p>
<p>当然，本文的结论也可以用很多其他理论来解释，但从第一性原理出发，从一个基点扩展到整个社交和传播领域，会更好的让我们理解现象背后的道理。</p></blockquote>
<h2 id="社交的本质">社交的本质</h2>
<h3 id="共情社交功利社交">共情社交&amp;功利社交</h3>
<p>一般来说，社交在本质上可以分为两种，一种是“<strong>共情社交</strong>”，一种是“<strong>功利社交</strong>”。</p>
<p>这里给出一个功利社交的粗浅定义：以结果为导向的社交行为。共情社交一般偏向于“随便逛逛”，无聊解闷，八卦等非结果导向的行为。当然两者的界限并不是特别的严格，很多共情社交也会慢慢走向功利社交，例如一些人在社区里的行为，最早是为了交流，慢慢变成为了谋求社区中更高的地位和话语权，这就是从共情社交走向了功利社交。</p>
<blockquote>
<p>在一个大的范围内来看，商品交易本质上就是一种社交——参与交易的人需要进入到社会中进行信息交互，最终寻求到自己需要的商品信息，并与其他人进行沟通，达成最终的共识，形成商品的交换。</p>
<p>本质上来说，商品交易应该算作是“功利社交”。</p></blockquote>
<p><strong>功利社交</strong>是以客观需求为导向的，基本上都是一般的经济学规律易于解释的，从而有时不包含在狭义的“社交”范畴之内。但哪怕狭义的“社交”中，也无法完全剔除“功利社交”的影响，例如通常所谓的网络上流行的“色情”、“暴力”所形成的社交，本质上也是“功利社交”。</p>
<blockquote>
<p>老外们的社交有不少都是单纯的功利社交，所以时常会出现电影中的桥段：“Just a business”（只是个生意）。但是中国的功利社交则往往伴生着共情社交，甚至共情社交是功利社交的基础，于是诞生了饭桌文化，“买卖不成仁义在” 等现象。</p></blockquote>
<p>共情社交和功利社交并不能完全的切分。但有了这样的概念，会让我们更容易理解一些社交现象：</p>
<blockquote>
<p>以微博为例，内容发布者（大 V）以功利社交为主，粉丝以共情社交为主；而豆瓣上，发布者以共情社交为主，浏览者的行为一部分是功利社交（看评分，找好看的内容），一部分是共情社交（看完剧之后，看看别人的评论，寻求认同）；</p>
<p>抖音和快手本质上是有差别的：抖音的发布者往往是以功利社交为主，浏览者是共情社交；而快手的发布者往往是一共情社交为主（非头部的内容生产者），浏览者也是共情社交。这个结论也同两者的定位相关。</p>
<p>浏览维基百科的人是功利社交，但为维基百科撰写词条的人，则有不少都是共情社交。</p>
<p>淘宝上的社交产品，甚至乃至阿里系的社交产品，都是针对功利社交的产品。钉钉之于微信，就是典型的例子。</p></blockquote>
<p>相比之下，<strong>共情社交</strong>则处于鄙视链的上游，以马斯洛的需求理论来说，“功利社交”更多的是解决生理（第一层次）和安全（第二层次）方面的需求，而“共情社交”则是解决情感和认同（第三层次）、尊重（第四层次）方面的需求。</p>
<img alt="需求理论" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/b005b863a4f284e4a6d963805d12c6c4.png"><p>本文也主要致力于研究共情社交方面的核心要素。</p>
<h3 id="共情社交的目的">共情社交的目的</h3>
<p>以马斯洛的需求理论来看，共情社交的目的就是获得情感认同和尊重。为了更好的从一个客观角度来描述这件事，我们认为一切情感或认同等等的行为都是信息传递和交互的过程，于是上面描述的共情社交所带来的过程可以描述为：“获得内部信息和外部信息的一致性”或者说“与外部信息形成共识”。</p>
<blockquote>
<p>我们参与社交活动，除了完成特定的需求（目的）外，都是期望与外界信息达成一致的，例如我们心中有一些观点，希望看到和我们观点一致的分析或肯定，又或者是我们期望发出我们的声音，让别人表达对我们的观点的认同。</p>
<p>从进化的角度来说，形成越多的认同（群体意见达成一致），就可以形成合力，形成更大的能量。所以进化过程中是期望生物体之间可以达成共识的。如《人类简史》中描述的，人类了不起的进步就是源于共同的想象——一个巨大的共识的诞生。</p></blockquote>
<p>共情社交的过程是个体“与外部信息形成共识”的过程，但是形成共识并不是共情社交的目的。</p>
<blockquote>
<p>例如我们买了一件很喜欢的时尚服饰，如果我们看到有明星也穿着这件服饰，这事我们会获得共情社交中的认同感；但是如果我们是看到一个不是很潮的人或者是跳广场舞的大妈也穿着这件服饰，我们就会觉得失去了那种认同感，怀疑自己是不是出现了什么差错。</p></blockquote>
<p>所以，人们参与到社交活动的目的（动机），本质上并不简简单单是获得更多的认同，因为上面这个例子中，我们得到了更多的认同，只不过我们觉得这种认同带来的是认同感的丧失——我们期待被我们期待的人所认同。用一种更加理性的方式来解释：<strong>人们参与到共情社交中的动因是提升获得认同的可能性，但不是认同本身</strong>。</p>
<p>上面的例子，我们认为我们同明星是有更大可能形成认同的，所以当发生了“我们喜欢同一件衣服”的事件后，我们发生认同的可能性变得更大；而我们认为我们同广场舞大妈本质上是小概率能形成认同的（我们不认同广场舞大妈的品味），所以当我们与她们发生了“我们喜欢同一件衣服”的事件后，我们与我们可能形成认同的人群之间获得认同的可能性变小了。这里需要额外做一点解释：并不是我们与广场舞大妈发生一切共识我们都会产生不好的感觉，背后最核心的是<strong>我们（主体）内在的认知同外部达成共识的可能性是否变大</strong>。</p>
<h3 id="主体的认知和外部共识">主体的认知和外部共识</h3>
<p>基于上文的描述，获得更多达成共识的可能性是人参与社交的主要目的，那么如果外部已经存在一个很多人认可的共识，我们接受这个共识所能获得的达成共识的可能性自然就会非常的高。是不是我们不断的接受已经达成的最大的共识（区块链技术），就能获得更好的社交体验呢？很不幸，答案确实是这个样子的，很多人都倾向于接受外部已经形成的共识。但是任何一个人（个体），都是存在其个体对事物的天然认知的。所以最终决定一个人的认知的，是主体的认知和外部共识共同作用的结果。</p>
<p>一件事情，如果主体的自我认知形成的结论和外部共识一致性非常的高，那么主体就更容易倾向于接受这种共识；反过来如果自我认知形成的结论和外部共识的差异性非常大，那么主体就会排斥接受这种共识。</p>
<p>例如上文中广场舞大妈的例子，主体自身形成的关于“品味”的认知和大妈们通常的共识差异是非常大的，所以主体就会排斥接受大妈们的共识；而相关认知同明星们通常的共识差异是小的，主体就会倾向于接受明星们的共识。</p>
<p>不同主体认知的人们会自发的形成差异性的群体，倾向于接受彼此之间的共识（因为未来彼此之间继续形成共识的可能性大），排斥接受其他群体的共识（因为未来彼此之间继续形成共识的可能性小）。</p>
<h3 id="达成共识的相关要素">达成共识的相关要素</h3>
<p>达成共识是一个信息传播的过程，关于传播后文我们再进一步研究，这里先从信息传播这个层面上来拆解达成共识的相关要素和重要程度。信息传播的要素是：介质（编码）、信息（内容），所以这些也是达成共识的相关要素，并且达成共识还需要一点是：信息的一致性（主体认知同外部信息的一致性）。这就得到了达成共识的三个核心要素：信息的编码（表达形式）、信息本身、信息的观点。三个要素的重要性依次提升。</p>
<h4 id="信息的编码">信息的编码</h4>
<p>信息的编码一致的时候，才有可能达成共识。所以信息的编码一致是一个非常基础的，能带来“达成共识可能性”的要素。具体举例就是，如果两个人会同一种语言，他们就会更愿意互相交流（出国后，在语言不通的环境里遇到一个可以交流的人）。甚至一些其他的信息交流方式，例如音乐、舞蹈，甚至一些抽象的信息编码模式，例如一起踢足球、打篮球等等。</p>
<p>如果一个人掌握的一种编码方式已经很容易与外界形成共识，那么信息编码就不再是重要因素，但反过来时，则会是非常重要的要素。例如一个人在一个外界都在用同一种信息编码方式的环境下，是有极大的倾向掌握这种编码方式。</p>
<p>抖音、初期的微信等都是这种信息编码的方式。大家使用这种编码方式的目的，都是为了获得更多的达成共识的可能性。从而编码方式本身就具有了极强的传播性（内增长）。</p>
<h4 id="信息的内容">信息的内容</h4>
<p>如果一个人掌握的信息的内容同外部信息的内容有更大的重叠性，那么他与外部获得共识的可能性就会更大。例如一个人在外遇到同乡，可以聊起家乡彼此共同的回忆，就会更愿意相互交流，以期待形成更多的认同。所以共同的信息内容是比编码更上一层的达成共识的相关要素。</p>
<p>一般来说，亲友、同乡、同学、熟人等等就是具备有更多的共同信息的人，所以彼此之间就更容易形成社交关系。而基于爱好或话题形成的社群，也是聚集了一群掌握了共同信息的人群，彼此之间更容易形成共识。这就是熟人社交和社群能够发挥作用的基础。</p>
<p>通常来说，大家都愿意看热门的电视剧，因为这样的话，跟其他人沟通的时候就会比较容易有话题可聊，这就是人们为了获得更大的形成共识的可能性，而主动获得更多的外部大家都掌握的信息。类似的，潮流、热点等等，都是基于人们试图获得共同的信息内容所形成的。</p>
<h4 id="信息观点的一致">信息观点的一致</h4>
<p>共识中最重要的一点，就是观点的一致性。例如某个名人，或许所有人都知道他（编码、内容一致），但不同的人会对他怀有不同的观点，如果观点不一致，那么就无法形成共识。所以就像上一节的例子，广场舞大妈和明星有相同的编码和内容，但是因为他们的观点不同，就会形成不同的共识，导致最终的共识无法形成。</p>
<p>而观点是达成共识的相关要素中，最主要的与主体认知有关的，也是最难改变的。</p>
<p>每个人的成长经历不同，所形成的观点就会千差万别，观点上达成一致的可能性就会变小，而且哪怕能达成一致，其个体的差异也是非常大的。例如同样一个明星，有些人喜欢，有些人讨厌，有些人无感；哪怕是喜欢他的人当中，其原因也是各异的，有些人仅仅是觉得长相比较好、有些人觉得他的成长中的努力感动了自己、有些人觉得他声音甜美等等。</p>
<p>主体观点强的人倾向于表达自己的观点，获得别人的认同；主体观点弱的人倾向于寻找自己认同的观点，从而达成共识，甚至有时会抛弃自己的观点，以外部共识的观点来替代自己的主体观点。</p>
<p>幸好本文试图研究和解释的并不是为何大家会在某一个观点上形成了一致，而是怎样的方式能让这种的共识未来可以继续形成更多的共识的可能性。</p>
<blockquote>
<p>例如，大家喜欢某个明星的原因千差万别，关注的要点可能也有各种不同。但是我们考虑粉丝在微博上 follow 某个明星这个社交动作时，是否 follow 这个明星其实都不影响（通常来说）粉丝本身对明星的喜爱程度，但是粉丝却都愿意做出这个动作，背后的原因是：无论基于怎样的动机，follow 之后都可以获得更多明星的消息，从而获得更多达成共识的可能性。</p></blockquote>
<h2 id="传统观点中的社交要素">传统观点中的社交要素</h2>
<p>传统社交主要涉及到的要素是：关系、内容、互动；一般涉及到的诉求分为：打发时间、炫耀、获得刺激。本文将试图用 “社交是为了增加达成共识的可能性” 这个观点出发，对上述一些传统社交的要素做一定的解读：</p>
<h3 id="关系内容互动">关系、内容、互动</h3>
<h4 id="关系">关系</h4>
<p>传统社交中的关系要素，主要是提升了“增加达成共识可能性”的三个要素中的编码和内容。</p>
<p>编码方面，因为是常见社交关系，自然是可以沟通的，而且也有一致的沟通编码方式，例如俚语、暗号等等。</p>
<p>而内容方面，则是因为共同生活的关系，势必会积累很多共同的内容从而形成内容方面的优势。</p>
<blockquote>
<p>有时我们有找老同学（旧社交关系）聊天的渴望，也是因为老同学具备了一定程度的编码方面的一致性及共同的内容。而有时真的跟老同学聊起来了，有时又和想象的不一样，或许是编码方面的不统一（不同地方形成了不同的语言习惯之类的），也或许是内容的观点方面的不统一（内容上一般找老同学，就是要聚焦相同内容的）。而乐趣之一就是补齐内容方面的不统一（八卦以前同学现在的状况），未来获得更大的达成共识的可能性。</p></blockquote>
<p><strong>关系</strong>是一种确定性的“达成共识可能性高”的通道，或者说，任何全新的社交模式形成后，最终一定会伴随着新的“关系”的出现，以便于未来继续达成新的共识。</p>
<h4 id="内容">内容</h4>
<p>首先需要解释一下什么是内容。这一节的内容并不是前文中所述的内容——信息的主体；而是包装信息的形式——信息的载体。就是说这一节提到的内容，即包含前文所说的编码方式，也包含前文所说的信息的内容，甚至还包含信息的观点。</p>
<p>内容是如何促进社交的呢？首先，并不是所有内容都有助于社交；只有具有特定特点的内容才有助于社交：</p>
<ul>
<li>内容形式容易被更多的人接受，甚至是当前流行的形式（信息的编码方式）；</li>
<li>内容本身同更多人的已有知识有尽可能多的重叠，或者是热门话题，大家都渴望了解（信息的内容）；</li>
<li>内容的观点和尽可能多的人的内在主观认识是一致的（信息的观点）</li>
</ul>
<p>内容对于社交的主要作用是跨期的可积累性——关系这种通道效率虽然高，但是很多时候并不能保证通畅（很难说想找人聊天时就一定有人会来），所以通过信息的载体，让不能实现实时的信息交互时，依然可以提供“达成共识”的通道，而且因为通道是被选择过的，依然有较高的达成共识的可能性。</p>
<p>好的文章、杂志、社区中的帖子等等，都是提供了更高可能性的“达成共识”的通道——内容。而杂志、社区本身就是对可能性的一种背书，从而让容易达成共识的人在这里可以保证“可能性”一直比较高。</p>
<p>内容的观点非常的重要，也是人们选择阅读内容的重要考量因素之一。观点是否能和尽量多的人的内在主观认识一致是内容是否容易被接受的重要因素（内容与内容比）。</p>
<blockquote>
<p>小红书上的内容都有套路。例如内容推荐的大多是非常贵的包包或者珠宝，但这种推荐未必能跟大多数人的主观认知形成共识（不同人的喜好不同等等），所以内容最终表达出的观点并不是“这么贵的包包非常好”，而是“这么贵的包包，我的男朋友还是给我买了”。</p>
<p>用大家容易产生共识的：男朋友愿意为我付出，替代包包是否好，从而内容的接受度就会提升很多，因为这样的内容更容易与读者形成共识。反过来读者在小红书（背书）上阅读内容就更容易达成共识，这里的内容就是一个高“达成共识的可能性”的通路，更好的解决用户的社交诉求。</p></blockquote>
<h4 id="互动">互动</h4>
<p>类似与上文的内容，并不是互动都会带来更好的社交体验——“提升达成共识的可能性”。如果我们来到了一个和我们共识较少的场所，哪怕有更多的互动，也都像《消愁》中描述的那样：“各色的脸上各色的妆，没人记得你的模样”，也就是“越长大越孤单”。</p>
<p>功利社交中互动非常的多，但很多时候并不会让人（所有人）快乐，它的互动更像是一种交换——让一部分人快乐，而另一部分人从这个过程中获得机会，未来获得某些利益。</p>
<p>回到共情社交这个话题，看一看互动是如何促进共情社交的。互动最重要的要点是给予反馈，因为强制性的反馈，使得 达成共识\无法达成共识 的过程加快。如果原本就存在基于关系或者内容的高“达成共识可能性”的通路，那么互动就可以提升参与者感受到的“达成共识的可能性”（单位时间内达成共识的量变多了）。所以，在已经形成良好氛围的共情社交环境下，互动的提升可以提升社交的效果。</p>
<p>进一步的，参与者如果找到了一个适合他的社交环境，自我认知与外部很容易达成共识，那么它主动表达自我就可以增加达成共识的可能性。良好的社交环境下，表达自我是参与者天然的诉求，互动本质上也是社交自然的产物。</p>
<p>另外，互动的形式上是可以控制的，如果能约束互动的结果只可能“达成共识”，不能表达“无法达成共识”，那么不需要外在的好的社交环境这个约束，互动本身就会提升“达成共识的可能性”</p>
<blockquote>
<p>快手首先利用游戏化的手段，激励普通观看者喜欢“点赞”（双击 666）这个行为，然后发布者无论发布怎样的内容都会非常容易的获得“点赞”这种认可和共识。从而激励了内容发布者更愿意表达自我。</p>
<p>而一些社交应用中采用了“顶”和“踩”两种交互模式时，内容发布者如果被踩，就会极大的打击其发布的积极性。所以主流的社交应用大部分都是只有点赞，没有“踩”的。</p>
<p>或者也可以对观看者保留“踩”，但对发布者不予以反馈相关信息，保证其积极性。</p></blockquote>
<h3 id="打发时间炫耀获得刺激">打发时间、炫耀、获得刺激</h3>
<h4 id="获得刺激">获得刺激</h4>
<p>理论上说，这应该算作 “功利社交” 的动因，获得刺激的方式不仅仅限于社交，暴力的游戏、音影作品都可以获得这种感觉，所以个人理解，这并不算社交中人们最重要的诉求；</p>
<h4 id="打发时间">打发时间</h4>
<p>和获得刺激类似，这一点也不是只有社交才可以解决。只不过社交的方式是更好的解决这件事情的方法之一。</p>
<p>因为有空闲，所以人们希望在空闲当中获得乐趣，方法之一是类似于游戏化的方式，通过一些方式让身体分泌多巴胺（社交的快乐应该也是分泌多巴胺）。按照游戏化的理论，反馈是重要的机制，所以往往获得快乐需要更大的消耗；而社交中“提升达成共识的可能性”这件事，往往仅仅需要信息的交互就可以完成，可以更低的成本获得快乐（不一定是最性价比的，但是最低价的），所以打发时间的重要表现形式就成为了社交。</p>
<p>换句话说，我们打发时间刷朋友圈或者逛论坛的时候，希望的就是获得更多认同的可能性。愿意看而不愿意互动不是因为性价比，而仅仅是因为成本（懒）。所以提升互动，更重要的方式应当是降低互动成本，而不是提升收益（提升收益有帮助，但主要是把参与者从共情社交引导向了功利社交）。</p>
<h4 id="炫耀">炫耀</h4>
<p>炫耀是最契合本文描述的理论的。人们炫耀是因为觉得值得炫耀，觉得值得炫耀是因为预期可以获得更多的认同。如果未能获得足够的认同，就需要做解释弥补以达到期望。如果还是不能获得认同，就会对社交（或者某种特定的信息编码方式）失望，进而从这种方式中流失掉。</p>
<p>因为有主体认知的存在，不同人炫耀的方式是不同的。有的更倾向于以获得更多认同而调整主体认知；有的则倾向于为主体认知找更多认同。</p>
<p>其中第二类，为主体认知找认同的人，一般就是我们所说的思想独立的人。他们更倾向于主动表达，或者主动寻找能与主体认知产生共识的信息或信息渠道。这是因为对于这类人而言，获得更大的“达成共识的可能性”必须通过更主动的方式去实现，而且会常常产生“知音难求”的感觉。</p>
<p>而第一类人，更容易出现所谓的善于社交的人，因为他们更善于获取“获得更多认同的信息”使之成为自己的“信息观点”，从而可以与更多的人达成共识。</p>
<p>当然，当下的社会，或者说当下的社交应用中，有很多已经不是基于情感的炫耀了，而是转变为“功利社交”的“炫耀”，看重的不是“炫耀”带来的情感刺激，而是寻求更实际的利益因素。</p>
<blockquote>
<p>淘宝的买家秀，基本上都不是为了“炫耀”，而更多的是图商家的反现；而商家期望消费者秀出来，也不是为了情感上的认同，而是期待所谓的“真实评价”能带来更好的成交转化和流量。</p>
<p>更准确的说，当开始用利益刺激互动率时，社交就会从共情社交走向功利社交，用户在这里就不再能获得“提升达成共识的可能性”的快乐。</p></blockquote>
<h2 id="社交的性质">社交的性质</h2>
<p>有了上文的各种解释，可以将“提升达成共识的可能性”作为非“功利社交”的唯一驱动力了，本节将基于这个观点，分析社交中一些的性质。</p>
<h3 id="社交是基于人完成的">社交是基于“人”完成的</h3>
<p>通常说有人就有社交，社交不能脱离人存在。这里的“人”，并不是真实的人，而是指具有人格化的信息交互对象。类似于前面提到的“关系”，单纯的某一条信息是不能实现所谓的“提升达成共识的可能性”，最多就是带来一些共识，但不会有进一步的可能性了。所以在不考虑“功利社交”的情况下，知识付费是一个伪命题。</p>
<p>“提升可能性”意味着这个信息主体会持续带来新的信息或者能与之进行交互，而要求达成共识则意味着信息主体的信息编码逻辑是与我们一致的，而且信息的内容和观点在长时间内是保持一致的。也就是说我们需要的是一个可持续交流的有观点的对象，这就是“人”的定义。</p>
<p>能达到上述定义的一些例子：</p>
<ul>
<li>人</li>
<li>宠物（虽然同宠物交流的信息编码和人的不同，但满足的性质是一致的）</li>
<li>帐号（作者、导演、明星等各种信息产生的主体）</li>
<li>杂志、应用</li>
<li>集会、党派</li>
</ul>
<blockquote>
<p>当我们阅读了一个和自己观点很一致的文章之后，我们期待的是去找寻作者其他的文章（以达成更多可能的共识），寻找信息主体而不是信息本身。所以大多数的社交过程都是通过信息检索和筛选人的过程，使得我们在看待社交的时候，往往觉得社交的本质是人或者人与人之间的关系。</p>
<p>一个典型的例子就是追星：人们通过信息熟悉了一个人之后，渴望进一步了解这个人未来的信息，更多的信息，以达成更多共识的可能性。所以我们看到有“追星”的现象，相对而言很少有所谓的追物的现象（也不是完全没有，要么是把物人格化，例如追小说，追电视剧，要么是功利化，例如收藏）。</p></blockquote>
<h4 id="人格化主体的选择">人格化主体的选择</h4>
<p>当我们将人格化主体这个概念抽象出来之后，我们会发现，<strong>社交并都不是一个人与一群人达成共识的过程，而是一个人的主体认知同外在的一个人格化概念达成共识的过程</strong>。最终每个人真正需要达成共识的人格化主体并不会很多。</p>
<p>能以一个人撑起一个人格化主体的，就是我们通常说的主要的社交关系，否则我们都是与一个群体（同学、同事、同一个组织的伙伴等）或者一个符号（兴趣、明星、帐号、应用）达成的共识。一个人达成共识的人格化主体越少，他的信息或者观点就会越“偏激”，或者说偏离大众。</p>
<blockquote>
<p>人人网、facebook、微信等等，最先建立的都是以单个人为主体的社交关系，此时的社交体验是最好的。后来我们不能以群体或者符号的方式与其他人格化主体建立链接，而必须一单个人为单位建立链接，既削弱了原本的单个人的社交关系，由不能很好的服务人格化主体的社交关系，所以此类应用都会在一定时间后就从共情社交走向了功利社交，或者一些人把这些工具整体（微信朋友圈是看同学、同事近况的地方；聊天是维系与单个人连接的方式）当作了某种人格化主体。</p>
<p>微信的群相对于原本的聊天更好的解决了人格化主体的社交关系，但社交是一个人的事情，不是一群人的事情，每个人对群组的定义其实是不同的。</p>
<p>当然，微信后来的解决办法是引入新的人格化主体——公众号，来弥补社交体验的下降。但总得来说，社交中应当还有很多市场可以尝试：例如重塑以人为单位的社交方式，或者构建更适合的与人格化主体社交的方式。</p></blockquote>
<p>所以从社交的本质来看，社交并不是一群人的事情，而是一个人的事情。只不过一群人的社交通常可以带来更大的好处和便利性（更多达成共识的可能性），马上我们就会探索到这一点。</p>
<p>社交关系的建立和选择，其实就是主体认知选择外在的人格化主体的过程。选择的依据，就是“未来达成共识的可能性”作为评判标准。达成共识可能性高的人格化主体，我们会保持跟这类主体的关系，以保证未来持续的高可能性；达成共识可能性低的，我们就会避免与其保持关系（共情社交中是这个样子的，功利社交有时会不得不维持关系）。</p>
<blockquote>
<p>前文提到的广场舞大妈的例子，因为我们与明星达成共识可能性高，与大妈达成共识的可能性小，所以我们倾向于保持同明星的联系，避免同大妈的联系。当我们的一个选择与大妈的观点达成了共识，意味着我们同明星的共识变少，所以我们会不开心；而是否能与大妈达成共识，并不在我们 Top 的人格化主体选择中，所以共识也不能增加我们整体达成共识的可能性。</p></blockquote>
<p>社交中的行为准则，一方面是与高共识的人格化主体保持联系，以获得未来共识的可能性；另一方面是会倾向于做一些能增加与各种高共识的人格化主体达成共识的可能性的行为。</p>
<blockquote>
<p>八卦爱好者们喜欢打听八卦的原因是，获得的这个新的信息更容易与他的其他人格化主体达成共识，从而提升了与其他人格化主体达成共识的可能性。所以如果一个人社交中主要的人格化主体们都达成了某个共识，这个人就会更倾向于接受这个共识。这一点后面的群体效应中会进一步阐述。</p></blockquote>
<p>因为我们会进行社交关系的选择，这就意味着社交关系会发生更替。例如曾经容易达成共识的人，感觉到与其达成共识的可能性降低了，我们就会寻找其他高共识的人格化主体。给予前文中达成共识的相关要素：编码、内容、观点，很容易就能得出结论：曾经的同学、同事之间的关系会逐渐疏远，除非能一直保持联系，或者观点上一直都能达成共识。最后一种社交关系（观点共识）相对而言在时间维度上更加稳定。</p>
<blockquote>
<p>社交恐惧症是新时期出现的现象——很多人更愿意在网络上交流，而不愿意在真实社会中面对面交流。本质原因是网络社交中，我们通过了一些手段（例如更容易点赞，更难表达否定观点），让用户觉得相关的渠道（人格化主体）达成共识的可能性更高，于是在渠道选择中，实体社交排在了更靠后的位置，于是就会出现社交恐惧症——放弃了真实生活中的社交关系和行为，以维持自己整体的达成共识的高可能性。</p></blockquote>
<h3 id="社交中必须创造新的共识">社交中必须创造“新”的共识</h3>
<p>还是因为“可能性变大”这一点，社交中必须有新的东西出现，否则哪怕达成的共识非常的多，也无法带来“可能性变大”这一特性。</p>
<blockquote>
<p>所以阅读 wiki 百科 并不是共情社交行为，但参与 wiki 百科 的编写创作的过程是共情社交行为。类似的，大部分的学习行动都不是社交行为，但不代表不能通过社交行为进行学习。</p></blockquote>
<p>新共识包含两件事：一是新的信息（至少感觉是新的）、二是信息观点一致。而更多的可能性意味着“新的达成共识”的信息越多，效果越好。所以社交中重要的衡量要素是“共识的增速”，或者说“新共识的数量”。这就解释了 wiki 为何给人的感觉不是一种社交行为。</p>
<h4 id="更多新信息">更多新信息</h4>
<p>结合上文中人格化主体的概念，如果对应的人格化主体需要保持“新共识的数量”，产出信息的能力和传递信息的能力必须足够。所以如果人格化主体是一个与你通路顺畅的信息源，效果好于通路不顺畅的信息源；如果人格化主体的产出新信息能力强，效果好于低产的主体。</p>
<p>基于这样的观点，很容易就能产生如下这种自然的结论：</p>
<ul>
<li>网络时代是社交的重要温床（信息通路顺畅）；</li>
<li>异地恋很难；身边的朋友效果更好；</li>
<li>如果人格化主体是一群人，效果好于一个人（产出新信息的能力强）；</li>
<li>愿意闹绯闻的明星比专心演戏的明星更容易吸引人。</li>
</ul>
<h4 id="更多共识">更多共识</h4>
<p>主体的人感知能否达成更多的共识（提高达成更多共识的可能性），判断标准并不是达成共识的总数量，而是单位努力下，达成共识的数量。说白了就是达成共识的效率。</p>
<p>例如一个不能被很好检索的图书馆，哪怕其可能达成的共识非常的多，但是带来的感觉就是：达成共识的可能性很低；反过来，如果一个更小的图书馆，但是有便捷的检索，就会让人觉得达成共识的可能性很高。</p>
<p>所以达成共识的效率也是社交中非常重要的要素。解决效率问题有两个主流的途径：</p>
<ul>
<li>基于人的偏好把更可能达成的共识展示给对应的人；</li>
<li>基于人的已有共识产出新的共识。</li>
</ul>
<p>其中第一种是当下抖音、快手采用的方式，但相比于第二种，未必有本质的优势；第二种，基于人已有的共识产出新的共识，就是社群、社区、论坛的模式，既通过已有的共识吸引具有相似共识的人聚集，又可以通过已有的共识不断的产出新的共识。</p>
<blockquote>
<p>社区的命门之一就是生命周期，或者更确切的说，是规模问题。</p>
<p>首先分析一下社区这种社交形式的优点：先通过少数人（可能是一伙有紧密关系的人）或者少数内容烘托氛围，背书社区这个人格化主体在基于共识创造新信息的能力，其中的共识就是社区的内核（主题）。由此，人格化主体形成，与之有共识的人就会聚集在这个社区，而且会形成网络效应，更多的人贡献共识，提升创造新信息的能力，进一步又吸引了更多的人来。绝大部分的论坛都是这种模式的写照。</p>
<p>随着规模的扩大，共识的范畴也会扩大，原本基于信息内容的观点的共识，慢慢发展出更多的基于内容的共识（最早我们是为了共同的目标走在一起，后来怀念的确实共同战斗的岁月），同时产出的优质共识还会吸引一批怀着功利社交目的而来的人，他们会进一步增加社区成员的复杂性，最终就会出现<strong>共识的偏移</strong>。共识偏移的过程一般来说应当是信息范畴扩大的过程，越来越多的人进入导致不同人达成的共识的总范围变大，但是对于但个人而言，却意味着单位努力下获得的共识变少了，因为总信息变多了，检索效率下降了。当效率降低到一定程度时，社区给人的感受就变成了“获得更多共识的可能性在降低”，那么这个用户就会舍弃这个社交渠道，而且往往最先流失的会是最初形成优质共识的那批用户。</p>
<p>这样的过程几乎是不可逆的，甚至是注定要发生的。所以这也是各种社区都需要坛主、吧主进行管理的原因之一；甚至有些社区还需要邀请制来控制社区的规模，以此来延长社区的生命周期。</p></blockquote>
<h3 id="社交中的群体效应">社交中的群体效应</h3>
<p>前文提到了，社交是一个人的内在与外部发生交互的过程，也就是说社交应当是一件很个人的事件，为何又会出现群体效应呢？</p>
<p>主要的原因是每个人的社交对象都不单一，会有多个与其内在认知发生交互的人格化主体，而多个人格化主体都需要与内在认知形成互动以达成更多的共识，那么就会影响内在认知。换句话说，一个人的内在认知最终是与一些人格化主体达成共识的，如果这些人格化主体已经形成了某些共识，那么这种共识就会极大的影响这个人的内在认知，甚至影响这个人未来对新的人格化主体的选择。</p>
<p>原本所谓的“形成更多共识的可能性”就会天然形成关系，将有共同性的人聚集在一起，他们又会进一步强化彼此之间的共识，形成群体的共识。所以自然的社交就会发展出一种群体效应，同时这种群体效应也会自然的影响到这个个群体中的每一个人。也就是“三人成虎”这个典故所昭示的道理。</p>
<h4 id="意见领袖">意见领袖</h4>
<p>如果一个群体中，某一个人相比之下更容易与这个群体中的其他人达成共识，那么对于其他人而言，与这个人的交流就可以更好的提升“达成共识的可能性”，于是群体中更多人的选择是与这个人交流，从而导致这个人会获得更多的信息（而且是有一定群体共识基础的信息），从而提升新信息的数量，进而让自己与其他人相比达成共识的可能性更大。最终群体性的共识将几乎由这个人把控：每个人比较优先的信息来源都是从这个人这里获得的，同时如果这个人可以影响其中的一部分人，那么就会因为群体效应影响更多的人，乃至影响整个群体共识。</p>
<p>这样的人就是<strong>意见领袖（KOL）</strong>，其影响力是比其他人大非常多的。前面描述的过程是指，意见领袖通常来说都是非常稳固的，但形成意见领袖的途径有很多，也许是意见领袖自己的人格魅力，也许是外力要求所有人必须将信息与意见领袖交换，也许真的只是人与人的差异导致意见领袖更加擅长与人交流或者搜罗信息（小群体中的八卦之王）。</p>
<p>当然，因为意见领袖是有可能自发形成的，所以意见领袖的地位也不是牢不可破的。基于前面的描述，有两个直接的途径可以销毁一个意见领袖：</p>
<ul>
<li>一段时间内使意见领袖无法产生新信息；</li>
<li>降低意见领袖信息达成共识的效率，例如使其产生一个与大家共识都完全不一致的信息；</li>
</ul>
<blockquote>
<p>一般来说，明星都可以算作是意见领袖的一种，所以上面描述的两点也是抹杀明星的最直接手段：封杀、丑闻（人设崩塌）。</p></blockquote>
<p>关于意见领袖的一些要点，在传播一章中有单独的论述。</p>
<h2 id="传播">传播</h2>
<p>传播和社交本质上是共生体：他们都是在描述人同信息之间的交互关系。唯一的区别在于：社交所针对的主体是人，而传播所针对的主体是信息：<strong>传播的就是如何让信息与更多的人达成共识</strong>。</p>
<p>一般的传播研究会分为三个部分：大众传播、人际传播和人内传播。<strong>人内传播</strong>描述的就是内在认知同外在人格化主体达成共识的过程；人际传播，主要是上文中描述的社交的<strong>群体效应</strong>。传播学中关于人际传播一个非常重要的性质就是<strong>二级传播理论</strong>：由媒介到意见领袖再到受众的传播方式。各种信息一定是通过意见领袖的筛选，再到达每个人的。</p>
<blockquote>
<p>人际传播中，并不是说只有意见领袖再获取外部信息，再次传递给其他人。而是意见领袖所带领的一群人中，所有人都在获取外部信息，但在这群人中，任何信息都没有办法直接扩散到其他所有人，而是先传递到意见领袖这里，由意见领袖甄别是否能让这群人中更多的人达成共识，然后再完成意见领袖到大家的传播路径。这群人就是一个星状的网络结构，其中星节点就是意见领袖。</p></blockquote>
<p><strong>大众传播</strong> 更多的是研究信息如何通过渠道在更多的人面前获得曝光的。曝光不代表能被认同（达成共识）。所以大众传播相对而言偏渠道性质，更反映传播的广度，人际传播相对而言更反映传播的深度（传播的效率），两者相结合，最终影响传播的效果。</p>
<p>传播都是有目标人群的——即期望与目标人群对某一内容或观点达成共识。与社交不同，传播的目的不是维持一个“达成高共识可能性”的人格化主体（渠道），而是一个非常具体的共识（某一个信息被认可），所以传播的要点和社交就有了很多的不同。</p>
<h3 id="信息在传播中的要点">信息在传播中的要点</h3>
<p>传播的主体是信息，目的是与目标人群对信息达成共识。而信息几乎是传播中最可控的要素，所以最值得探讨其要点。依旧从信息的三个要素出发：编码、内容、观点。</p>
<h4 id="信息的编码-1">信息的编码</h4>
<p>首先是编码，为了让信息可以同目标人群达成共识，编码需要具备两个特点：</p>
<ul>
<li>编码与目标人群是一致的；</li>
<li>编码与触达到目标人群的通路是一致的；</li>
</ul>
<p>第一点便于理解，尤其是那些可以直接触达到目标人群的传播方式（例如电视广告、互联网广告等），只需要保证编码与目标人群一致即可。当然编码也有选择的余地，例如是通过语言的方式、还是通过音乐的方式、是通过图文的方式、还是通过视频的方式。这些不是由传播者决定的，而是由目标人群决定的，他们更容易接受什么样的编码，就值得用什么样的编码。</p>
<blockquote>
<p>拼多多传播中的编码，就是改编的歌曲，即是大家熟知和可接受的形式，且易于二次传播，又体现了和其他信息在编码上的差异性，跳出了同编码下的内容竞争，更容易被目标人群差异化的对待。</p></blockquote>
<h4 id="信息与目标人群达成一致">信息与目标人群达成一致</h4>
<p>信息与目标人群达成一致，有两种情况：</p>
<ul>
<li>目标人群是与信息的内容达成一致</li>
<li>目标人群是与信息的观点达成一致</li>
</ul>
<p>难易程度来说，与内容达成一致更容易，与观点达成一直更难；但从效果来说，内容的一致会被冲淡和覆盖，不一定长久，而观点的一致具有相对而言的可持续性。所以下面我们将针对这两种情况分别分析。</p>
<h5 id="内容的信息传播">内容的信息传播</h5>
<p>如果只需要就信息的内容达成一致的话，那么信息只要可以触达到目标人群即可，知道了，便可以达成一致。例如绝大部分的广告（包括拼多多的），主要就是曝光广告的名称，当目标人群再次看到时，自然就会有非常大的达成共识的可能（因为不需要观点一致）。</p>
<p>但一个人接收到的信息的内容是非常多的，他只会选择留下未来更容易达成共识的信息内容，或者有实际价值的信息（功利社交），所以通常情况下得到曝光的信息内容，其价值并不大。只有满足了这个人能够经常与这条信息内容达成共识，才会形成价值。也就是说，信息的内容需要经常被目标人群看到，内容达成一致需要屡次曝光，多渠道曝光，最好是铺天盖地的曝光。</p>
<blockquote>
<p>一般来说，《通知》就是一种需要对内容达成一致的传播需求，保证通知切实有效，最好的办法就是在目标人群生活的各个场景中都予以曝光，例如食堂的桌子上、卫生间的门上、工作空间的墙壁上等等，以保证多次曝光，目标人群就会提起足够的重视。</p>
<p>当然，如果《通知》是包含利益因素的，那么目标人群会用功利社交的方式来对待，自己就会找办法来保证通知的触达率和效率的。</p>
<p>一般来说，大多数政党的传播，是偏通告性质的，而且往往未必包含利益因素，就需要铺天盖地的方式进行宣传，例如我党当年用大喇叭做思想传播，即具有编码的优势，又保证了频繁曝光，所以效果拔群。但现在就缺少行而有效的传播手段，大家对党产生的共识就会越来越少。</p></blockquote>
<h5 id="观点一致的信息传播">观点一致的信息传播</h5>
<p>如果信息所蕴涵的观点天然就是和目标用户一致的，那么这件事情容易很多，只需要触达就可以了。例如一下兴趣群体或者小众网站，其目标人群就是能同自己达成共识的人群，那么也不需要铺天盖地的广告轰炸（内容达成共识的传播方式），只需要有触达即可。例如女权、同性恋、地下党等等。甚至为了追求触达的有效性，可以通过搜索引擎的广告方式，精准定位目标人群。</p>
<blockquote>
<p>“酒香不怕巷子深”描述的就是这种情况，酒香原本吸引的就是喜欢喝酒的人，目标用户就是可以达成共识的人，所以只要可以触达（酒的香气触达到喜欢喝酒的人），就可以达成好酒的共识，不需要看到华丽的店铺门面，或者门庭若市的场景。</p></blockquote>
<h5 id="观点不一致的信息传播">观点不一致的信息传播</h5>
<p>但还有很多情况是信息所蕴涵的观点不那么容易同目标人群达成一致，或者说希望可以改变一部分人的内在认知，以达到同信息的观点一致的目的。这件事是传播中大家最希望做到的，因为传播的目的就是希望覆盖和影响更多的人。如果仅仅是与信息能达成一致的人达成一致，那传播的价值就非常的小，传不传播，都是能达成一致的。</p>
<p>还是先从人内传播来看如何影响一个人的观点：如果与一个人发生关联的主要人格化主体都对某一个观点达成了共识（或者是都输入了某个信息，但信息蕴涵了同样的共识），那么这个人就容易接受这个观点，因为接受了这个观点，可以提升与多个人格化主体达成共识的可能性。</p>
<p>这就得到了观点不一致时，信息传播所需要具备的条件：让与目标人群相关联的主要人格化主体都传播蕴涵某一观点的信息，最终就可以增加目标人群接受这个观点的可能性。</p>
<p>所以针对观点不一致的信息传播，第一个要点是：<strong>信息的传播要覆盖目标人群的主要连接对象（就是说不仅仅需要触达，而且需要覆盖）</strong>。这样一个人才可能从多个不同的人格化主体中接收到蕴涵同一观点的信号。</p>
<blockquote>
<p>淘宝传播的困境：现在对于中国的大部分人而言，微信都是其社交中非常重要的人格化主体，甚至在微信上会有多个人格化主体（各种朋友关系现在更多是通过微信维护的）。但是淘宝的传播却很难通过这个人格化主体发出声音，从而淘宝的传播几乎不具有影响目标人群观点的能力，之具有内容传播的能力。这对于树立淘宝的品牌形象等都是巨大的挑战。</p>
<p>所以现在很多人交流的人格化主体向抖音转移时，更应该把握这样的机会，通过抖音做观点的传播。</p>
<p>拼多多利用微信完成了一个观点的传播：拼团。淘宝做拼团最大的挑战也许不是价格或者品质等问题，而是用户想到拼团，就只会想起拼多多。</p></blockquote>
<p>仅仅覆盖了目标人群的主要连接对象，不代表信息就能传达到目标人群。因为那些中间节点（目标人群的主要连接对象）只会接受与自己能达成共识的信息，且想目标人群只会传播容易与目标人群达成共识的信息。鉴于信息的观点和目标人群是不一致的，这就导致中间节点并不会倾向于把这样的信息传播给目标人群，所以这些蕴涵观点的信息必须还要具备两个特性：</p>
<ul>
<li>蕴涵观点的信息依然可以同中间节点达成共识；</li>
<li>蕴涵观点的信息依然可以同目标人群达成共识；</li>
</ul>
<p>因为中间节点已经是目标人群的主要节点了，所以还好，不需要针对中间节点和目标人群分别形成共识，只需要一个目标人群可以认同的共识即可，然后通过多个中间节点与目标人群传播信息，就可以让目标人群也易于接受我们试图传播的观点了。</p>
<p>解决手段也比较明显了：</p>
<ul>
<li>蕴涵观点的信息，其携带观点的内容是可以与目标人群达成共识的；</li>
<li>蕴涵观点的信息，其携带观点的信息中还有可以与目标人群达成共识的观点；</li>
</ul>
<blockquote>
<p>咪蒙的文章，大多是利用了内容与大多数人达成共识，进而携带观点来影响人的。类似的，蹭热点，也是传播中非常常见和有效的手段；</p>
<p>而本文中开篇提到的，小红书的营销套路——则是用更普世的观点来携带其他观点，用“老公爱我”来携带“这个包值这么多钱”，或者用“她没我好”来携带“我也应该有这么贵的包”。</p></blockquote>
<h3 id="人际传播">人际传播</h3>
<p>或许是受限于科技水平或什么原因，当下人类的社交活动，从信息传播的编码来看，主要是人和人的传播，甚至是同一种语言的人之间的传播；从信息传播的内容来看，主要会受限于血缘关系、地域等因素。</p>
<p>所以哪怕信息的编码有不断的创新（Facebook、微信、抖音），内容更加泛化（全国热播的电视剧、全国连锁的商店、全国的淘宝等等），但是绝大部分情况下，一个人的交流主体还是会限定在一定的范围内，其中的人相互之间都会有连接（社交关系），从而形成了一个个群体。而在群体中，就会出现意见领袖。哪怕是新的社交编码，背后连接起来的，依然是这样的群体（会出现新的群体，但是范式没有差别）。</p>
<p>于是绝大部分的信息传播，还是会形成<strong>二级传播</strong>，需要通过人际传播来传递信息。</p>
<p>针对人际传播，我们再回顾上一章中信息传播的要点便会发现，在人际传播中，信息与目标人群达成一致变得非常的容易——只需要 KOL 保持一定频率的传播，就可以保证信息的传播效率，无论是期望内容达成共识还是观点达成共识。因为 KOL 可以同时影响到目标受众及其主要的社交主体，而消息还会在圈子内持续传播，所以信息的覆盖度和频次也会成倍的加强，达到各种预期的传播效果。</p>
<p>当然 KOL 传播的内容也还是需要有一点约束的：即容易与其群体大多数人形成共识，或者说不会影响到 KOL 在群体社交中的影响力。例如明星代言需要与人设相匹配、社区的达人不要随意发广告等等。</p>
<p>通过 KOL 进行传播就成了传播中异常有效的方式：</p>
<blockquote>
<p>各种政权或者政党，都会在其管辖的区域内设立管理机构，设定管理者，并营造成 KOL。例如我党的书记角色，相比于行政管理者未必有更大的实权，但是在特定时期却是最重要的 KOL。然后政权或者政党可以通过 KOL 来形成自己的传播。对于自发形成的社群，收编其 KOL ，以达到控制传播的效果。</p>
<p>随着社会的发展，原先的 KOL 的传播能力或影响力逐渐下降，越来越多的出现自发形成的 KOL，配套而来的就需要更完善的对 KOL 的监管方案。</p>
<p>所以 PGone 的事件出现后，这个 KOL（PGone） 立马就会遭到封杀，这样这个群体不多久就会形成新的 KOL。</p></blockquote>
<h4 id="kol-的要素">KOL 的要素</h4>
<p>进一步来说，如果可以营造可控的 KOL 就意味着传播中拥有了一个稳定的传播通路，可以形成稳定的传播抓手。这里再梳理一下 KOL 的一些要素：</p>
<ul>
<li>粉丝 对 KOL 是共情社交，不要用利益刺激粉丝，导致粉丝从共情社交走向功利社交（low 一点说，就是 KOL 宁可向粉丝要钱，也千万不能给粉丝发钱）；</li>
<li>KOL 要保持与粉丝之间的高的达成共识的可能性，也就是说 KOL 要保证新内容的量和质量；</li>
<li>粉丝之间具有社交的群体效应，所以 KOL 要保持与整体达成共识，也就是 KOL 需要有人设，这一点同上面的质量的含义是一致的；</li>
<li>为巩固 KOL 的地位，应更多的接纳源自粉丝的信息，即强化自己，又避免粉丝中自发形成新的 KOL（除非 KOL 自己就是这个群体的共识，例如明星）；</li>
<li>保证粉丝之间的交流，以此扩大 KOL 传播的信息的影响力。</li>
</ul>
<h2 id="人际交往">人际交往</h2>
<p>人际交往不过是社交行为的一个方面，理论上并没有什么更新的内容。但是如何做好人际交往，确实我们每个人都需要真实面对的事情，所以还是值得拿出来探讨的。</p>
<p>首先需要理解一个概念：社交关系并不是相互的。例如一个人对你是共情社交，并不意味着你对他也必须是共情社交。所以哪怕我们是以功利的态度面对人际交往，也不代表我们无法收获真情（共情社交）。</p>
<h3 id="善于社交是长于共情社交">善于社交是长于共情社交</h3>
<p>首先需要探讨的是共情社交和功利社交在人际交往中的价值。功利社交，别人是怀着目的性的，如果目的无法达成，就意味着社交关系的终结；一个人能解决其他人的某些特定需求，但是覆盖度是有上限的，往往总有些人对你一无所求。另外有所求，往往也意味着你需要给予他人利益，还需要考量具体的成本。</p>
<p>而共情社交，别人是追求达成共识的可能性，换句话说追求的不是当下而是未来，这就意味着共情社交具有一定的长期性；另一方面，我们在共情社交中只需要交换信息，并不需要更具体的利益付出，也就是说是低成本的。最后，如果一个人有能力与各种人达成共识，那么对他来说，就非常容易获得社交关系。</p>
<p>所以通常，善于社交的人一般都是在共情社交方面具有更强的能力，当然也不排出例外，例如电影《教父》中马龙 · 白兰度饰演的老教父，就是用功利社交维持的大量社交关系，获取自己共情社交上的满足。但这种情况只有少数人可以享受。</p>
<p>大多数情况，善于社交的人都会让与其接触的各种人都感到舒服，感到被尊重，被认同；对于他们来说，功利社交只是扩大共情社交的渠道——别人找我办事，然后成为我的朋友，为我办事；而不是变成一笔交易，你为我付出多少，我就为你付出多少。当然，这里只是举例，并不是说所有善于社交的人都是怀着功利社交的目的的。</p>
<h3 id="人际交往中共情社交的要点">人际交往中共情社交的要点</h3>
<p>先从某个特定的人做起。如果我们试图让这个人对我们形成共情社交的关系，要点还是从信息交流出发，三个因素：编码、内容、观点。</p>
<h4 id="编码一致">编码一致</h4>
<p>首先是编码尽可能的一致，不要刻意展现自己以为的“高大上”，而是与目标贴近。</p>
<blockquote>
<p>除了特定环境下，通常我们都会对中文中夹杂着很多英文单词的人感到反感，觉得他们很装，本质上就是编码上无法达成共识；反过来如果一个老外突然说了一句中文，甚至是中文的方言，亲切感就倍生。例如“福圆爱”说着一口东北话时，很轻易就征服了中国广大人民。</p>
<p>当我们在异国他乡时，遇到任何一个会说中文的人，不论是中国人还是外国人，都会倍感亲切。</p></blockquote>
<h4 id="内容匹配">内容匹配</h4>
<p>其次是内容尽可能的一致，多说对方熟悉的，了解的，认同的东西，少说对方不理解的东西。这一点很多时候会被人误解，觉得“我说一些你们不懂的事情，会显得我很厉害，你们就会更愿意听我说”。但能达到这样的效果，只有两种可能性：1. 你是 KOL，大家已经都愿意认同你的观点；2. 这是功利社交，大家就是为了听你说他们不知道的事情。</p>
<blockquote>
<p>马总说的道理大家都喜欢听，因为马总已经是 KOL 了。但如果一个谁都不认识的人大谈特谈“新零售战略”，只会让人觉得反感。类似的，明星的各种小故事我们都喜欢听、甚至我们身边已经是朋友的人，他们自己的故事我们也愿意听，但这是建立在已经形成了共情社交的基础上的。</p>
<p>当然，如果你就是想了解新零售，也会愿意听他的说法。或者你相信他可以帮助你解决一些具体的问题。这个情况就是功利社交了。我们进行知识付费，或者参加学术论坛、讲座等等，都是基于功利社交的。</p></blockquote>
<p>所以如果单从内容角度来说，尽量是找和对方更有共同语言的事情来沟通，或者是你真的了解对方生活中的事情，或者就找一些社会热点、国家大事来聊。出租车司机、公园老大爷往往都是“国家大事”派的。</p>
<p>这里需要额外强调两点：</p>
<ul>
<li>并不是内容必须是对方知道的，而是说内容必须是可以让对方认同的。例如街边拿身世故事乞讨的人，他们的内容是我们不知道的，但是往往是让我们容易认同的。</li>
<li>并不是内容上必须与对方一致，而是不能认为内容不一致是更好的策略。哪怕是 KOL ，也是先通过一致的内容和观点，达成了跟大多数人的共识后，才可以发表一些内容不一致的信息的；</li>
</ul>
<blockquote>
<p>很多社交大人都是非常善于观察的人，他们可以从别人的一举一动中洞察到对方的来历，从而准备好适合与之达成共识的内容。</p>
<p>当然，也有很多的套路存在。例如见面聊天先问对方是哪里人，然后扯出当地自己了解的信息，这样就很大概率可以达成共识。如果能碰巧找到一个共同认识的人，一下子就创造了非常大的达成共识的可能性，从而大概率形成稳定的社交关系。这个要点就是社交网络中常用的“<strong>朋友的朋友</strong>”，或者“<strong>共同的好友</strong>”。</p></blockquote>
<h4 id="观点的认同">观点的认同</h4>
<p>关于编码的一致和内容的匹配都是容易理解的。但是对于真实的人际交往中，编码方面大家大多数都能达成一致；而内容的匹配则很难做到匹配上进行人际交往的所有的人——我们每个人掌握的信息总是有限的，寻求信息内容的共识并不是总能达成的。</p>
<p>但是哪怕信息的内容不一致，并不完全影响观点的一致性。例如歌星的粉丝，也许有些人是因为其颜值喜欢这个明星的，也许有些人是因为歌声喜欢这个明星的，但他们的观点达成了一致：“我们喜欢这个歌星”，这就不妨碍他们彼此形成共情社交，互相交流更多的信息，从观点的一致进一步完成内容的一致性。</p>
<p>前文说过，观点可以形成比内容更加持久的社交关系，持有相同观点的人更容易成为社交好友，但是了解一个人的观点往往比了解一个人知晓的内容更加复杂。所以除了类似于内容匹配中提到的，通过高超的洞察能力发现对方的观点外，更可行的办法是鼓励对方说出自己的观点，然后表达理解和认同。</p>
<blockquote>
<p>我们描述人际交往时，会用到两个词“盛气凌人”和“如沐春风”。盛气凌人，指得就是这个人对别人的观点都不认可，只认可自己的观点；如沐春风，则是愿意让人愿意表达出自己的观点，而对方还会与之附和，感到被理解被认同。</p></blockquote>
<h4 id="社交达人的特点">社交达人的特点</h4>
<ul>
<li>语言通俗易懂，与大家打成一片；</li>
<li>善于找到大家共同的话题，促成大家达成共识（特定场合可能还会有一些荤段子之类的）；</li>
<li>有洞察能力，跟各种人都可以找到共同话题，可以引导群体边缘的人进入到群体的共识中来；</li>
<li>善于倾听，乐于对他人的观点表示认同或认可；</li>
<li>能带给整个群体丰富的新的共识，即对群体中很多的个体，都是能达成高共识可能性的人。</li>
</ul>
<blockquote>
<p>知识渊博的人未必就能成为社交达人，哪怕找他们问询的人很多，但他们也很可能成为“老学究”；</p>
<p>八卦之王也未必能成为社交达人，虽然他们很容易找到大家的共同话题，促成大家达成共识，但也可能不善倾听，或不善带来更多共识，让人觉得只是个“嚼舌根子”的人。</p>
<p>有钱有权，会提升成为社交达人的可能性，因为更容易通过功利社交构建社交关系，进而形成内容的匹配，以培养共情社交，降低社交难度（观点的认同）。</p></blockquote>
<h3 id="社交关系中最常见的矛盾">社交关系中最常见的矛盾</h3>
<p>最常见的矛盾，也是社交应用中常常犯的错误，就是<strong>当一方寻求共情社交的信息认同时，另一方却反馈的是功利社交中的利益</strong>。</p>
<blockquote>
<p>男女朋友间很常见这种情况，女生做出种种举动试图寻求男朋友的安慰或理解，但男朋友却反馈成非常物质的表达。</p>
<p>类似的问题也常常出现在家庭教育中：家长往往会对孩子表达严厉的不认可，并强调“我这是为了你好”，这是一种典型的功利社交的态度来回应孩子。但孩子往往想要的不是这种“功利的好”，而是渴望得到共情社交中下，家长的认可。这种认可并不是某个确定的事情或观点，而是对未来形成认可的预期。孩子觉得未来家长也会经常认可自己，就会充满自信，快乐与家长交流；但孩子如果觉得未来家长给予的也更多的是打击，那么他们就会试图回避同家长的社交连接，造成家长和孩子间关系紧张。</p></blockquote>
<p>所以当遇到类似情况时，首先要判断对方寻求的是共情社交还是功利社交。如果是共情社交，那重要的不是判断事情的对错，或者寻找解决办法，而是需要找到各种能与对方达成共识的机会。例如朋友找你抱怨时，不需要表达对抱怨的事情的看法，拉着朋友一起吃大家都喜欢吃的好吃的就 OK 了。</p>
<p>如果一个 APP 是“有温度”的，也就是愿意倾听用户的表达，并表达认可的，那么它就可以让用户觉得更温暖；反过来，如果一个 APP 一直持着“我是为你好”来服务用户，哪怕真的让用户得到了更多的利益价值，但用户也只会在“功利社交”中寻求特定需求的时候，想到使用这个 APP</p>
<blockquote>
<p>Web2.0 时代，带来的最大的变革就是用户可以表达自己的观点，而 Web2.0 技术赋予了其他人认同的能力，从而让用户觉得自己在相关的 APP 上是被认可的，相关的 APP 便充满了温度。例如 博客、 微博（早期）、人人网等等。</p>
<p>反过来，电商、搜索等工具性质的应用，却缺少让用户表达并获得认同的空间。淘宝中提供了旺旺，由商家来承接社交关系，出现了“亲”文化；百度则通过贴吧、百科、问答承接大家表达和获得认同的意愿。</p>
<p>非互联网领域，海底捞是非常典型的运用共情社交提升价值感的案例，它让每个用户都感到被尊重被认同，所以海底捞不仅仅是一个火锅品牌，还是一个可社交的人格化主体（当然很多人不会把海底捞当作社交对象，主要是在于编码、频次等问题）。</p></blockquote>
<h3 id="关于爱情">关于爱情</h3>
<p>我们这里主要从共情社交的角度谈爱情，但不谈性、也不谈婚姻，虽然这两者都与爱情有着密切的联系，甚至这两者也能促进共情社交，达成很多的共识。</p>
<p>爱情有多种因素共同促成，不同的爱情里，各种因素起到的作用大小各不相同，各因素也没有优劣之分。爱情的对象往往会发生在：<strong>一个可以极大提升个人共情社交的人</strong> 。这个条件意味着如下的可能性：</p>
<ul>
<li><strong>这个人可以与我达成大量的共识。</strong> 青梅竹马是一种很容易发生爱情的方式，共同的经历也容易萌发爱情；而如果能跳脱基于信息内容的共识，上升到基于信息观点的共识，那么两个人就会觉得无论是否彼此都了解的东西，都会形成共识，这时候就是有所谓默契或者一见钟情的感觉。</li>
<li><strong>这个人可以创造大量我与其他渠道的共识。</strong> 一个人是公认的品格好的人、长得帅的人、有名望的人；或者是这个人可以创造很多让我与其他渠道达成共识的人，例如如果一个地区的人都具有某一特性，他们也希望找具有同样特性的人。</li>
<li><strong>这个人可以满足我的大量功利社交诉求，从而帮助到我完成更好的共情社交。</strong> 这种帮助可以是节约了工作时间，可以将更多的经历投入到共情社交；也可以是提供了很多共情社交的机会和便利。</li>
</ul>
<p>这三点在任何一段爱情里都不是孤立存在的，也会共同发挥作用。例如一个女生跟一个男生有精神上的共鸣，这并不妨碍她也希望男生能送她一个可以在朋友们面前炫耀的包包；也不会因为女生希望男生更辛苦些赚钱养家，或者买房买车，就意味着女生只爱男生的钱。</p>
]]></content:encoded>
    </item>
    <item>
      <title>场景的用户认知度（熟客率）研究</title>
      <link>https://blog.uglyboy.cn/posts/2016/09/14/</link>
      <pubDate>Wed, 14 Sep 2016 22:40:50 +0800</pubDate>
      <guid>https://blog.uglyboy.cn/posts/2016/09/14/</guid>
      <description>如何用数据描述用户心智</description>
      <content:encoded><![CDATA[<h2 id="背景">背景</h2>
<p>个性化推荐去年在我们集团取得了非常优异的成绩，为大促、日常场景都带来了非常大的突破，我身边的朋友们也都纷纷说现在的淘宝的个性化很棒。尤其是首图个性化技术，为我们打开了崭新的一扇窗，让诸多场景的日活、成交等核心指标都有了翻倍的提升。</p>
<p>有了更先进的技术，却也带来了一些新的疑问——首图个性化究竟给各个具体的业务 场景带来了什么？各种业务指标的提升可能更多的反映着首图个性化的效果，与场景内的效 果怎样并没有特别大的关系了。所以我们需要有一个新的判定标准，让我们自己知道——现在场景内的效果并不能让用户满意，不急于通过首图来引流；或者现在场景内的用户认知度 和归属感很高，应当获得更多的引流来吸引更多的用户。</p>
<p>如果存在这样一个判定标准，那么它应当是用户对一个场景的认知情况——即当用户 并不知道现在场景内的推荐效果时，他依然愿意来到这个场景的意愿。这便是我的一个基本假设：<strong>用户对一个场景的认知（喜爱程度）是可以不依赖于用户在场景内的行为，而仅仅通 过用户对场景的访问频率即可反映出来。</strong></p>
<h2 id="熟客率定义">熟客率定义</h2>
<p>依据上文的基本假设，我们可以给出熟客率的定义：</p>
<p>$$
\text{熟客率} = \frac{\text{单位时间内内用户访问某场景的次数}}{\text{单位时间内用户访问手淘的次数}}
$$</p>
<p>上述数据中有一些技术细节实现起来较复杂（例如一次访问手淘会多次访问某场景；或场景的 PV 日志是仅仅是请求的日志，未必是真实访问等等），我们用了一个粗略的方法来近似的得到熟客率：</p>
<p>$$
\text{熟客率} = \frac{\text{某场景月访问天数}}{\text{手淘的月访问天数}}
$$</p>
<p>于是对每一个用户，都可以算出他最近一个月在各个场景的访问频度（熟客率）了。</p>
<p>对某一个特定的场景而言，我们想要了解的是类似于：“高频用户的占比”这样的指标，来了解用户对于一个场景的喜爱程度，所以我们按照用户熟客率统计用户的分布情况：</p>
<img alt="eh9us" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/ed36fef5db6aa9a3a0d485a801b9bba1.png"><p>除去搜索的图型比较特别外，其他的场景数据都很像一个指数分布，于是我们对用户分布情况取对数再来观察：</p>
<img alt="1de9k" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/4a9d5159812fb1bcb0dc7719467b5efb.png"><p>从这个图来看，基本符合我们的假设：用户分布情况是接近于指数分布的，但在访问频度为 1 的位置会产生很奇异的拐点，中间也有很多位置并不平滑。进一步分析发现，这样的现象主要是因为部分用户对手淘的访问天数过短，造成他们的访问信息并不具有有效性。为此，我们仅统计每月访问手淘 15 天以上的用户（用户量为：139523167，基本覆盖手淘的活跃用户）。</p>
<p>重新统计的上图如下：</p>
<img alt="r4srx" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/22e865edcd6e328d9e8dae49e470329f.png"><p>这一回，整个曲线都变得平滑了（仅仅是在 0.3~0.4 附近感觉有拐点）。根据这个图，我们增加如下的假设：对一个场景而言，有认知的用户的的分布模型是一个指数分布；高频访问的用户基本都是有认知的用户。</p>
<h2 id="场景指标的计算">场景指标的计算</h2>
<p>接上文，我们根据熟客率的指标可以用回归的方法统计出“有认知”的用户再场景内访问频率的分布情况：</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>斜率</th>
          <th>截距</th>
          <th>有认知的用户量</th>
          <th>认知用户平均熟客率</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>猜你喜欢</td>
          <td>1.7431016306</td>
          <td>16.357052585</td>
          <td>7285426</td>
          <td>57.37%</td>
      </tr>
      <tr>
          <td>有好货</td>
          <td>5.6656943459</td>
          <td>16.580370553</td>
          <td>2802272</td>
          <td>17.65%</td>
      </tr>
      <tr>
          <td>发现好店</td>
          <td>7.198381533</td>
          <td>17.412020797</td>
          <td>5066519</td>
          <td>13.89%</td>
      </tr>
      <tr>
          <td>她在买</td>
          <td>6.7192760078</td>
          <td>13.955514186</td>
          <td>171191</td>
          <td>14.88%</td>
      </tr>
      <tr>
          <td>女装</td>
          <td>7.8873829626</td>
          <td>16.842386083</td>
          <td>2615908</td>
          <td>12.68%</td>
      </tr>
      <tr>
          <td>男装</td>
          <td>9.3844322456</td>
          <td>14.08435403</td>
          <td>139428</td>
          <td>10.66%</td>
      </tr>
      <tr>
          <td>搜索</td>
          <td>4.7601938755</td>
          <td>19.56287163</td>
          <td>65829803</td>
          <td>21.01%</td>
      </tr>
  </tbody>
</table>
<p>其中的斜率就是指数分布的概率密度函数中的系数 $\lambda$，根据 $\lambda$ 可以算出数学期望（平均 熟客率），截距和 $\lambda$ 合在一起可以计算出指数分布的用户总量：</p>
<p>$$
f(x) = \lambda e^{-\lambda}
$$</p>
<p>从而就能得到上面表格中的数据指标。</p>
<p>我们再根据这个数据去反推一下没有场景认知的用户访问情况：</p>
<img alt="ppjon" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/d9bdc2064350252b725e10fef415af1d.png"><p>图形也是呈指数分布，但这里的降幅快了很多（而且相比于原有的数据，误差增加了不少）。还是通过取对数，再根据散点图来观察：</p>
<img alt="haxhn" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/bcd7581514347ac8c1f4f2237786d5e1.png"><p>通过这个图来观察的话，指数分布的趋势明显了很多。用类似的方法，我们可以得到：</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>斜率</th>
          <th>截距</th>
          <th>无认知的用户量</th>
          <th>无认知用户平均访问频率</th>
          <th>认知用户占比</th>
          <th>认知用户转化增益</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>猜你喜欢</td>
          <td>9.3894679073</td>
          <td>17.8609986824</td>
          <td>6085443</td>
          <td>10.65%</td>
          <td>54.49%</td>
          <td>5.4</td>
      </tr>
      <tr>
          <td>有好货</td>
          <td>19.6362279409</td>
          <td>19.0102797422</td>
          <td>9183358</td>
          <td>5.09%</td>
          <td>23.38%</td>
          <td>3.5</td>
      </tr>
      <tr>
          <td>发现好店</td>
          <td>18.7503695019</td>
          <td>18.2540393165</td>
          <td>4514594</td>
          <td>5.33%</td>
          <td>52.88%</td>
          <td>2.6</td>
      </tr>
      <tr>
          <td>她在买</td>
          <td>27.7360657413</td>
          <td>19.191585643</td>
          <td>7793901</td>
          <td>3.61%</td>
          <td>2.15%</td>
          <td>4.1</td>
      </tr>
      <tr>
          <td>女装</td>
          <td>61.7233508719</td>
          <td>21.8490324548</td>
          <td>49941863</td>
          <td>1.62%</td>
          <td>4.98%</td>
          <td>7.8</td>
      </tr>
      <tr>
          <td>男装</td>
          <td>22.816772562</td>
          <td>17.6569131267</td>
          <td>2041953</td>
          <td>4.38%</td>
          <td>6.39%</td>
          <td>2.4</td>
      </tr>
  </tbody>
</table>
<p>其中的认知用户转化增益是指一个用户从无认知转变为有认知的用户，其访问频率提升的倍数。当然，并不是无认知的用户一定可以转化为有认知的用户，以女装为例，部分无认知的用户（例如男性）就很难转化为有认知的用户。</p>
<p>注：这里的结论是，各个场景的用户分布都是两个指数分布的叠加，两个指数分布的参数不同。对于这个结论，我这边也通过其他的数据分析验证了这样的结果。另外对于这两个指数分布的参数求解问题，上文中的方法并不是最合适的方法，更合理的方式应该是用 EM 算法来反复迭代求解两个系数。本文的数据足以说明结论，所以没有进一步采用更精确的求解方法。</p>
<p>另注：上述表格中用户量仅仅是统计分析的趋势值，并不是真实值预估，因为对重要指标计算基本没有影响，所以我并没有很细致的做归一化等工作。这个绝对数值还不能用来作为场景间评估和考量标准。</p>
<h2 id="指标的可靠性分析">指标的可靠性分析</h2>
<h3 id="用户维度熟客率指标的可靠性">用户维度熟客率指标的可靠性</h3>
<p>因为场景下的用户分为：有认知的和无认知的两类，尤其是在熟客率较低的部分，两类用户不可分。为了便于分析，本节的讨论都仅针对有认知的用户进行分析，采样的方法：仅取熟客率大于 $0.5$ 的用户</p>
<p>用户的熟客率是用一个月分访问频率来描述的，这个指标在某一天的体现即：一部分用户访问了手淘的用户这一天访问某场景的比例。熟客率的稳定性即：高频访问的用户和低频访问的用户是明显可分的——通常情况下不会出现大量的高频用户和低频用户的访问比例发生反转</p>
<p>为验证这个结论我们用猜你喜欢的数据，统计了熟客率在 $0.5$<del>$0.6$ 之间的用户和熟客率 再 $0.6$</del>$0.7$ 之间的用户，他们再过去一个月中，每天访问概率的变化情况：</p>
<img alt="3esx3" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/577e89955a2765d9c6bbb38c526427af.png"><p>是跟次日回访率一致的——用户再某一天的访问概率跟入口情况和时间周期有密切的联系， 但依然可以非常明显的看出：不同熟客率区间的用户，他们的访问概率是很明显的不同的， 但趋势又非常的一致。这说明熟客率这个指标是一个很稳定的衡量用户对场景认知的指标。</p>
<h3 id="用户熟客率与客户价值之间的关联">用户熟客率与客户价值之间的关联</h3>
<p>用户价值是指用户在场景内的具体行为贡献，包括点击、购买、客单价等等。因为熟客率计算是完全不考虑用户在场景内的行为的，所以跟诸如：CTR，CVR 等指标没有直接关联。我们想统计一下熟客率是否跟用户价值是正相关的。为简化研究复杂度（或者说我偷了个懒），这里仅统计了点击率（uctr），来验证用户熟客率和客户价值之间的关系。</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>0%</th>
          <th>10%</th>
          <th>20%</th>
          <th>30%</th>
          <th>40%</th>
          <th>50%</th>
          <th>60%</th>
          <th>70%</th>
          <th>80%</th>
          <th>90%</th>
          <th>100.00%</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>有好货</td>
          <td>37.29%</td>
          <td>41.91%</td>
          <td>47.34%</td>
          <td>50.48%</td>
          <td>52.69%</td>
          <td>54.36%</td>
          <td>55.92%</td>
          <td>57.71%</td>
          <td>60.13%</td>
          <td>63.20%</td>
          <td>67.11%</td>
      </tr>
      <tr>
          <td>猜你喜欢</td>
          <td>57.78%</td>
          <td>64.47%</td>
          <td>69.98%</td>
          <td>74.35%</td>
          <td>78.15%</td>
          <td>81.56%</td>
          <td>84.59%</td>
          <td>87.41%</td>
          <td>90.20%</td>
          <td>93.33%</td>
          <td>95.60%</td>
      </tr>
  </tbody>
</table>
<img alt="pftq1" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/7c2e807c970ece961bffa00abcc0e33e.png"><p>如预期的，点击率同用户的熟客率很明显的正相关，即熟客率越高，点击率越高。有充分的理由相信，pctr，cvr， 客单价等指标也会有显著的相关性，但具体的结论还待进一步研究。</p>
<p>同时这个结论也反向反映了另一个结论：上文中场景认知用户的指数分布系数 λ 越小，场景内价值体现的越好（高熟客率的用户占比更多，从而价值指标的平均值更高）。用通俗的话来解释就是：我们应当培养更多的老用户，他们能贡献更大的价值（虽然看上去是很显然的结论，但数据可以模型化这个结论并进一步的量化带来的收益）。</p>
<h3 id="相关结果对场景优化方向的指导性">相关结果对场景优化方向的指导性</h3>
<p>我们反过来再分析一下场景下的“有认知用户平均熟客率（访问频度）”和“无认知用户平均熟客率”这两个指标。</p>
<p>无认知的用户更多的是通过偶然的点击或者首图个性化吸引到场景内的，好的首图个性化可以提升这部分用户的占比，同时也可以提升这部分用户的熟客率分布曲线，创造出更多的将无认知用户转化为有认知用户的可能性；同时，好的首图个性化也能提升有认知的用户的熟客率分布曲线，从而提升场景价值。</p>
<p>不同场景的“无认知用户平均熟客率”描述的是一个场景比较客观的，对新用户的友好程度——在手淘上位置隐藏越深，对新手而言越不友好；没有首图个性化对新手不友好；场景里用户心智不明确或者缺少新手引导等等，也是对新手的不友好。种种因素都会导致这个指标的高低；</p>
<p>认知用户占比，说明的是流量冗余的问题（上文中的认知用户占比的计算可能因为未归一化带来较大的误差，不能作为一个确定值来对待，但整体趋势问题不大，例如猜你喜欢的认知用户占比远高于女装）。如果认知用户占比很低，说明流量中大量的都是新用户或者无认知用户。这个指标也说明了另一个潜在的问题：我们现有统计用户留存的指标都没有考虑用户本身的属性，像女装这类场景的用户留存，可能更多的反映的是首图的引流效果，而并不是场景内的真实的用户留存能力。</p>
<p>对于占比很低的场景，需要的工作有三点：</p>
<ol>
<li>因种种原因决定了用户分布中大量的都是新用户，所以需要在产品设计上对新用户更友好；</li>
<li>提升场景内的用户留存能力（将新用户转化为老用户的能力）；</li>
<li>控制外部引流。现在场景的主要问题不是流量不足，而是场景本身兜不住这么大的流量。</li>
</ol>
<p>当然，最核心的，最能创造价值的，还是有认知用户的熟客率指标。不断提升有认知 用户的熟客率指标，是一个产品长久发展的核心！另外我们也可以根据现有的有认知用户， 观察一个场景究竟更能吸引怎样的用户，以此来决定产品定位的调整或者做定向的投放来引流。</p>
<h2 id="熟客率指标作为算法优化指标的尝试">熟客率指标作为算法优化指标的尝试</h2>
<p>以猜你喜欢数据为例，对两个不同的分桶统计了单日访问率指标和次日回访率（）曲线是两个桶之间的差值：</p>
<img alt="6dtdw" loading="lazy" src="https://oss.uglyboy.cn/image/2024/11/c1ab411d8d1424b4545620fe8d1db359.png"><p>因为我这边并不知道猜你喜欢上具体做得是什么实验，所以没有办法进行物理含义的分析。但是从图上还是可以看到，熟客率是一个可衡量的指标，能够对具体的算法实验进行效果评估。</p>
<p>另一方面，熟客率指标的整体趋势和回访率是比较接近的，但在一些特定的点上会有较大的差异性。带来这部分差异的，应该就是“有认知的用户”和“没有认知的用户”对待一些变化的不同反映。</p>
<p>关于熟客率作为算法指标的优化，这部分现在还不是很成熟，需要进一步的探究。</p>
<h2 id="展望">展望</h2>
<p>为了简化工作量，相关的统计和计算都做了不少的简化，也由此带来了一些系统误差。例如用访问天代替访问次，会在物理含义上出现较大的变化，也会带来数据分析上的一些问题（指数分布会限制在 $0$~$1$,$1$ 的地方在特定情况下会产生奇异点）。没有用 em 算法来计算，新用户的数据的可靠性也不能保证。所以后续希望能和 BI 的同学合作，把数据的准确性提升来。</p>
<p>另外前两天袁全给了一些指导性的建议，后续可以尝试用类似的方法考察用户在品牌（店铺）上的认知度，计算用户对商家而言的价值。</p>
<p>再者，通过 $\text{熟客率}&gt;0.5$ 的条件，可以圈定一个产品的高认知用户群，以此可以定向的分析用户图谱，做用户访谈等等，能更好的得到对产品而言更有价值的用户划分，以此来指导产品的后续改进方向。</p>
<p>另外能否产出算法可优化指标方面，可能还需要进一步的探索。个人预期是，至少找到一些明确的能提升用户熟客率的手段（也许会带来其他指标的下降），给算法优化背后的物理意义一些更明确的理论阐述</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
