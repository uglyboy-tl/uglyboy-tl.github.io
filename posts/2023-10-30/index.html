<!doctype html><html lang=zh-cn dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>大语言模型的计算能力 | 拾柒读库</title>
<meta name=keywords content="LLM"><meta name=description content="大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中 $Q$ 是"><meta name=author content="癸老师"><link rel=canonical href=https://blog.uglyboy.cn/posts/2023-10-30/><link crossorigin=anonymous href=/assets/css/stylesheet.0c4fd3725171366f335155acde6fb3c7b7042cd2fd075bebf5023d9b28a701b2.css integrity="sha256-DE/TclFxNm8zUVWs3m+zx7cELNL9B1vr9QI9myinAbI=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.uglyboy.cn/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.uglyboy.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.uglyboy.cn/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.uglyboy.cn/apple-touch-icon.png><link rel=mask-icon href=https://blog.uglyboy.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{enableExplorer:!1,skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/babel-polyfill/7.12.1/polyfill.min.js integrity="sha512-uzOpZ74myvXTYZ+mXUsPhDF+/iL/n32GDxdryI2SJronkEyKC8FBFRLiBQ7l7U/PTYebDbgTtbqTa6/vGtU23A==" crossorigin=anonymous referrerpolicy=no-referrer></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.min.js></script><link rel=stylesheet href=https://cdn.bootcdn.net/ajax/libs/lxgw-wenkai-webfont/1.6.0/style.min.css><style>body,code,section{font-family:lxgw wenkai,sans-serif}</style><meta property="og:title" content="大语言模型的计算能力"><meta property="og:description" content="大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中 $Q$ 是"><meta property="og:type" content="article"><meta property="og:url" content="https://blog.uglyboy.cn/posts/2023-10-30/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-30T07:50:00+08:00"><meta property="article:modified_time" content="2023-12-06T17:20:12+08:00"><meta property="og:site_name" content="拾柒读库"><meta name=twitter:card content="summary"><meta name=twitter:title content="大语言模型的计算能力"><meta name=twitter:description content="大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中 $Q$ 是"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"文章","item":"https://blog.uglyboy.cn/posts/"},{"@type":"ListItem","position":2,"name":"大语言模型的计算能力","item":"https://blog.uglyboy.cn/posts/2023-10-30/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"大语言模型的计算能力","name":"大语言模型的计算能力","description":"大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\\Sigma,\\delta,q_0,F$，其中 $Q$ 是","keywords":["LLM"],"articleBody":"大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\\Sigma,\\delta,q_0,F$，其中\n$Q$ 是一个有穷集合，称为状态集。 $\\Sigma$ 是一个有穷集合，称为字母表。 $\\delta:Q\\times\\Sigma_\\varepsilon\\rightarrow \\mathcal{P}(Q)$ 是转移函数。 $q_0\\in Q$ 是起始状态。 $F \\subseteq Q$ 是接受状态集。 大模型是 NFA 的证明 令 $q_0 =\\varepsilon$ 为初始状态，大语言模型的预测函数记为\n$$ \\phi:{s_0s_1s_2…s_{n-1}}\\rightarrow s_n,s_i \\in \\Sigma $$\n取 $\\delta$ 为：\n$$ \\begin{equation} \\delta(q,\\sigma) = \\left \\{ \\begin{array}{ll} q \\circ\\sigma \u0026 \\sigma \\neq \\varepsilon \\\\ q\\circ\\phi(q) \u0026 \\sigma = \\varepsilon \\end{array} \\right. \\end{equation} $$\n也就是将 $Q$ 设置为已经拥有的上文，连续预测下一个字符（若当前是输入过程，则只需要简单的叠加到状态集，不需要预测的过程）。这描述了大语言模型下的“Next Token Prediction” 范式。也就是说这个范式下的一切模型（无论是 Transformer 还是 其他的什么算子进行这种模式的预测），都跳不出这个基本的范式。\n即当前的大模型无论如何提升自己的能力，其计算能力也不过是一个有穷自动机。\n也就是说，类似于 $\\{0^n\\#1^n\\}$ 这个模式是无法被有穷自动机学习和预测出来的。换句话说，大模型的智能在这个例子上直接会被锁死，注定达不成所谓的**“AGI”**。\n以这个例子泛化来说，我们仅通过构造正负样本和机器学习做概率预测的方式，永远也无法对上面的模式做完美的判定。这个结论正是上面的推理想表达的意思。\n这件事可以拿 ChatGPT 来测试，对于 0#1，00#11，000#111，…，这个序列，让 ChatGPT 续写，它可以继续写下去且不出错（但这只是假象），而且也会明确的说出这个序列是 $\\{0^n\\#1^n\\}$ 这个模式的产物。但当你要求它输出 n=100 时的输出，或者你拿 n=100 时的输入让 ChatGPT 判定时，它就会出错了。\n直接得到的重要启示是：\n除了大模型，我们还需要新的范式来解决 AGI 问题。仅靠提升模型规模，注定有很多事情做不到。\n额外的说明 有穷自动机是做不出基础四则运算的计算器 1 的。这也意味着大模型的推理能力是不存在的。\n我们认为的推理能力，不过是在有限状态空间下的穷举，例如上文中的 $\\{0^n\\#1^n\\}$ 这个例子。更大的模型可以通过训练模拟出更长的匹配，但是从“压缩比”的角度看，终究是没有能掌握这个规律，而是通过空间换时间的方式将更多的答案在训练的过程中记住。\n所以可能又回到了最初的问题——大模型是不是必须要足够大？继续增加大模型的规模还可以进一步提升泛化性，在类似这样的原本有穷自动机解决不了的问题上缓存更多的答案，“假装”大模型是可以解决它的。但这不是我们想要的答案。\nAgent + LLM 可以成为完备图灵机 While 循环的图灵完备性 编程语言 WHILE 语义 (Semnatik):\n一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\\mathbb{N}^k\\rightarrow\\mathbb{N}$ 其他未定义的参数可以在程序里被调用,但是必须设定为 $0$ WHILE 程序的结果在结束结束后通过 $x_0$ 传达 对于一个 WHILE 程序,有三种运行模式: 变量赋值: $x_i=x_j+c,c\\in{0,1,−1}$ $P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$ WHILE $x_i \\neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0 定理：编程语言 WHILE 是图灵完备的\n证明: 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 2，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。\n需要注意的是，循环中 ${x_i}$ 的个数对其是否是图灵完备的有影响。具体来说，任意图灵机可以被拥有 $8$ 个变量的 WHILE 程序模拟计算。\n这里的大部分变量其实是用来操控 RAM 或者用来操控图灵机的。真实使用时，不需要这么多的掣肘。\nAgent 的基本范式 Agent 的基本范式恰好就是一个 While 程序，其 Python 描述如下：\n@dataclass class ReAct(ABC): thought: Optional[str] = None action: Optional[str] = None def __post_init__(self): self.obs = self.run() # 获取执行 Action 的结果 @abstractmethod def run(self) -\u003e str: \"\"\" 执行Action \"\"\" pass @classmethod @abstractmethod def parse(cls, text: str) -\u003e \"ReAct\": \"\"\" 从大模型返回的文本解析成 ReAct 的实例 \"\"\" pass @property @abstractmethod def done(self) -\u003e bool: \"\"\" 终止条件 \"\"\" pass @abstractmethod def __str__(self) -\u003e str: \"\"\" 从 ReAct 中抽取信息形成新的 Prompt \"\"\" pass act = ReAct() acts = [] while not act.done: acts.append(act) prompt = get_prompt(acts) act: ReAct = ReAct.parse(llm.call(prompt)) # 调用大模型，并将 response 解析成 ReAct 的实例 其中，存储和变量有两种选择：可以保存在函数 get_prompt 中（这意味着更多的人工控制设定），也可以保存在 ReAct 中（这意味着让大模型在上下文中自行决定保存哪些信息）。\n所以，Agent 的基本范式是图灵完备的。\n典型的几个 Agent 流程：\nReAct 获得反思推理能力 BabyAGI 基础的计划任务 Agent Reflexion 长期记忆和短期记忆（短期记忆就符合上述流程） AutoGPT 第一个全能 Agent 都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。\nAgent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 Prompt Engineering，不能自适应，不能进化，也没有利用上足够多的人类知识。\n突破的方向——可训练的 Agent 如果想获得更强的计算能力，需要提升的不仅仅是 LLM，而是结合了 Agent 后的整体系统。所以微调（fine tuning）和对齐（Alignment）更应该在整合了一个可学习的 Agent 之后进行。\n另外，基础的预训练模型或许并不需要特别的大（当然，越大性能越好的结论不变，但与其记更多的数据不如记更多的规律），而需要把更多的训练工作后置到集成了 Agent 之后进行，这样才有可能将有穷自动机无法识别的模式学习出来。\nAgent 的 While 程序模式，其实也恰好符合一个强化学习的学习过程，这里确实是可以做很多工作的。\n这是通往 AGI 之路吗 到今天为止，其实我们也没有一个关于智能的合理定义。\n学会了人的技能就算是智能了吗？会不会千百万年后的未来人回头看，会觉得人类太傻，并不具有智能呢？所以大模型学习人这件事是不是就是最好的选择？\n但至少今天人能够完成的一切，都没有可以超出图灵机范式的计算能力，所以图灵机的计算能力可以当作今天人类的极限。\nAGI 可以定义为:\n无需人类的介入，实现任意的图灵机能力。\n如果以这个定义来看，那么当下的 Agent + LLM 在理论上已经可以到达人类能够触达的一切天空了。\n有穷计算机无法模拟括号的匹配和乘除法的运算优先级。 ↩︎\n证明细节请看：·while循环，源自 Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen ↩︎\n","wordCount":"2307","inLanguage":"zh-cn","datePublished":"2023-10-30T07:50:00+08:00","dateModified":"2023-12-06T17:20:12+08:00","author":{"@type":"Person","name":"癸老师"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.uglyboy.cn/posts/2023-10-30/"},"publisher":{"@type":"Organization","name":"拾柒读库","logo":{"@type":"ImageObject","url":"https://blog.uglyboy.cn/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.uglyboy.cn/ accesskey=h title="拾柒读库 (Alt + H)">拾柒读库</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.uglyboy.cn/ title=主页><span>主页</span></a></li><li><a href=https://blog.uglyboy.cn/slides/ title=演示><span>演示</span></a></li><li><a href=https://blog.uglyboy.cn/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://blog.uglyboy.cn/tags/ title=标签><span>标签</span></a></li><li><a href=https://blog.uglyboy.cn/archives/ title=时间轴><span>时间轴</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.uglyboy.cn/>主页</a>&nbsp;»&nbsp;<a href=https://blog.uglyboy.cn/posts/>文章</a></div><h1 class=post-title>大语言模型的计算能力</h1><div class=post-meta><span title='2023-10-30 07:50:00 +0800 CST'>2023-10-30</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;癸老师</div></header><div class=post-content><h2 id=大模型是有穷自动机>大模型是有穷自动机<a hidden class=anchor aria-hidden=true href=#大模型是有穷自动机>#</a></h2><h3 id=非确定型有穷自动机nfa的定义>非确定型有穷自动机（NFA）的定义<a hidden class=anchor aria-hidden=true href=#非确定型有穷自动机nfa的定义>#</a></h3><p>非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中</p><ol><li>$Q$ 是一个有穷集合，称为<strong>状态集</strong>。</li><li>$\Sigma$ 是一个有穷集合，称为<strong>字母表</strong>。</li><li>$\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是<strong>转移函数</strong>。</li><li>$q_0\in Q$ 是<strong>起始状态</strong>。</li><li>$F \subseteq Q$ 是<strong>接受状态集</strong>。</li></ol><h3 id=大模型是-nfa-的证明>大模型是 NFA 的证明<a hidden class=anchor aria-hidden=true href=#大模型是-nfa-的证明>#</a></h3><p>令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为</p><p>$$
\phi:{s_0s_1s_2&mldr;s_{n-1}}\rightarrow s_n,s_i \in \Sigma
$$</p><p>取 $\delta$ 为：</p><p>$$
\begin{equation}
\delta(q,\sigma) = \left \{
\begin{array}{ll}
q \circ\sigma & \sigma \neq \varepsilon \\
q\circ\phi(q) & \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$</p><p>也就是将 $Q$ 设置为已经拥有的上文，连续预测下一个字符（若当前是输入过程，则只需要简单的叠加到状态集，不需要预测的过程）。这描述了大语言模型下的“<em>Next Token Prediction</em>” 范式。也就是说这个范式下的一切模型（无论是 Transformer 还是 其他的什么算子进行这种模式的预测），都跳不出这个基本的范式。</p><p>即当前的大模型无论如何提升自己的能力，其计算能力也不过是一个有穷自动机。</p><blockquote><p>也就是说，类似于 $\{0^n\#1^n\}$ 这个模式是无法被有穷自动机学习和预测出来的。换句话说，大模型的智能在这个例子上直接会被锁死，注定达不成所谓的**“AGI”**。</p><p>以这个例子泛化来说，我们仅通过构造正负样本和机器学习做概率预测的方式，永远也无法对上面的模式做完美的判定。这个结论正是上面的推理想表达的意思。</p><p>这件事可以拿 ChatGPT 来测试，对于 <code>0#1</code>，<code>00#11</code>，<code>000#111</code>，&mldr;，这个序列，让 ChatGPT 续写，它可以继续写下去且不出错（但这只是假象），而且也会明确的说出这个序列是 $\{0^n\#1^n\}$ 这个模式的产物。但当你要求它输出 n=100 时的输出，或者你拿 n=100 时的输入让 ChatGPT 判定时，它就会出错了。</p></blockquote><p>直接得到的重要启示是：</p><p>除了大模型，我们还需要新的范式来解决 <strong>AGI</strong> 问题。<strong>仅靠提升模型规模，注定有很多事情做不到</strong>。</p><h3 id=额外的说明>额外的说明<a hidden class=anchor aria-hidden=true href=#额外的说明>#</a></h3><p>有穷自动机是做不出基础四则运算的计算器 <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> 的。这也意味着大模型的推理能力是不存在的。</p><p>我们认为的推理能力，不过是在有限状态空间下的穷举，例如上文中的 $\{0^n\#1^n\}$ 这个例子。更大的模型可以通过训练模拟出更长的匹配，但是从“<strong>压缩比</strong>”的角度看，终究是没有能掌握这个规律，而是通过空间换时间的方式将更多的答案在训练的过程中记住。</p><p>所以可能又回到了最初的问题——大模型是不是必须要足够大？继续增加大模型的规模还可以进一步提升泛化性，在类似这样的原本有穷自动机解决不了的问题上缓存更多的答案，“<strong>假装</strong>”大模型是可以解决它的。但这不是我们想要的答案。</p><h2 id=agent--llm-可以成为完备图灵机>Agent + LLM 可以成为完备图灵机<a hidden class=anchor aria-hidden=true href=#agent--llm-可以成为完备图灵机>#</a></h2><h3 id=while-循环的图灵完备性>While 循环的图灵完备性<a hidden class=anchor aria-hidden=true href=#while-循环的图灵完备性>#</a></h3><p>编程语言 WHILE 语义 (Semnatik):</p><ul><li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li><li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li><li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li><li>对于一个 WHILE 程序,有三种运行模式:<ul><li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li><li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li><li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li></ul></li></ul><p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p><p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p><blockquote><p>需要注意的是，循环中 ${x_i}$ 的个数对其是否是图灵完备的有影响。具体来说，<strong>任意图灵机可以被拥有 $8$ 个变量的 WHILE 程序模拟计算</strong>。</p><p>这里的大部分变量其实是用来操控 RAM 或者用来操控图灵机的。真实使用时，不需要这么多的掣肘。</p></blockquote><h3 id=agent-的基本范式>Agent 的基本范式<a hidden class=anchor aria-hidden=true href=#agent-的基本范式>#</a></h3><p>Agent 的基本范式恰好就是一个 While 程序，其 <code>Python</code> 描述如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@dataclass</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ReAct</span>(ABC):
</span></span><span style=display:flex><span>    thought: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    action: Optional[str] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__post_init__</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>obs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>run() <span style=color:#75715e># 获取执行 Action 的结果</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run</span>(self) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        执行Action
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@classmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parse</span>(cls, text: str) <span style=color:#f92672>-&gt;</span> <span style=color:#e6db74>&#34;ReAct&#34;</span>:
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        从大模型返回的文本解析成 ReAct 的实例
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@property</span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>done</span>(self) <span style=color:#f92672>-&gt;</span> bool:
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        终止条件
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@abstractmethod</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __str__(self) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        从 ReAct 中抽取信息形成新的 Prompt
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>pass</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>act <span style=color:#f92672>=</span> ReAct()
</span></span><span style=display:flex><span>acts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> act<span style=color:#f92672>.</span>done:
</span></span><span style=display:flex><span>    acts<span style=color:#f92672>.</span>append(act)
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> get_prompt(acts)
</span></span><span style=display:flex><span>    act: ReAct <span style=color:#f92672>=</span> ReAct<span style=color:#f92672>.</span>parse(llm<span style=color:#f92672>.</span>call(prompt)) <span style=color:#75715e># 调用大模型，并将 response 解析成 ReAct 的实例</span>
</span></span></code></pre></div><p>其中，存储和变量有两种选择：可以保存在函数 <code>get_prompt</code> 中（这意味着更多的人工控制设定），也可以保存在 <code>ReAct</code> 中（这意味着让大模型在上下文中自行决定保存哪些信息）。</p><p>所以，<strong>Agent 的基本范式是图灵完备的</strong>。</p><p>典型的几个 Agent 流程：</p><ol><li><a href=http://arxiv.org/abs/2210.03629>ReAct</a> 获得反思推理能力</li><li><a href=https://github.com/yoheinakajima/babyagi>BabyAGI</a> 基础的计划任务 Agent</li><li><a href=http://arxiv.org/abs/2303.11366>Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li><li><a href=https://github.com/Significant-Gravitas/AutoGPT>AutoGPT</a> 第一个全能 Agent</li></ol><p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p><blockquote><p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p></blockquote><h3 id=突破的方向可训练的-agent>突破的方向——可训练的 Agent<a hidden class=anchor aria-hidden=true href=#突破的方向可训练的-agent>#</a></h3><p>如果想获得更强的计算能力，需要提升的不仅仅是 LLM，而是结合了 Agent 后的整体系统。所以微调（fine tuning）和对齐（Alignment）更应该在整合了一个可学习的 Agent 之后进行。</p><p>另外，基础的预训练模型或许并不需要特别的大（当然，越大性能越好的结论不变，但与其记更多的数据不如记更多的规律），而需要把更多的训练工作后置到集成了 Agent 之后进行，这样才有可能将有穷自动机无法识别的模式学习出来。</p><blockquote><p>Agent 的 While 程序模式，其实也恰好符合一个强化学习的学习过程，这里确实是可以做很多工作的。</p></blockquote><h2 id=这是通往-agi-之路吗>这是通往 AGI 之路吗<a hidden class=anchor aria-hidden=true href=#这是通往-agi-之路吗>#</a></h2><p>到今天为止，其实我们也没有一个关于智能的合理定义。</p><blockquote><p>学会了人的技能就算是智能了吗？会不会千百万年后的未来人回头看，会觉得人类太傻，并不具有智能呢？所以大模型学习人这件事是不是就是最好的选择？</p></blockquote><p>但至少今天人能够完成的一切，都没有可以超出图灵机范式的计算能力，所以图灵机的计算能力可以当作今天人类的极限。</p><p>AGI 可以定义为:</p><blockquote><p>无需人类的介入，实现任意的图灵机能力。</p></blockquote><p>如果以这个定义来看，那么当下的 Agent + LLM 在理论上已经可以到达人类能够触达的一切天空了。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>有穷计算机无法模拟括号的匹配和乘除法的运算优先级。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>证明细节请看：·<a href=https://zhuanlan.zhihu.com/p/343107128>while循环</a>，源自 <a href=https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf>Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.uglyboy.cn/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://blog.uglyboy.cn/posts/2023-11-09/><span class=title>« 上一页</span><br><span>大语言模型的数学理解</span>
</a><a class=next href=https://blog.uglyboy.cn/posts/2023-10-25/><span class=title>下一页 »</span><br><span>所谓“压缩即是智慧”毫无意义</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://blog.uglyboy.cn/>拾柒读库</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制代码";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制代码"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>