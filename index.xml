<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>拾柒读库</title>
    <link>https://blog.uglyboy.cn/</link>
    <description>Recent content on 拾柒读库</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 01 Dec 2023 08:56:12 +0800</lastBuildDate><atom:link href="https://blog.uglyboy.cn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>浮躁的时代</title>
      <link>https://blog.uglyboy.cn/posts/2023-12-01/</link>
      <pubDate>Fri, 01 Dec 2023 08:56:12 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2023-12-01/</guid>
      <description>浮躁的时代，浮躁的人们，浮躁的世界。</description>
      <content:encoded><![CDATA[<h2 id="从拼多多市值超过阿里说起">从拼多多市值超过阿里说起</h2>
<p>昨天，拼多多的市值超过阿里了。</p>
<p>随后，看到了诸多的自媒体和身边资本圈的朋友开始热议，然后是前同事们的讨论，然后是马老师讲话又炸出一波人跟着转发着各种评论。</p>
<p>大家都在认真的研究一个问题：<strong>拼多多做对了什么？</strong> 可是有一个前提却被大家选择性遗忘——<strong>拼多多真的做对了吗？</strong></p>
<p>当然，这个问题不是针对拼多多的，并不是想说这个企业有什么不对。我们类比一下，三年前阿里市值最辉煌的时候，似乎也没有人在批判阿里有什么不对？而今天评判拼多多的每一个“对”，其实都在反衬着三年前阿里的“错”。回顾三年前，大家觉得阿里做对了吗？为什么今天就会觉得拼多多对了？那三年后，又会如何评说拼多多今天的对错呢？</p>
<p>这是一个浮躁的时代——随波飘零的片片落叶，被浪花裹挟到高点时，就会被认为是对；反过来若恰好落到了波谷，则又会被认为是错。于是对错的标准也如同波浪般变幻莫测了。</p>
<p>身边总有些朋友在努力寻找对与错的规律，希望自己成为对的人，奈何苦寻不得。或许，根源不是他们不够努力不够聪颖，而是误信了对错的标准，失去了正确的方向。</p>
<h2 id="芒格去世了大家尊敬着他的长期主义却依然做着短视的判断">芒格去世了，大家尊敬着他的长期主义，却依然做着短视的判断</h2>
<p>也是在昨天，芒格去世了。</p>
<p>朋友圈里追捧着拼多多的朋友们也纷纷缅怀着芒格。讽刺的是，他们说：芒格教会了他们要坚持长期主义；他们又在宣扬着自己从事的长期事业——无一例外的都是当下最热闹的赛道：大模型、跨境电商、AGI 。。。</p>
<p>或许他们的观念中，未来是一成不变的，当下的热门可以持续一辈子；也或许他们的观念中，“长期”其实是短暂的，或许数年，或许更短，当热门变换时，就是下一个长期的到来。</p>
<h2 id="无一幸免">无一幸免</h2>
<p>这种浮躁，在整个世界中蔓延，无一幸免。</p>
<p>政治圈，中国会赢。不过是因为其他的人都太浮躁——我们可以愿意用 20 年改变什么事情，但是大部分的国家不愿意等，没有耐心等，甚至不相信等待的力量。</p>
<blockquote>
<p>芯片封锁了又能怎样？我们在芯片上再等 20 年又能怎样？这其实才是中国人真正的底气所在——我可以用 20 年的坚持去等待，去改变。而你们，不行！</p>
<p>“时间在我”，不是因为重要的节点临近，而是因为我们相信我们的坚持。时间永远会站在正确的方向上，短期的挫折，不过是时机未到罢了。</p>
</blockquote>
<p>科研圈，也充斥着急功近利的浮躁的人们——大模型领域的论文越来越像新闻 PR 稿件，唯恐标题不醒目，满篇 &ldquo;xxx is all you need&rdquo;。</p>
<p>娱乐圈。。。</p>
<p>或许体育圈反而成了最后的净土，最强的战士们依然是上个时代遗留下来的老将，他们用刻苦和努力告诉后辈们，流星确实可以闪亮一瞬，绚烂无比；<em>你可以选择成为流星，但我选择成为太阳</em>。</p>
<h2 id="像我这样的人">像我这样的人</h2>
<p>喜欢毛不易的《像我这样的人》，唱出浮躁的时代里，每个人的沉沦：</p>
<blockquote>
<p>像我这样优秀的人
本该灿烂过一生
怎么二十多年到头来
还在人海里浮沉</p>
<p>像我这样聪明的人
早就告别了单纯
怎么还是用了一段情
去换一身伤痕</p>
<p>像我这样迷茫的人
像我这样寻找的人
像我这样碌碌无为的人
你还见过多少人</p>
<p>像我这样庸俗的人
从不喜欢装深沉
怎么偶尔听到老歌时
忽然也晃了神
像我这样懦弱的人
凡事都要留几分
怎么曾经也会为了谁
想过奋不顾身</p>
<p>像我这样孤单的人
像我这样傻的人
像我这样不甘平凡的人
世界上有多少人</p>
</blockquote>
<p>身边很多的人，意气风发，拿着高薪，天天讲着行业里最热门的黑话，谈论着阿里衰落的必然性和拼多多的成功，努力在各种场合里抛头露脸，努力成为时代的“精英”，换取别人崇拜的目光。他们不是互联网上的网红，却努力活成了自己生活圈中的网红，小心经营着自己的人设，努力迎合着身边的一切“粉丝”的期盼。</p>
<p>可他们身上却似乎又都带着 BGM：“<em>像我这样迷茫的人/像我这样寻找的人/像我这样碌碌无为的人/你还见过多少人</em>”</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>大语言模型的数学理解</title>
      <link>https://blog.uglyboy.cn/posts/2023-11-09/</link>
      <pubDate>Thu, 09 Nov 2023 12:08:22 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2023-11-09/</guid>
      <description>大语言模型的基本逻辑 大语言模型的本质是一个 N-GRAM 模型，即： 定义： 假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率： $$ p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i}) $$ 该模型是一</description>
      <content:encoded><![CDATA[<h2 id="大语言模型的基本逻辑">大语言模型的基本逻辑</h2>
<p>大语言模型的本质是一个 <code>N-GRAM</code> 模型，即：</p>
<p><strong>定义：</strong></p>
<p>假设 $w_1, w_2,\dots, w_{N}$ 是一个单词序列。我们可以按如下公式计算单词序列的概率：</p>
<p>$$
p(w_{1},w_{2},\dots,w_{N})=\prod^N_{i=1}p(w_{i}|w_{1},w_{2},\dots,w_{i})
$$</p>
<p>该模型是一个 $N-1$ 阶的马尔可夫链，称为 <code>N-GRAM</code> 模型</p>
<p><strong>推论：</strong> 有限马尔可夫链（或 <code>N-GRAM</code> 模型）背后的「语法」是有穷自动机，也就是正则表达式。是 <code>乔姆斯基体系</code> 最底级的文法。</p>
<h3 id="agent--llm-可以成为完备图灵机">Agent + LLM 可以成为完备图灵机</h3>
<p>一般来说，希望将有穷自动机扩充成完备图灵机，朴素的想法就是添加外部存储，如 <a href="http://arxiv.org/abs/2301.04589">Schuurmans et al(2023)</a> 就证明了使用外部存储的大模型是图灵完备的。但这种图灵完备性的实现依然需要大量的人工介入。所以我们希望找到一种更加自然的，可以自我学习的具有图灵完备性的模式。</p>
<h4 id="while-循环的图灵完备性">While 循环的图灵完备性</h4>
<p>编程语言 WHILE 语义 (Semnatik):</p>
<ul>
<li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li>
<li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li>
<li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li>
<li>对于一个 WHILE 程序,有三种运行模式:
<ul>
<li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li>
<li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li>
<li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li>
</ul>
</li>
</ul>
<p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p>
<p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD, STORE, CLOAD, CADD, CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p>
<h4 id="agent-流程都是-while-循环">Agent 流程都是 While 循环</h4>
<p>典型的几个 Agent 流程：</p>
<ol>
<li><a href="http://arxiv.org/abs/2210.03629">ReAct</a> 获得反思推理能力</li>
<li><a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> 基础的计划任务 Agent</li>
<li><a href="http://arxiv.org/abs/2303.11366">Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> 第一个全能 Agent</li>
</ol>
<p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p>
<blockquote>
<p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p>
</blockquote>
<h2 id="大语言模型的泛化性">大语言模型的泛化性</h2>
<p>从机器学习的角度看，大语言模型是一个生成式模型——学习原始数据的概率分布。这里有一个基础的问题：用哪种机器学习的方法来学习这个生成式模型。</p>
<p>我们也逐步来分析这个问题，首先是传统统计学习的学习方法和深度神经网络之间的选择。从结果上看，我们选择了深度神经网络，因为我们不可能见过所有人说过的所有话，所以我们希望我们训练的模型在我们未见过的样本上也能取得很好的效果，这就是<strong>模型的泛化能力</strong>。而实验表明，深度神经网络的泛化性更好。为什么呢？</p>
<h3 id="过参数化是泛化性好的本质原因">过参数化是泛化性好的本质原因</h3>
<p>在传统统计学习中，我们希望使用的参数尽可能的少（奥卡姆剃刀原理），这样才能带来更好的泛化效果。另一方面，我们又希望我们的模型的表达能力尽可能的强，这样才能更好的拟合真实的概率空间。所以会有经典的微笑曲线：</p>
<p><img loading="lazy" src="https://s2.loli.net/2023/11/11/X9Q8xVuroRI3fzU.png" alt=""  />
</p>
<p>假设空间的大小不能太小，也不能太大（否则会过拟合）。</p>
<p>但当时的人们都没有尝试一件事情，就是如果进一步增大参数空间（已经发生过拟合之后），会发生什么？</p>
<p>下图是实际发生的事情：</p>
<p><img loading="lazy" src="https://s2.loli.net/2023/10/21/hIYNgQoBmRy32PH.png" alt=""  />
</p>
<p>随着参数空间的继续增大，泛化性又逐步的提升了，而且比过拟合之前的最优值还要好了。</p>
<p>这件事是深度学习拥有良好泛化性的本质原因——<strong>过参数化</strong>。如 <a href="http://arxiv.org/abs/1802.01396">Belkin et al(2018)</a> 中描述的，其实这种能力也并不是深度神经网络所独有的，而是一切过参数化的机器学习方法都能具备的性质。</p>
<p>深度神经网络一方面可以通过网络结构学习任意形状的可积函数的分布，另一方面，又可以通过过参数化获得良好的泛化性，于是就成为了真实世界大部分问题的最优机器学习方法——我们可以从猜测真实问题的函数结构中解脱出来，也不用担心样本量少无法遍历全部解空间。</p>
<h3 id="过参数化带来的思考">过参数化带来的思考</h3>
<p>过参数化的机器学习过程有无穷多最优解（训练数据上 Loss 为零），所以一定是一个非凸优化问题。但是不同的解对应的泛化性是不同的。而至今为止，我们也没有一个关于解的泛化性的指导性优化理论。所以深度学习能否获得良好的泛化性是一个随机事件。</p>
<p>但另一方面，从实践的角度我们能得到，深度学习获得良好泛化性又是一个大概率的事件。</p>
<p>结合深度学习中已经获得的大量实验结果，我们可以形成这样的物理认知：泛化性好的解空间应该是空间范围比较大（或者是梯度变化更平缓）的区域；而泛化性不好的空间则反之。从而自然会有结论：解落入更大空间的概率会更大，所以解能大概率是泛化性好的。</p>
<p>而基于上面这个未被证实认知也会带来一些推论：</p>
<ul>
<li><strong>收敛速度快的算法，可能其泛化性不如收敛速度慢的算法</strong>；</li>
<li><strong>增加收敛时的随机扰动可以提升泛化性</strong>；</li>
</ul>
<p>这些结论与已知的实验结果是相符的：Adam 收敛速度好于 SGD，但泛化性很多时候不如 SGD；而 SGD 的泛化性好于 GD。</p>
<p>以及，当下的一些研究，例如尝试将已经训练好的模型中的部分参数扣掉——“因为这些参数的变化不会影响训练集上的 Loss，或者我们已知的测试集上的 Loss”……这些尝试是危险的，很可能损失掉良好的泛化性。</p>
<p>过参数化的泛化性问题，现在还没有很好的数学解释，从而也没有合适的理论来衡量一个解的泛化性效果。一段时间之内，这个问题都会是大模型的“阿喀琉斯之踵”，考验大部分的深度学习优化算法——<strong>当你带来计算效率的提升时，是不是能确保泛化性不下降</strong>？</p>
<h2 id="大语言模型的-transformer-算子">大语言模型的 Transformer 算子</h2>
<p>当我们确定了使用 <code>N-GRAM</code> 作为语言模型，以及利用深度神经网络作为机器学习的方法，以获得模型良好的泛化能力。下一步就需要进一步研究模型更细节的结构上是否为大语言模型带来的新的能力，亦或者是限制了什么能力。</p>
<p>这里首先引入一个结论：</p>
<p>当前所有的深度学习中的算子，都可以展开成全链接网络。也就是说，当前的各种深度学习的算子，并不能获得全链接网络获得不了的能力。所以如果是作为基础能力的研究，例如“网络层深是如何带来更强的表达能力的”这种研究课题，是可以将任意算子都抽象成全链接网络来进行探索。这也是 <code>NTK</code> 理论的重要价值。</p>
<p>于是，各种具体算子带来的好处，是在于使用时效率的提升。这种提升等价于——给网络带来良好的先验知识。所以深度学习中的算子不存在优劣之分，只有不同的算子对于不同的数据，先验知识的匹配程度的差别。</p>
<p>所以下面我们即将讨论的 Transformer 算子，研究的重点是它带来了哪些先验知识（或者可以说它舍弃了哪些信息，而只关注哪些知识）。</p>
<h3 id="transformer-算子的位置编码">Transformer 算子的位置编码</h3>
<p><code>N-GRAM</code> 模型是时不变的，具体来说，就是一句话的分布，不会因为它前后位置的小变化而改变。例如一个文章中一句话前面多打了一两个空格，并不会影响将要说的这句话。</p>
<p>更具体来说，就是 <code>N-GRAM</code> 中的信息只与相对位置信息有关，而与绝对位置信息无关。基于这个信息，就可以优化全链接网络，设计出算子结构，使得其只与相对位置信息有关，而与绝对位置信息无关。</p>
<p>放到 Transformer 算子中来说，就是位置编码的设计应该满足：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,m−n)
$$</p>
<p>只与 $m-n$ 有关，而跟 $m,n$ 的具体数值无关。从这条性质就能比较容易地得到 <code>RoPE</code> 旋转位置编码。</p>
<h4 id="位置编码的内差">位置编码的内差</h4>
<p>大模型当前研究的重点之一是上下文窗口的大小，我们希望这个窗口可以进一步的扩大以捕获更多的输入信息。</p>
<p>但因为训练数据有限，以及模型本身需要有一个明确的形状，所以训练时的数据基本上还是要维持差不多在 4k 的水平上，但希望能对更长的文本进行预测。这时，从位置编码的性质来看，是与上下文窗口的大小无关的，所以是可以合理外推到无限大的。但是受限于训练样本的数量，当上下文窗口更大时，基本上还是只能有效捕获到训练窗口大小的信息，对更多的信息是无法利用的。</p>
<p>这时自然的想法时，如果我对信息内差（将更长的文本挤成短文本窗口大小的样子），就可以利用已经训练的信息来推测更多的信息了。</p>
<p>可以理解成，将位置编码设计成：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,\frac{m−n}{s})
$$</p>
<p>其中，$s$ 是窗口长度。这表达的是位置编码与相对位置的绝对大小无关，而只与相对位置的相对大小有关。</p>
<p>但是这样的问题是，在更常用的场景下，相对位置的绝对大小是更重要的，例如比较短的句子中，两个 token 究竟是相隔几个位置是十分重要的。这意味着，无法直接使用这样的位置编码获得任意的窗口能力。</p>
<p>所以，当下流行的位置编码内差的方法是：</p>
<ol>
<li>通过 <code>RoPE</code> 算法训练一个 $s$ 长的窗口</li>
<li>然后再用内差的办法，重新扩张了窗口大小，此时低频（长文本部分）通过内差获得了还不错的训练性能。但高频（短文本）部分却被严重破坏了。</li>
<li>此时重新微调模型，将高频部分调整到合适的位置，可以理解成只需要训练高频部分的信息（这部分信息其实也已经有了一些合理的先验知识了），所以可以更快的将短窗口扩展到长窗口。</li>
</ol>
<p><a href="https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/">NTK-aware Scaled RoPE</a> 方法，使用的是高频内推，低频外差的办法，做到了不需要额外训练即可扩大上下文窗口，即：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,l(m−n))
$$</p>
<p>其中：</p>
<p>$$
\begin{equation}
l(m-n) \approx
\left \{
\begin{array}{ll}
m-n &amp; \text{当}(m-n)\text{较小时} \\
\frac{m-n}{s} &amp; \text{当}(m-n)\text{较大时}
\end{array}
\right.
\end{equation}
$$</p>
<p>但这个方法还是会在很多位置上失去原本训练时学到的信息，使得外推时有性能损失。</p>
<p>类似的，其实我们还可以用这样的思考方式，重新设计位置编码，使得模型可以更好的利用现有的训练数据获得合理的外推能力。例如 <code>ReRoPE</code> 算法的位置编码的设计是这样的：</p>
<p>$$
⟨f(q,m),f(k,n)⟩=g(q,k,l(m−n))
$$</p>
<p>其中：</p>
<p>$$
\begin{equation}
l(m-n) =
\left \{
\begin{array}{ll}
m-n &amp; m-n &lt; s\\
s &amp; m-n \geq s
\end{array}
\right.
\end{equation}
$$</p>
<p>这是因为，训练样本中，我们从未见过 $m-n&gt;s$ 的样本，所以更长程的样本都用 $s$ 来替代，近似的获得信息的利用。类似这样的编码设计，可以保证训练集上无性能损失，并且具备了一定的扩展能力，最大化的利用了训练信息。</p>
<p>基于这个思想，还可以扩展出很多的位置编码的设计，尽最大可能性来挖掘训练样本中的信息。</p>
<h4 id="位置编码是否需要时间衰减">位置编码是否需要时间衰减？</h4>
<p>包括 <code>RoPE</code> 在内的各种位置编码，都增加了时间衰减的先验。而这部分信息其实是可以通过训练来学习到的。所以是否真的需要时间衰减这个先验信息，它是否能更有效的帮助我们训练？是一个值得研究和思考的问题。</p>
<h3 id="transformer-算子的信息编码">Transformer 算子的信息编码</h3>
<p>类似于上面位置编码的分析，我们知道分析算子的核心，是考虑它保留了什么信息（或者说舍弃了什么信息，是否有不应当舍弃的信息被舍弃了）。</p>
<blockquote>
<p>Transformer 信息编码的设计表达的是：某个位置所蕴含的信息，只与这个位置之前的所有文本间两两的相似度信息有关。</p>
</blockquote>
<p>其中极为重要的信息是如下公式：</p>
<p>$$
Attention(Query,Source) = \sum_{i=1}Similarity(Query,Key_i)*Value_i
$$</p>
<p>于是也可以将 Attention 机制看作一种软寻址（Soft Addressing）。</p>
<p>至于注意力模型中是否丢失了什么重要的信息？是否有更加合理的选择？是进一步分析 Transformer 算子的核心。但这一部分同样也没有什么更加基础的数学依据，所以就没有什么进一步讨论的余地了。</p>
<p>稍值得留意的是，具体的 $Similarity$ 算法的选取，还是可以从一切其他不变量中获得部分更加有意义的约束的。例如，<a href="https://spaces.ac.cn/archives/8823">从熵不变性看Attention的Scale操作</a>，还是可以从提升上下文窗口外推能力的角度，获得一个更有效的系数项。</p>
<h2 id="大语言模型的对齐">大语言模型的对齐</h2>
<p>这部分其实在数学上值得分析的内容不多，因为对齐的操作本质上是一个偏应用的操作，是让预训练模型更加符合人类的使用场景的操作。所以对齐之后，模型能力层面是没有本质提升的，更多的是在方便人类使用的层面获得了提升。这部分从应用和工程角度是需要而且极为重要的，但没有额外的数学信息。</p>
<p>其中只有一个话题值得探索，即为什么对齐的操作选择了强化学习而不是继续用传统的模式识别的方法训练？</p>
<blockquote>
<p>坊间的笑谈是，当时 OpenAI 负责对齐的团队恰好手边有现成的 RL 的算法，所以就用它搞出了 RLHF。</p>
</blockquote>
<p>网上关于这个问题有一些解释，大体上就是表达 RL 的调整效率是高于传统的模式识别的。这部分内容我还没有仔细的研究，就先不胡扯了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>证明细节请看：·<a href="https://zhuanlan.zhihu.com/p/343107128">while循环</a> ，源自 <a href="https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf">Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>大语言模型的计算能力</title>
      <link>https://blog.uglyboy.cn/posts/2023-10-30/</link>
      <pubDate>Mon, 30 Oct 2023 07:50:00 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2023-10-30/</guid>
      <description>大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中 $Q$ 是</description>
      <content:encoded><![CDATA[<h2 id="大模型是有穷自动机">大模型是有穷自动机</h2>
<h3 id="非确定型有穷自动机nfa的定义">非确定型有穷自动机（NFA）的定义</h3>
<p>非确定型有穷自动机是一个 5 元数组 $Q,\Sigma,\delta,q_0,F$，其中</p>
<ol>
<li>$Q$ 是一个有穷集合，称为<strong>状态集</strong>。</li>
<li>$\Sigma$ 是一个有穷集合，称为<strong>字母表</strong>。</li>
<li>$\delta:Q\times\Sigma_\varepsilon\rightarrow \mathcal{P}(Q)$ 是<strong>转移函数</strong>。</li>
<li>$q_0\in Q$ 是<strong>起始状态</strong>。</li>
<li>$F \subseteq Q$ 是<strong>接受状态集</strong>。</li>
</ol>
<h3 id="大模型是-nfa-的证明">大模型是 NFA 的证明</h3>
<p>令 $q_0 =\varepsilon$ 为初始状态，大语言模型的预测函数记为</p>
<p>$$
\phi:{s_0s_1s_2&hellip;s_{n-1}}\rightarrow s_n,s_i \in \Sigma
$$</p>
<p>取 $\delta$ 为：</p>
<p>$$
\begin{equation}
\delta(q,\sigma) = \left \{
\begin{array}{ll}
q \circ\sigma &amp; \sigma \neq \varepsilon \\
\phi(q) &amp; \sigma = \varepsilon
\end{array}
\right.
\end{equation}
$$</p>
<p>也就是将 $Q$ 设置为已经拥有的上文，连续预测下一个字符（若当前是输入过程，则只需要简单的叠加到状态集，不需要预测的过程）。这描述了大语言模型下的“<em>Next Token Prediction</em>” 范式。也就是说这个范式下的一切模型（无论是 Transformer 还是 其他的什么算子进行这种模式的预测），都跳不出这个基本的范式。</p>
<p>即当前的大模型无论如何提升自己的能力，其计算能力也不过是一个有穷自动机。</p>
<blockquote>
<p>也就是说，类似于 $\{0^n\#1^n\}$ 这个模式是无法被有穷自动机学习和预测出来的。换句话说，大模型的智能在这个例子上直接会被锁死，注定达不成所谓的**“AGI”**。</p>
<p>以这个例子泛化来说，我们仅通过构造正负样本和机器学习做概率预测的方式，永远也无法对上面的模式做完美的判定。这个结论正是上面的推理想表达的意思。</p>
<p>这件事可以拿 ChatGPT 来测试，对于 <code>0#1</code>，<code>00#11</code>，<code>000#111</code>，&hellip;，这个序列，让 ChatGPT 续写，它可以继续写下去且不出错（但这只是假象），而且也会明确的说出这个序列是 $\{0^n\#1^n\}$ 这个模式的产物。但当你要求它输出 n=100 时的输出，或者你拿 n=100 时的输入让 ChatGPT 判定时，它就会出错了。</p>
</blockquote>
<p>直接得到的重要启示是：</p>
<p>除了大模型，我们还需要新的范式来解决 <strong>AGI</strong> 问题。<strong>仅靠提升模型规模，注定有很多事情做不到</strong>。</p>
<h3 id="额外的说明">额外的说明</h3>
<p>有穷自动机是做不出基础四则运算的计算器 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 的。这也意味着大模型的推理能力是不存在的。</p>
<p>我们认为的推理能力，不过是在有限状态空间下的穷举，例如上文中的 $\{0^n\#1^n\}$ 这个例子。更大的模型可以通过训练模拟出更长的匹配，但是从“<strong>压缩比</strong>”的角度看，终究是没有能掌握这个规律，而是通过空间换时间的方式将更多的答案在训练的过程中记住。</p>
<p>所以可能又回到了最初的问题——大模型是不是必须要足够大？继续增加大模型的规模还可以进一步提升泛化性，在类似这样的原本有穷自动机解决不了的问题上缓存更多的答案，“<strong>假装</strong>”大模型是可以解决它的。但这不是我们想要的答案。</p>
<h2 id="agent--llm-可以成为完备图灵机">Agent + LLM 可以成为完备图灵机</h2>
<h3 id="while-循环的图灵完备性">While 循环的图灵完备性</h3>
<p>编程语言 WHILE 语义 (Semnatik):</p>
<ul>
<li>一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\mathbb{N}^k\rightarrow\mathbb{N}$</li>
<li>其他未定义的参数可以在程序里被调用,但是必须设定为 $0$</li>
<li>WHILE 程序的结果在结束结束后通过 $x_0$ 传达</li>
<li>对于一个 WHILE 程序,有三种运行模式:
<ul>
<li>变量赋值: $x_i=x_j+c,c\in{0,1,−1}$</li>
<li>$P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$</li>
<li>WHILE $x_i \neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0</li>
</ul>
</li>
</ul>
<p><strong>定理：编程语言 WHILE 是图灵完备的</strong></p>
<p><strong>证明:</strong> 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。</p>
<blockquote>
<p>需要注意的是，循环中 ${x_i}$ 的个数对其是否是图灵完备的有影响。具体来说，<strong>任意图灵机可以被拥有 $8$ 个变量的 WHILE 程序模拟计算</strong>。</p>
<p>这里的大部分变量其实是用来操控 RAM 或者用来操控图灵机的。真实使用时，不需要这么多的掣肘。</p>
</blockquote>
<h3 id="agent-的基本范式">Agent 的基本范式</h3>
<p>Agent 的基本范式恰好就是一个 While 程序，其 <code>Python</code> 描述如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ReAct</span>(ABC):
</span></span><span style="display:flex;"><span>    thought: Optional[str] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>    action: Optional[str] <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__post_init__</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>obs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>run() <span style="color:#75715e"># 获取执行 Action 的结果</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@abstractmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        执行Action
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@classmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@abstractmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse</span>(cls, text: str) <span style="color:#f92672">-&gt;</span> <span style="color:#e6db74">&#34;ReAct&#34;</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        从大模型返回的文本解析成 ReAct 的实例
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@property</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@abstractmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">done</span>(self) <span style="color:#f92672">-&gt;</span> bool:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        终止条件
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">@abstractmethod</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __str__(self) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        从 ReAct 中抽取信息形成新的 Prompt
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>act <span style="color:#f92672">=</span> ReAct()
</span></span><span style="display:flex;"><span>acts <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> act<span style="color:#f92672">.</span>done:
</span></span><span style="display:flex;"><span>    acts<span style="color:#f92672">.</span>append(act)
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> get_prompt(acts)
</span></span><span style="display:flex;"><span>    act: ReAct <span style="color:#f92672">=</span> ReAct<span style="color:#f92672">.</span>parse(llm<span style="color:#f92672">.</span>call(prompt)) <span style="color:#75715e"># 调用大模型，并将 response 解析成 ReAct 的实例</span>
</span></span></code></pre></div><p>其中，存储和变量有两种选择：可以保存在函数 <code>get_prompt</code> 中（这意味着更多的人工控制设定），也可以保存在 <code>ReAct</code> 中（这意味着让大模型在上下文中自行决定保存哪些信息）。</p>
<p>所以，<strong>Agent 的基本范式是图灵完备的</strong>。</p>
<p>典型的几个 Agent 流程：</p>
<ol>
<li><a href="http://arxiv.org/abs/2210.03629">ReAct</a> 获得反思推理能力</li>
<li><a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> 基础的计划任务 Agent</li>
<li><a href="http://arxiv.org/abs/2303.11366">Reflexion</a> 长期记忆和短期记忆（短期记忆就符合上述流程）</li>
<li><a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> 第一个全能 Agent</li>
</ol>
<p>都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。</p>
<blockquote>
<p>Agent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 <code>Prompt Engineering</code>，不能自适应，不能进化，也没有利用上足够多的人类知识。</p>
</blockquote>
<h3 id="突破的方向可训练的-agent">突破的方向——可训练的 Agent</h3>
<p>如果想获得更强的计算能力，需要提升的不仅仅是 LLM，而是结合了 Agent 后的整体系统。所以微调（fine tuning）和对齐（Alignment）更应该在整合了一个可学习的 Agent 之后进行。</p>
<p>另外，基础的预训练模型或许并不需要特别的大（当然，越大性能越好的结论不变，但与其记更多的数据不如记更多的规律），而需要把更多的训练工作后置到集成了 Agent 之后进行，这样才有可能将有穷自动机无法识别的模式学习出来。</p>
<blockquote>
<p>Agent 的 While 程序模式，其实也恰好符合一个强化学习的学习过程，这里确实是可以做很多工作的。</p>
</blockquote>
<h2 id="这是通往-agi-之路吗">这是通往 AGI 之路吗</h2>
<p>到今天为止，其实我们也没有一个关于智能的合理定义。</p>
<blockquote>
<p>学会了人的技能就算是智能了吗？会不会千百万年后的未来人回头看，会觉得人类太傻，并不具有智能呢？所以大模型学习人这件事是不是就是最好的选择？</p>
</blockquote>
<p>但至少今天人能够完成的一切，都没有可以超出图灵机范式的计算能力，所以图灵机的计算能力可以当作今天人类的极限。</p>
<p>AGI 可以定义为:</p>
<blockquote>
<p>无需人类的介入，实现任意的图灵机能力。</p>
</blockquote>
<p>如果以这个定义来看，那么当下的 Agent + LLM 在理论上已经可以到达人类能够触达的一切天空了。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>有穷计算机无法模拟括号的匹配和乘除法的运算优先级。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>证明细节请看：·<a href="https://zhuanlan.zhihu.com/p/343107128">while循环</a>，源自 <a href="https://algo.rwth-aachen.de/Lehre/WS1920/BuK/BuK/h10.pdf">Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>所谓“压缩即是智慧”毫无意义</title>
      <link>https://blog.uglyboy.cn/posts/2023-10-25/</link>
      <pubDate>Wed, 25 Oct 2023 10:57:47 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2023-10-25/</guid>
      <description>算数编码才是压缩的本质 一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 Compression for AGI - Jack Rae | Stanford MLSys #76 里面核心模式只有一个：</description>
      <content:encoded><![CDATA[<h2 id="算数编码才是压缩的本质">算数编码才是压缩的本质</h2>
<p>一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 <a href="https://www.youtube.com/watch?v=dO4TPJkeaaU&amp;t=247s">Compression for AGI - Jack Rae | Stanford MLSys #76</a></p>
<p>里面核心模式只有一个：</p>
<blockquote>
<ol>
<li>假定我有一个程序 f，我将 f 的代码传输给另一端；</li>
<li>我有一个序列需要传输，我通过 f 对逐个字符出现的概率进行了预测；</li>
<li>我根据算数编码，将结果编码后，传输给了另一端；</li>
<li>最后传输的信息量最小。</li>
</ol>
</blockquote>
<p><strong>这不过是算数编码的定义好不好！！！</strong> 哪里有什么神奇的地方。。。</p>
<p>如果非说细节，也不过就是说明了为什么不用传输参数，将大模型的训练跟编码合到了一起而已。这完全证明不了大模型为什么有效果，以及为什么更大的模型效果更好。说出来的道理仅仅是：<strong>概率预测得越准，使用算数编码的压缩率越高</strong>。这件事结合算数编码的定义，不就是显然的问题吗？</p>
<p>而且它原始的流程中，也没有能体现出“<em>Next Token Prediction</em>”的优越性和必要性。</p>
<ol>
<li>如果序列很小，那么压缩效率的核心是 f 的代码量。此时使用 <code>lambda:x=x</code> 达到的效果最好。</li>
<li>如果序列很大，那么传参也不会是压缩算法优劣的核心差别。那么其他模式训练出来的能对文本做良好概率预测的模型都可以达到好的压缩效果。</li>
<li>如果序列中等，我们需要的是是否存在一个方法，一次传输了多个算数编码和多个残差，能否通过这些信息还原出初始编码？针对这个问题，我们单独开一章来分析</li>
</ol>
<h2 id="是否只能用-ntp-做压缩">是否只能用 NTP 做压缩？</h2>
<p>由自然归纳法，如果一次传输两个编码和两个残差，能还原出原始信息，那么，一次传输 $n$ 个算数编码和 $n$ 个残差就一定可以还原出原始编码。</p>
<p>假设我们使用的算法的过程是先用除第一个字符以外的所有字符来预测第一个字符的概率，同时梯度下降；然后再用除第二个字符以外的其他字符预测第二个字符的概率，同时梯度下降。这样可以得到两个算数编码和两个残差，应该如何用这些信息还原初始的字符呢？</p>
<p>方法和不确定型自动机的原理类似，或者用更土的办法来理解算法：</p>
<blockquote>
<p>我们用词表中的所有字符，重试这个过程，看哪个字符可以匹配上。虽然计算效率相比原版的 $\mathcal{O}(1)$，这个方法的复杂度是 $\mathcal{O}(n^2)$，但至少从压缩率的角度来看，我们对算法的要求没有计算速度方面的考量，更不用提这个算法一定是可以被优化的。</p>
</blockquote>
<p>以此推广，也就是对于任何模式的文本预测算法，都可以用同样的方法进行信息解压缩。于是不同方法之间在压缩率方面的差距还是会回归到对概率预测的精度上。甚至理论上看，使用了更多上下文的算法，应当可以比只做 &ldquo;Next Token Prediction&rdquo; 的算法精度更高。</p>
<h3 id="其他的无效解读">其他的无效解读</h3>
<p>至于残差究竟是不是用信息熵，其实对这个压缩算法没有什么核心的影响，无论哪种残差该反向传播依旧按原本的方式传播，无所谓其物理意义。因为所有的意义都只体现在传递的残差能否还原原来的编码。残差能对应上什么物理意义的各种解释其实对压缩率和计算都没有帮助。</p>
<h2 id="结论">结论</h2>
<p>所以那个演讲其实不过是个披着数学魔术的神奇表演，本质上不过是说：大模型谁的性能好，谁就是更好的大模型——典型的废话文学新版本了。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Hugo 搭建流程</title>
      <link>https://blog.uglyboy.cn/posts/2023-10-18/</link>
      <pubDate>Wed, 18 Oct 2023 11:57:00 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2023-10-18/</guid>
      <description>基本安装 搭建站点 hugo new site &amp;lt;name of site&amp;gt; -f yml 初始化 Git 仓库 git init git branch -m main # 兼容 Github 的设置 安装主题 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 若是已经安装过主题的，需要下面的命令激活 git submodule update --init --recursive</description>
      <content:encoded><![CDATA[<h2 id="基本安装">基本安装</h2>
<h3 id="搭建站点">搭建站点</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo new site &lt;name of site&gt; -f yml
</span></span></code></pre></div><h3 id="初始化-git-仓库">初始化 Git 仓库</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git init
</span></span><span style="display:flex;"><span>git branch -m main <span style="color:#75715e"># 兼容 Github 的设置</span>
</span></span></code></pre></div><h3 id="安装主题">安装主题</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git submodule add --depth<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod
</span></span></code></pre></div><p>若是已经安装过主题的，需要下面的命令激活</p>
<pre tabindex="0"><code>git submodule update --init --recursive
</code></pre><h3 id="本地调试">本地调试</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo server
</span></span></code></pre></div><h3 id="添加新文章">添加新文章</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>hugo new posts/my-first-post.md
</span></span></code></pre></div><h2 id="配置">配置</h2>
<h3 id="配置-configyml">配置 <code>config.yml</code></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#f92672">baseURL</span>: <span style="color:#e6db74">&#34;https://examplesite.com/&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">languageCode</span>: <span style="color:#ae81ff">zh-cn</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">title</span>: <span style="color:#ae81ff">ExampleSite</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">theme</span>: <span style="color:#ae81ff">PaperMod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">timeZone</span>: <span style="color:#ae81ff">Asia/Shanghai</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">enableInlineShortcodes</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">enableGitInfo</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">enableRobotsTXT</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">enableEmoji</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">hasCJKLanguage</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">outputs</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">home</span>: [<span style="color:#ae81ff">HTML, RSS, JSON]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">Params</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">title</span>: <span style="color:#ae81ff">ExampleSite</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">description</span>: <span style="color:#e6db74">&#34;ExampleSite description&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">author</span>: <span style="color:#ae81ff">xxx</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">homeInfoParams</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">Title</span>: <span style="color:#ae81ff">Hi there wave</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">Content</span>: <span style="color:#ae81ff">Can be Info, links, about...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">socialIcons</span>: <span style="color:#75715e"># optional</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">rss</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">url</span>: <span style="color:#ae81ff">/index.xml</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ShowFullTextinRSS</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ShowReadingTime</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ShowCodeCopyButtons</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">DateFormat</span>: <span style="color:#e6db74">&#34;2006-01-02&#34;</span>  <span style="color:#75715e"># 日期格式化</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">menu</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">main</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">identifier</span>: <span style="color:#ae81ff">home</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">主页</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">url</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">identifier</span>: <span style="color:#ae81ff">search</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">搜索</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">url</span>: <span style="color:#ae81ff">/search</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">identifier</span>: <span style="color:#ae81ff">tags</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">标签</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">url</span>: <span style="color:#ae81ff">/tags</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">identifier</span>: <span style="color:#ae81ff">archives</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">时间轴</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">url</span>: <span style="color:#ae81ff">/archives</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">40</span>
</span></span></code></pre></div><h3 id="配置-contentarchivesmd">配置 <code>content/archives.md</code></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>title: &#34;时间轴&#34;
</span></span><span style="display:flex;"><span>layout: &#34;archives&#34;
</span></span><span style="display:flex;"><span>summary: archives
</span></span><span style="display:flex;"><span>---
</span></span></code></pre></div><h3 id="配置-contentsearchmd">配置 <code>content/search.md</code></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>title: &#34;搜索&#34; # in any language you want
</span></span><span style="display:flex;"><span>layout: &#34;search&#34; # is necessary
</span></span><span style="display:flex;"><span>summary: &#34;search&#34;
</span></span><span style="display:flex;"><span>placeholder: &#34;Typing something...&#34;
</span></span><span style="display:flex;"><span>---
</span></span></code></pre></div><h3 id="增加-latex-数学公式的支持">增加 Latex 数学公式的支持</h3>
<p>在 <code>layouts/partials</code> 路径下新建文件 <code>extend_head.html</code>：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-html" data-lang="html"><span style="display:flex;"><span>{{ if or .Params.math .Site.Params.math }} {{ partial &#34;math.html&#34; . }} {{ end }}
</span></span></code></pre></div><p>和 <code>math.html</code> 文件：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-html" data-lang="html"><span style="display:flex;"><span>&lt;<span style="color:#f92672">script</span>&gt;
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">MathJax</span> <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">tex</span><span style="color:#f92672">:</span> {
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">inlineMath</span><span style="color:#f92672">:</span> [
</span></span><span style="display:flex;"><span>        [<span style="color:#e6db74">&#34;$&#34;</span>, <span style="color:#e6db74">&#34;$&#34;</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#e6db74">&#34;\\(&#34;</span>, <span style="color:#e6db74">&#34;\\)&#34;</span>],
</span></span><span style="display:flex;"><span>      ],
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">displayMath</span><span style="color:#f92672">:</span> [
</span></span><span style="display:flex;"><span>        [<span style="color:#e6db74">&#34;$$&#34;</span>, <span style="color:#e6db74">&#34;$$&#34;</span>],
</span></span><span style="display:flex;"><span>        [<span style="color:#e6db74">&#34;\\[&#34;</span>, <span style="color:#e6db74">&#34;\\]&#34;</span>],
</span></span><span style="display:flex;"><span>      ],
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">processEscapes</span><span style="color:#f92672">:</span> <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">processEnvironments</span><span style="color:#f92672">:</span> <span style="color:#66d9ef">true</span>,
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">options</span><span style="color:#f92672">:</span> {
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">skipHtmlTags</span><span style="color:#f92672">:</span> [<span style="color:#e6db74">&#34;script&#34;</span>, <span style="color:#e6db74">&#34;noscript&#34;</span>, <span style="color:#e6db74">&#34;style&#34;</span>, <span style="color:#e6db74">&#34;textarea&#34;</span>, <span style="color:#e6db74">&#34;pre&#34;</span>],
</span></span><span style="display:flex;"><span>    },
</span></span><span style="display:flex;"><span>  };
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  window.<span style="color:#a6e22e">addEventListener</span>(<span style="color:#e6db74">&#34;load&#34;</span>, (<span style="color:#a6e22e">event</span>) =&gt; {
</span></span><span style="display:flex;"><span>    document.<span style="color:#a6e22e">querySelectorAll</span>(<span style="color:#e6db74">&#34;mjx-container&#34;</span>).<span style="color:#a6e22e">forEach</span>(<span style="color:#66d9ef">function</span> (<span style="color:#a6e22e">x</span>) {
</span></span><span style="display:flex;"><span>      <span style="color:#a6e22e">x</span>.<span style="color:#a6e22e">parentElement</span>.<span style="color:#a6e22e">classList</span> <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;has-jax&#34;</span>;
</span></span><span style="display:flex;"><span>    });
</span></span><span style="display:flex;"><span>  });
</span></span><span style="display:flex;"><span>&lt;/<span style="color:#f92672">script</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">script</span> <span style="color:#a6e22e">src</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;</span>&gt;&lt;/<span style="color:#f92672">script</span>&gt;
</span></span><span style="display:flex;"><span>&lt;<span style="color:#f92672">script</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">type</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;text/javascript&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">id</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;MathJax-script&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">async</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">src</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js&#34;</span>
</span></span><span style="display:flex;"><span>&gt;&lt;/<span style="color:#f92672">script</span>&gt;
</span></span></code></pre></div><h2 id="利用-github-actions-自动发布">利用 Github Actions 自动发布</h2>
<h3 id="编写-github-actions-脚本">编写 Github Actions 脚本</h3>
<p>在 <code>.github/workflows</code> 下新建文件 <code>build.yml</code>：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#75715e"># This is a basic workflow to help you get started with Actions</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">Auto Deploy Hugo</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Controls when the workflow will run</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">on</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Triggers the workflow on push or pull request events but only for the main branch</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">push</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">branches</span>: [ <span style="color:#ae81ff">main ]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">pull_request</span>:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># A workflow run is made up of one or more jobs that can run sequentially or in parallel</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">jobs</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># This workflow contains a single job called &#34;build&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">build</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The type of runner that the job will run on</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">runs-on</span>: <span style="color:#ae81ff">ubuntu-latest</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Steps represent a sequence of tasks that will be executed as part of the job</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">steps</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">actions/checkout@v2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">submodules</span>: <span style="color:#66d9ef">true</span>  <span style="color:#75715e"># Fetch Hugo themes (true OR recursive)</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">fetch-depth</span>: <span style="color:#ae81ff">0</span>    <span style="color:#75715e"># Fetch all history for .GitInfo and .Lastmod</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Setup Hugo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">peaceiris/actions-hugo@v2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">hugo-version</span>: <span style="color:#ae81ff">latest</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Build</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">run</span>: <span style="color:#ae81ff">hugo --minify</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Deploy</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">peaceiris/actions-gh-pages@v3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">if</span>: <span style="color:#ae81ff">${{ github.ref == &#39;refs/heads/main&#39; }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">deploy_key</span>: <span style="color:#ae81ff">${{ secrets.ACTIONS_DEPLOY_KEY }}</span> <span style="color:#75715e"># secret 中设置好私钥</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">external_repository</span>: <span style="color:#ae81ff">your-repo/your-repo.github.io </span> <span style="color:#75715e"># Page 仓库</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">publish_branch</span>: <span style="color:#ae81ff">main </span> <span style="color:#75715e"># Page 仓库的分支</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">publish_dir</span>: <span style="color:#ae81ff">./public</span> <span style="color:#75715e"># 静态网页路径</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">commit_message</span>: <span style="color:#ae81ff">${{ github.event.head_commit.message }}</span>
</span></span></code></pre></div><blockquote>
<p>记得在 Page 仓库的设置中开启 Github Pages，选择 <code>main</code> 分支，用你的仓库名替换 <code>your-repo/your-repo.github.io</code>。</p>
</blockquote>
<h3 id="生成私钥">生成私钥</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ssh-keygen -t rsa -b <span style="color:#ae81ff">4096</span> -C <span style="color:#e6db74">&#34;</span><span style="color:#66d9ef">$(</span>git config user.email<span style="color:#66d9ef">)</span><span style="color:#e6db74">&#34;</span> -f gh-pages -N <span style="color:#e6db74">&#34;&#34;</span>
</span></span></code></pre></div><p>你将得到两个文件：</p>
<ul>
<li><code>gh-pages.pub</code> 是 Public Key</li>
<li><code>gh-pages</code> 是 Private Key</li>
</ul>
<h3 id="在-github-中设置信息">在 Github 中设置信息</h3>
<ul>
<li>在本项目目录下设置 <strong>Sectets</strong> 的 <code>ACTIONS_DEPLOY_KEY</code> 信息，填入 Private Key</li>
<li>在 Pages 项目下设置 <strong>Deploy Keys</strong>，填入 Public Key，记得选中 <strong>Allow write access</strong></li>
</ul>
<table>
<thead>
<tr>
<th>添加 public key</th>
<th>Success</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/deploy-keys-1.jpg"><img loading="lazy" src="https://s2.loli.net/2023/10/19/PhTNCuG6okXRqaA.jpg" alt=""  />
</a></td>
<td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/deploy-keys-2.jpg"><img loading="lazy" src="https://s2.loli.net/2023/10/19/7b2PYUsfqCp6x5g.jpg" alt=""  />
</a></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>添加 private key</th>
<th>Success</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/secrets-1.jpg"><img loading="lazy" src="https://s2.loli.net/2023/10/19/bVgQDTKcXytfIGn.jpg" alt=""  />
</a></td>
<td><a href="https://github.com/peaceiris/actions-gh-pages/blob/main/images/secrets-2.jpg"><img loading="lazy" src="https://s2.loli.net/2023/10/19/WyYNH4azm3LOGwn.jpg" alt=""  />
</a></td>
</tr>
</tbody>
</table>
]]></content:encoded>
    </item>
    
    <item>
      <title>Scaling Law 的数学解读</title>
      <link>https://blog.uglyboy.cn/posts/2023-10-10/</link>
      <pubDate>Tue, 10 Oct 2023 11:50:00 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2023-10-10/</guid>
      <description>Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计： 模型下的理想真实世界的概率分布：$p(x|\theta)$ 我们不知道</description>
      <content:encoded><![CDATA[<h2 id="dataset-size-和-loss-的关系">Dataset Size 和 Loss 的关系</h2>
<h3 id="最大似然估计mle">最大似然估计（MLE）</h3>
<p>一切机器学习的本质都是最大似然估计：</p>
<ol>
<li>
<p>模型下的理想真实世界的概率分布：$p(x|\theta)$</p>
</li>
<li>
<p>我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\theta|x)$</p>
</li>
<li>
<p>现在 $x$ 已知，$\theta$ 未知，若对于两个参数 $\theta_1$ 和 $\theta_2$ 有</p>
<p>$$
L(\theta_1|x) = p(x|\theta_1) &gt; p(x|\theta_2) = L(\theta_2|x)
$$</p>
<p>那么意味着 $\theta=\theta_1$ 时，随机变量 $\theta_1$ 生成 $x$ 的概率大于当参数 $\theta=\theta_2$ 时。这也正是似然的意义所在，若观测数据为 $x$，那么 $\theta_1$ 是比 $\theta_2$ 更有可能为分布函数的参数。</p>
</li>
<li>
<p>在给定观测数据集 $X={x_n},n \in \mathbb{N}$ 时，真实世界最有可能的概率分布对应的参数 $\hat\theta$ 应该满足：</p>
<p>$$
L(\hat\theta|x) = p(x|\hat\theta) &gt; p(x|\theta) = L(\theta|x), \theta \in \mathbb{\Theta} 且 \theta \ne \hat\theta
$$</p>
<p>即：</p>
<p>$$
\hat\theta = \arg\max\limits_\theta L(\theta|x)
$$</p>
</li>
<li>
<p>求解最大似然函数：</p>
<p>$$
\frac{\mathrm{d}}{\mathrm{d}\theta} L(\theta|x) = 0
$$</p>
</li>
</ol>
<p>对这个方程数值求解的过程，对应的就是绝大部分机器学习算法中的梯度下降过程。</p>
<p>在测试集上评估的结果，我们预想的误差应当包含两部分：</p>
<ol>
<li>似然函数 $L(\theta|x)$ 对真实世界概率分布描述能力不足，带来的误差；</li>
<li>通过 $X$ 估计 $\theta$ 时，样本本身的误差；</li>
</ol>
<p>若假定我们可以通过梯度下降收敛（即上面最大似然函数的导数在 0 的一个很小的临域中），那么至少就是我们相信在观测数据集 $X$ 上，模型是正确的，那么评估的误差就更加明确的指向 $X$ 本身带来的误差。</p>
<h3 id="fisher-信息量">Fisher 信息量</h3>
<p>为了求解最大似然估计，我们常用的数值手段是：</p>
<p>假定观测数据集 $X$ 的真实世界概率对应的概率密度函数是 $f(x_i;\theta)$，定义似然函数：</p>
<p>$$
L(X;\theta) = \prod \limits^{n}_{i=1} f(x_i;\theta)
$$</p>
<p>求解时，先对 $L(X|\theta)$ 取对数，再求导，这个函数定义为 Score function：</p>
<p>$$
S(X;\theta) = \sum \limits^n_{i=1} \frac{\partial \ln f(x_i;\theta)}{\partial\theta}
$$</p>
<p>则 Fisher 信息量的定义就是这个 Score function 的二阶矩（second moment）</p>
<p>$$
I(\theta) = E[S(X;\theta)^2]
$$</p>
<p>Fisher 信息量最重要的意义是：通过中心极限定理，弱大数定律，依概率一致收敛，以及 Slutsky 定理，可以证明 MLE 的渐进分布是正态分布 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，即：</p>
<ol>
<li>$\hat \theta \stackrel{P}{\longrightarrow} \theta_0$，其中 $\theta_0$ 是参数的真实值；</li>
<li>$\sqrt{n}(\hat\theta - \theta_0) \stackrel{L}{\longrightarrow} N(0,I^{-1}(\theta))$ ;</li>
</ol>
<h3 id="数据量与误差的关系">数据量与误差的关系</h3>
<p>花了大量篇幅描述了 最大似然 和 Fisher 信息量后，最终真正值得我们关注的结论却异常的简单：</p>
<p>$$
L(D) \propto D^{-0.5}
$$</p>
<p>这个结论同计算均值时，数据样本带来的误差是完全一样的。</p>
<p>真实的机器学习条件下，我们的样本量的质量并不均匀，所以往往会优先使用更好的样本（小样本集不是大样本集的随机采样，而是精选），会导致观测数据集 $X$ 不能满足概率同分布，所以带来的结果是上述幂律关系中，实际的幂律值会小于 $0.5$。</p>
<p>理论上来说，如果我们能做到样本集随机采样，那样这个幂律就会更加接近 $0.5$，而如果样本集不能随机采样，某种意义上说，能否保持这种幂律关系是值得怀疑的。所以对于 OpenAI 和 Google 的 Scaling Law 的论文，在样本量同 Loss 的关系上，Google 的结果是更可信的。</p>
<p>哪怕依旧能维持幂律关系（维持幂律关系的数学基础是不存在的。。。），具体的数值也只能通过实际拟合来估计。因为这件事不是通用规律，只跟具体的训练数据集的分布有关，跟模型无关（前提条件是模型能在<strong>大数据</strong>下<strong>收敛</strong>，即满足大数定律、中心极限定律，并且模型可以拟合真实分布）。</p>
<h2 id="compute-和-loss-的关系">Compute 和 Loss 的关系</h2>
<h3 id="控制论和-pid-算法">控制论和 PID 算法</h3>
<p>梯度下降法的数值计算过程，某种视角下可以理解成就是控制论下的控制算法——我如何根据真实信息来控制我的预期值离目标值更近。</p>
<p>直观而好用的方法就是 PID 算法：</p>
<p>$$
u(t) = K_pe(t) + K_i\int^t_0 e(\tau)\mathrm{d}\tau + K_d\frac{\mathrm{d}e(t)}{\mathrm{d}t}
$$</p>
<p>当然，我们的梯度下降法原没有 PID 算法如此之精密，实际流程大概率只使用了 P 的部分，也就是对误差做补偿。在深度学习中，被称为反向传播。</p>
<h3 id="单参数计算量与误差的关系">单参数计算量与误差的关系</h3>
<p>单目标的 PID（只省 P 过程了）算法，误差与计算量（迭代次数）之间的关系：</p>
<p>$$
L(C) \propto K_p^{C}=e^{\lambda C}
$$</p>
<p>即，误差同计算量之间的关系是指数关系，不是幂律关系。</p>
<p>这一点在 <a href="https://arxiv.org/abs/2206.14486">Sorscher et al. (2022)</a> 中有所体现，它的结论是：至少对于某些任务，损失可以随着数据集 <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> 大小呈指数级增长，而不是作为幂律。</p>
<h3 id="总计算量与误差的关系">总计算量与误差的关系</h3>
<p>不同于优化问题中，我们会通过反复迭代的方式增加计算量，深度学习的计算量基本上是同模型规模和数据量正相关的。反过来意味着对单参数的优化迭代很少的固定步数就可以收敛，所以在通常数据量规模下，可以将单参数计算量带来的优化效果视作常数（都能优化到收敛）。</p>
<p>单参数计算量带来的优化效果视为常数（不会随计算量、节点数、数据量变化而变化），意味着计算本身同误差之间没有直接关联，总计算量与误差之间的关联体现的是数据量与误差的关系和节点数（结构）与误差的关系。</p>
<p>总计算量与数据量成正比，而数据量同优化效果之间的关联我们已经在前文完成了论述。下一步我们将分析节点数和误差之间的关系，或者其实更加精确的说，应当是在单参数误差不变的条件下，节点数的变化与总计算量之间的关系，是这个关系蕴含了总计算量与误差之间的关联。</p>
<h2 id="compute-和-parameters-的关系">Compute 和 Parameters 的关系</h2>
<h3 id="分形维度">分形维度</h3>
<p>具有自相似性的结构就是分形。而我们的深度学习计算就是典型的分形结构——当模型规模扩大时，主流的扩大的方式就是增加层数 <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，这带来的就是自相似性。</p>
<p>而自相似性带来的重要性质就是，系统会具有分形维度，分形维度会使得系统规模扩大时，对应的全局属性并不是等比增加，而是幂律增加，幂律的指数就是其分形维度。</p>
<blockquote>
<p>生物学中有重要的 $\frac{3}{4}$ 定律——生物随着重量的变大（原子数量的规模扩大），其相关的很多生物学特征，例如新陈代谢能力、血管长度、心跳、呼吸等等，并不与重量成正比，而是按照 $\frac{3}{4}$ 的幂律进行增长。
一个直观的理解，随着生物体长度增长，其体重会以幂律 $3$ 进行提升，而腿部的横截面则是幂律 $2$ 增长。所以生物的规模变大，就会带来腿部承受的压力不断变大，所以老鼠体型的动物的腿都很细，但大象规模的动物，腿都很粗；蚂蚁可以举起自身体重百倍的物品，但人只能举起和体重相仿的物品。这些都是因为规模变化带来的非线性，要求生物的动力学模型必须发生变化，而不能与小规模时一样。
类似的，在城市规模同城市中加油站、小超市、医院之类的城市核心建设之间，也存在着幂律增长的关系——相关幂律大约是 $0.85$。</p>
</blockquote>
<p>对应的，深度学习模型中，在保证单参数误差不变的条件下，Parameters 规模的增加所需要的 Compute 计算量的增加不是等比的，而是幂律的，而且这个幂律应当是小于 $1$ 的。</p>
<p>换句话说，计算量同损失之间的关系是伴生关系——计算量本身同损失是没有直接关联的。带来损失变化的根本原因不是计算不足，而是模型表达能力以及数据本身蕴含的信息带来的。</p>
<p>但因为这里的结论中，计算量与参数数量也是幂律关系，由前文，数据同损失也是幂律关系，如果参数数量同损失同样是幂律关系的化，那么计算量与损失也可以用幂律关系来表示。</p>
<h2 id="parameters-与-loss-之间的关系">Parameters 与 Loss 之间的关系</h2>
<p>这里要分析的是参数量增加为何能带来 Loss 的降低。这是因为 Parameters 的增加，可以提升模型的表达能力，可以更好的拟合目标函数。也就是说，一个模型距离真值的误差（Loss），除了因为 Dataset 自身的误差外，还有一部分是模型距离 Dataset 所描述的最大似然函数的误差。</p>
<p>这部分要是详尽分析起来会很复杂，幸好已经有一些这方面的研究：<a href="https://arxiv.org/abs/2004.10802">Sharma et al. (2020)</a> 和 <a href="https://arxiv.org/pdf/2102.06701.pdf">Bahri et al. (2021)</a> 都对这个问题进行了很好的分析，其结果也有对应的实验支撑。</p>
<blockquote>
<p>文章假定深度模型将数据映射到一个 $d$ 维数据流形上，增加的模型参数（无限数据的条件下）都会被模型用来将数据流形分割成更小的组件，然后模型将在数据流形的每个分量上进行独立的预测，以优化训练损失。</p>
<p>这样自然的，如果我们想让子区域的大小缩小 $2$ 倍，就需要增加 $2^d$ 倍的数据量或模型参数。进而就是直观的结论：</p>
<p>$$
L(P) \propto P^{-\frac{1}{d}}
$$</p>
<p>即 Loss 与 参数量之间是幂律关系，其幂律值小于 $1$（因为有 $d&gt;1$）。</p>
</blockquote>
<h2 id="总结">总结</h2>
<p>至此，关于 Scaling Law 的数学含义就已经基本都解释清楚了。</p>
<p>更重要的问题是，有了相关的理论支撑后，我们能做什么？哪些事情做不了。</p>
<h3 id="基于多份数据融合的实验结果预测">基于多份数据融合的实验结果预测</h3>
<p><strong>这件事是不可行的</strong>。</p>
<p>一切机器学习的基础都是最大似然估计，而最大似然估计的基础假设就是独立同分布。两组分布不同的数据融合，一定会破坏原有的分布，至于不同比例下融合后形成怎样的分布，具有怎样的特性，在两份数据的分布都已知的条件下，是可以计算的。但对于我们自己的机器学习任务，原本就是要去学习数据的分布，这就决定了，不可能在不了解数据分布的条件下，估计融合后的数据分布。</p>
<p>类似的，多分不同分布的数据集怎么融合能更贴近测试集也是不可知的，只能试出来。由于测试集也不是真值，甚至测试集对真实世界的表达很可能还不如训练集，所以针对测试集做针对性调优是不值得的。</p>
<p>这部分的定量分析，其实可以借鉴 OpenAI 关于 Scaling Laws 的经典文章 <a href="https://arxiv.org/abs/2001.08361">Kaplan et al. (2020)</a> 中尝试的方法：</p>
<blockquote>
<p>迁移学习与测试效果的提升：
当我们在与训练集分布不同的文本上评估模型时，其结果与训练验证集上的结果强烈相关，损失函数中有一个大致恒定的偏移量。换句话说，转移到不同的分布会带来一定的固定惩罚，但除此之外，其提升程度大致与训练集上的表现一致。</p>
</blockquote>
<p>可以用类似这样的方法，通过多份不同分布的测试集效果打分情况，评估模型表现。</p>
<p>当然，实操方面其实也不复杂，就是多看几个测试集的结果，记录下来。如果模型优化后，在各个测试集上的提升是基本一致的，那就说明这次改进不是因为数据分布变化带来的，而是因为模型能力带来的。</p>
<h3 id="判断最优的参数和模型数据量配比">判断最优的参数和模型数据量配比</h3>
<p>这件事不是特别值得做。因为我们当前模型的优质数据不够多。所以提供的数据质量是不稳定的。小模型上得到的预测数据值，在大模型上操作时，肯定不能按预测量来操作，而是还需要进一步增加数据量。但是具体增加多少，因为我们对数据质量无法在训练前得到评估，所以是不可预测的。</p>
<p>这件事值得做的条件是：我们已经用一份数据训练了一个很大的模型，然后我们可以通过抽样的方法构建小模型，用大模型预测小模型需要多少数据量，这件事是可行的。</p>
<p>当然，如果只是一个预估值做参考，这件事倒是可以做一下。</p>
<p>注：这件事值得做的数学理论基础是：我们需要找到样本的精度和模型训练的流型精度一致的对应比例。这件事的前提条件是：模型得到充分训练，且 Loss 与 样本、模型精度是同一个数量级（Loss 就是当前的精度）。如果这个精度不一致，loss 会被更大的精度所制约。带来的影响是会增加一定的无效计算量。</p>
<p>理论上，这件事更应该用适合的停机算法来避免冗余的计算，而不是需要精准的预估精度。</p>
<h3 id="尝试用更小的模型达到更优的效果">尝试用更小的模型达到更优的效果</h3>
<p>这件事价值不是特别大。</p>
<ol>
<li>不需要知道具体的比例，我们也知道，哪怕对于小模型，喂更多的数据可以达到更好的效果。</li>
<li>小模型的表达能力是有限的，所以也不是喂更多的数据就一定可以提升效果。</li>
</ol>
<p>于是哪怕做出了预估，也需要加好多限制条件，而实际应用场景也不多。</p>
<h3 id="其他">其他？</h3>
<p>昨天看完后，原本想说 Scaling Law 是个显然的结果，其规律并不蕴含更深层次的信息。但后来仔细想了想，可能还是有很多细节值得仔细的表述一下，以免遗漏什么可能性，所以写了这个文档。</p>
<p>总得来说，我对于 Scaling Law 并没有想到更深的应用场景，它所能表达的大概也只是：更多的数据、更大的模型（更多的模型参数）可以更好的拟合真实的概率分布。这件事对于机器学习来说，是自然的结论。这个规律几乎不涉及具体的模型形式——几乎只要是机器学习都符合这个规律。</p>
<p>所以从第一性原理角度出发，它算是一个数学上给出定性的存在性定理：我们的机器学习是可以不断优化的。但它不蕴含如何能更好地做优化的信息。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>这个定理的前置条件和证明过程这里就不赘述了，需要的话自己查一下。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>大部分的训练，增加迭代次数的方式都伴随着提供更多的训练样本，若模型距离收敛所需要的迭代次数比较多，例如如果 <code>学习律</code>（本质上就是 PID 中的 $K_p$）比较小，模型距离理论上限比较远，这时误差项主要不是来源于数据自身的误差，而是来自梯度下降逼近的误差，那么这个指数关系就会比较显著，对应的表象就是误差同数据量之间是指数关系。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>还可以增加每层的神经元数量（宽度），这种增加模式就不属于分层。类似于 Scaling Law 的规律，这种扩大的方式（形状变化）对于结果的影响不显著。当然，这件事是值得做实验，试一试少层数多神经元和多层数少神经元（参数总数一致）训练的结果是否一致。盲猜会有显著性能差异。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
]]></content:encoded>
    </item>
    
    <item>
      <title>Autorestic</title>
      <link>https://blog.uglyboy.cn/posts/autorestic/</link>
      <pubDate>Tue, 22 Feb 2022 06:44:00 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/autorestic/</guid>
      <description>完善的数据自动备份系统</description>
      <content:encoded><![CDATA[<blockquote>
<p>原本是打算不再在这里写技术类博客了，结果最近的一个小成果确实没啥合适的地方存放，所以还是留在这里吧。</p>
</blockquote>
<p>自从开始使用树莓派，忧虑的一个重要问题就是：万一我的硬盘坏了可怎么办？通常的方案是硬盘组磁盘列阵，例如 raid1，raid5，raid10。可惜这套方案价格太高，不符合我用树莓派做 NAS 的风格。所以还是追求一套通常且通用的数据备份方案。</p>
<p>其间尝试了一些手段，例如：rclone，Duplicati，rsync，自己写 shell 脚本等等，但是如何构建一个完善的数据备份方案还是比较复杂的，需要考虑包括备份的可靠性，备份文件的大小（冗余程度），备份的版本管理，按不同时效留档等很多的要素。</p>
<p>最终，终于遇到了 <a href="https://autorestic.vercel.app/">automation</a> ，并依此打造了一套完善的备份方案。</p>
<h2 id="备份方案的基本原则">备份方案的基本原则</h2>
<ul>
<li>备份数据要保证本地非源数据的硬盘保存一份，云端保存一份；</li>
<li>云端备份需要有数据加密机制；</li>
<li>备份数据需要有类似 git 的版本管理机制，保证冗余数据不被重复存储，且按版本标签可方便的管理；</li>
<li>云端保存需要支持各种云端数据源；</li>
<li>本地需要有旧版本文件清理机制；</li>
<li>可以对不同的数据源进行不同的备份机制设定；</li>
<li>自动化管理备份，无需过多的人工干预和介入；</li>
</ul>
<h2 id="数据备份方案的基本组件">数据备份方案的基本组件</h2>
<h3 id="rclone">rclone</h3>
<p>最早我是使用 rclone + shell 进行备份的，但是这只能解决云端备份和支持数据源的部分，而且设定异常的复杂。根本原因是在于，rclone 其实是一款同步数据应用，而不是数据备份应用。</p>
<p>但现在有了一个良好的开端：可以将数据同步到任何云端网盘中了。</p>
<h3 id="restichttpsresticnet"><a href="https://restic.net/">restic</a></h3>
<p>这是一款类似于 rclone 的软件，但是不同的是，restic 是专注于备份的软件，支持加密传输，增量备份，快照记录等等，而且还可以同 rclone 联动，利用 rclone 支持多种云端的能力，将数据备份到各种网盘中。</p>
<p>另外，restic 也可以非常便捷的还原任何一个版本的数据，总得来说，是一个很简单便捷的备份工具。但它是一个命令行工具，也就是说，并不是一个服务，无法提供自动备份的功能（定时备份），而且每一项操作都需要运行相关命令加参数。</p>
<p>如此一来，关于备份这件事，就只剩下自动化版本管理这个问题需要解决了。</p>
<h3 id="autorestichttpsautoresticvercelapp"><a href="https://autorestic.vercel.app/">autorestic</a></h3>
<p>autorestic 是 restic 的一个「包装器」，通过自动调用 restic 的方法，加上了配置文件、定时执行（伪）等功能。将命令行程序扩展成了一个基于固定配置可重复运行的应用。</p>
<p>相关的命令说明还是需要自己看一下官方的文档。</p>
<p>但 autorestic 依然是一个命令行，不是服务，虽然提供了配置文件的方式可重复操作，但依然无法实现定时自动备份功能</p>
<h3 id="crontab">crontab</h3>
<p>autorestic 的官方文档推荐的方式即配合 crontab 每 5 分钟执行一次的方式 将 autorestic 配置成一个伪服务，进而提供定时自动备份功能。</p>
<h3 id="docker">docker</h3>
<p>最后，终极的解决方案，是需要将这些工具组合起来，形成一套完整的工具链。于是将对应的工具打包进 docker image，就可以便捷的部署和使用对应的自动化备份方案了。</p>
<h2 id="最终的数据自动化备份解决方案">最终的数据自动化备份解决方案</h2>
<p>我通过 <a href="https://github.com/uglyboy-tl/autorestic-docker">Github</a> 的自动化流程，构建了实现上述 autorestic 服务的 <a href="https://hub.docker.com/r/guixi/autorestic">镜像</a> ，使用说明如下：</p>
<h3 id="features">Features</h3>
<p>Often it is usefully to trigger backups automatically. So in this image, it would be trigger the command every 5min.</p>
<h3 id="install">Install</h3>
<ol>
<li>Create an initial config file (<code>autorestic.yml</code>) such as:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#f92672">locations</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">my-location</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">from</span>: <span style="color:#ae81ff">/data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">to</span>: <span style="color:#ae81ff">my-backend</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cron</span>: <span style="color:#e6db74">&#39;0 3 * * 0&#39;</span> <span style="color:#75715e"># Every Sunday at 3:00</span>
</span></span></code></pre></div><p>You can read full <a href="https://autorestic.vercel.app/config">docs</a> to configure it.</p>
<ol start="2">
<li>Create an empty file (<code>autorestic.lock.yml</code>)</li>
<li>run [[docker-compose]] as below:</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">version</span>: <span style="color:#e6db74">&#34;3&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">services</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">autorestic</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">guixi/autorestic</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">container_name</span>: <span style="color:#ae81ff">autorestic</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">restart</span>: <span style="color:#ae81ff">unless-stopped</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">/etc/localtime:/etc/localtime:ro</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">$(pwd)/autorestic.yml:/root/.autorestic.yml:ro</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">$(pwd)/autorestic.lock.yml:/root/.autorestic.lock.yml</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro </span> <span style="color:#75715e">#optional</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">my-volume:/data</span>
</span></span></code></pre></div><h3 id="usage">Usage</h3>
<p>you can use autorestic to show all buckups such as</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker exec -it autorestic autorestic exec -av -- snapshots
</span></span></code></pre></div><p>and also use restic directly such as</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker exec -it autorestic restic
</span></span></code></pre></div><h3 id="license">License</h3>
<p><a href="https://github.com/uglyboy-tl/autorestic-docker/blob/main/LICENSE">MIT</a> © Uglyboy</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>庖丁解牛|究竟什么是“战略”？</title>
      <link>https://blog.uglyboy.cn/posts/2021-07-15/</link>
      <pubDate>Thu, 15 Jul 2021 08:00:00 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2021-07-15/</guid>
      <description>前几日，在朋友圈看到前同事分享某大厂高管在 XX 创业营上开了一课《如何制定战略》，内容里有“使命、愿景、价值观”、还有“一颗心，一张图，一场仗”</description>
      <content:encoded><![CDATA[<blockquote>
<p>前几日，在朋友圈看到前同事分享某大厂高管在 XX 创业营上开了一课《如何制定战略》，内容里有“使命、愿景、价值观”、还有“一颗心，一张图，一场仗”等行业黑话，而这两个话题凑一起，竟成了“从战略到执行的方法论”了。</p>
<p>什么是战略？战略到底是一门玄学还是一门科学？回想在老东家经历过的各个时期的“战略”，心有戚戚焉。</p>
<p>所以今天咱们就来硬核拆解一下：究竟什么是战略。</p>
</blockquote>
<h2 id="大而无当的战略">大而无当的“战略”</h2>
<p>先讲三个关于战略的小故事：</p>
<blockquote>
<p>当初老东家收编某视频巨头时，后者的市场份额跌至第三，于是许多本部的精兵强将前去支援。<!-- raw HTML omitted --></p>
</blockquote>
<p>当时便面临着一个战略问题：在不差钱的情况下，如何重新夺回市场份额第一的宝座？<!-- raw HTML omitted -->
就我参与阶段的了解，给出的答案是非常有该厂特色的解决方案：<!-- raw HTML omitted --></p>
<blockquote>
<ol>
<li>技术上提升用户体验，将 crash 率、秒播率、带宽压缩率等等各项指标都做到行业领先；</li>
<li>通过个性化算法优化匹配效率；<!-- raw HTML omitted --></li>
</ol>
<p>这个战略有一个宏大（但我记不住）的项目名称。一年之后，各项指标都在大家 996 的奋斗下达成了，项目也召开了一个隆重的表彰大会，获奖的团队笑脸盈盈。<!-- raw HTML omitted --></p>
<p>故事的结果大家都知道。<!-- raw HTML omitted -->
它的市场份额，相比于前两家竞对，又跌了不少。</p>
</blockquote>
<blockquote>
<p>之后不久，恰赶上了“猫狗大战”最激烈的那两年，于是老东家内部酝酿出一个核心战略，具体名称不重要，我姑且简单粗暴地称之为“打狗棒”。所有部门都在这个大战略下指定各自的战略，于是可以看到各个行业关于“打狗棒”的战略方案。<!-- raw HTML omitted --></p>
</blockquote>
<p>但令人意外的是，这个战略似乎只在 PPT 里彰显存在感。我们难以找到它在业务执行层面的体现。也就是说，有它没它，业务都在沿着既定的方向走。</p>
<blockquote>
<p>去年，跟快递公司有一些业务合作。某通的几个同学经常问我：菜鸟又出了什么什么战略，这个背后有什么思考？</p>
</blockquote>
<p>这个问题让我想起了刚到菜鸟时，跟菜鸟的同学聊天，大家都在感慨：每年都在谈战略，甚至一年一个战略，但真心没啥战略。</p>
<p>这三个小故事可以让我们对大厂“祛魅”：哪怕是大公司，哪怕是高管，也不一定能制定出正确的战略，甚至，以个人经验来看，他们中真的懂战略的，不多。</p>
<p>正因如此，“打工人”才常常有一种误解：一说“战略”，就是大而无当、虚头巴脑、不知所云。与其关心高层的“战略”，不如关心关心业务的 KPI，毕竟这才真正关乎自己的钱包。</p>
<blockquote>
<p>这里的三个故事，其实并不是想抨击大公司病或者揭露所谓的高管内幕。更明确地说，没有想表达对高管不敬的意味（认真脸）。<!-- raw HTML omitted --></p>
<p>高管的定义是高级管理者，是能够协同一个部门可能上百号人共同实现某个业务目标的领航人。但是，在战略这件事情上，执行能力强并不一定总能导向正确的方向，甚至会更容易形成偏见——我们擅长的部分就是最值得做的事情。</p>
</blockquote>
<p>这可能也是老人们时常会谈，“将才”和“帅才”的区别吧。</p>
<h2 id="战略是一门科学">战略是一门科学</h2>
<p>公司的战略，几乎都是领导们开会商量出来的。</p>
<p>是不是战略就应该由领导来决定呢？从权力的角度出发，是的。但从逻辑的角度来看，这样的战略决策会导致一个主观性极强的决定。就好像你爱吃甜，我爱吃辣一样，无所谓对错。</p>
<p>可是回到战略执行层面来看，这种做法会出现这样或那样的情况：执行后发现和想象的不一样只能重新规划的；换了领导于是调整战略的；也有执行了一阵子终于松口承认战略决策失误的……</p>
<p>如果战略就是这样一个既不可靠又不能延续的东西，那何必还要考虑什么“战略”呢？</p>
<blockquote>
<p>战略不是收入证明，不是只有高管或者大公司才配谈战略，也不存在说大公司的战略就一定是对的、好的，或者高管看问题就一定高瞻远瞩，高屋建瓴；战略也不是玄学，不应该有那么多的拍脑袋，“我觉得”，或者所谓的经验之谈。</p>
</blockquote>
<p>毛主席说：“战略问题是研究（战争）全局的规律性的东西。”战略应当是一门科学，它背后是有规律的，是可被描述和验证的。</p>
<p>在《营销的黑暗森林法则》一文中我做了一个铺垫：一定时期内，企业的竞争背后是一个零和游戏，企业发展是以抢占其他企业市场份额实现。</p>
<p>从而可以相对容易地理解战略的定义：<!-- raw HTML omitted -->
<strong>战略是指在相同投入的条件下，能够获得更大的市场份额的业务方向。</strong><!-- raw HTML omitted -->
这个定义和我们直觉上对战略的理解是基本一致的，但是，其中有几个很核心的要点，却往往会被人们所忽视。</p>
<h3 id="你搞清楚自己的定位没有">“你搞清楚自己的定位没有？”</h3>
<blockquote>
<p>（段子）“灵魂六问”：<!-- raw HTML omitted --></p>
</blockquote>
<p>配钥匙师傅：你配吗？<!-- raw HTML omitted -->
算命先生：你算什么东西？<!-- raw HTML omitted -->
食堂阿姨：你要饭吗？<!-- raw HTML omitted -->
快递小哥：你是什么东西？<!-- raw HTML omitted -->
上海垃圾分拣阿姨：你是什么垃圾？<!-- raw HTML omitted -->
最有文化的还是网约车司机：你搞清楚自己的定位没有？</p>
<p>关于战略，最核心的一个要点就是要看清自己所处的市场。不同的市场定位，战略打法会截然不同。</p>
<p>例如北京市和平里地区的餐饮市场，同全北京的餐饮市场，不同的内涵与外延形成的战略打法自然不同，最终达成的效果也会很不一样。</p>
<p>尤其是，不能自欺欺人地杜撰一个自以为是的市场。</p>
<blockquote>
<p>前段时间拜访了一家“无代码”的企业，企业的业务同学侃侃而谈“无代码”的美好前景，以及，“在‘无代码’的市场中，我们没有对手”。</p>
<p>可是，“无代码”这个概念自己就是一个市场吗？难不成发明一个不放鸡蛋的煎饼果子，就可以独霸“无鸡蛋煎饼果子市场”吗？哦，似乎说来也对，只是这样的“市场”很难说有什么价值。</p>
</blockquote>
<p>正经地说，市场需要一个科学的定义以便于我们理解：<strong>具有需求可替代性的产品的集合，才构成同一个市场。</strong></p>
<p>市场的范畴可小可大，例如火锅市场必然也可以延展到饮食市场。这个范畴的选择，需要根据企业自身的发展情况来抉择——不断成长的企业必然是不断扩大自己所在市场范畴的企业，让自己的产品可以在更大的范畴里解占据更多的市场份额，解决更多的用户需求。</p>
<h3 id="同样的资源为什么要给到你">“同样的资源，为什么要给到你？”</h3>
<p>战略的价值衡量不是简单地拿结果说话，而要看投入产出比。</p>
<p>曾经参加的很多项目决策会，项目方会卖力地说：“我的项目具有什么什么价值，只要资源到位就好”。但上位决策者考虑问题的方式是：相同的资源，给到这个项目还是其他项目，哪一个能够获得更大的价值。</p>
<p>有价值的事情很多，但不代表有价值就值得（当下）做。或者说，战略决策，某种意义上讲就是对各种选择进行优先级排序。</p>
<p>资源要优化配置的道理已是老生常谈，但放到自己身上，可能就成了“灯下黑”。</p>
<p>所以脱离投入产出比谈业务价值，就是耍流氓。</p>
<h3 id="大盘涨了你的市场份额涨了吗">“大盘涨了，你的市场份额涨了吗？”</h3>
<p>战略中值得在意的不是绝对价值，而是市场份额。</p>
<p>在一个不断发展的市场中，赚钱了，证明不了你的本事，也许你就是风口上的猪呢。至少你的增长速度要超过大盘的增长，才是真的增长。换句话说，就是需要有市场份额的提升。</p>
<p>所以谈战略的时候，份额才是真实的价值。</p>
<p>淘宝内部很多项目都要赶着双十一上线，接着双十一的流量大爆发，就可以容易地拿到一个很好看的数字。但是这样的结果能证明项目的价值吗？谈战略时，自欺欺人真心没什么意思。</p>
<p>尤为重要的一点是，企业自身当下的市场份额对如何选择战略方向起着决定性作用。</p>
<p>小企业自然需要立足于发展，而大企业则需要考虑扩张自己的势力范围。企业在不同阶段一定需要截然不同的战略选择。这也是很多成功的创业者，企业做大，就难以更进一步；而很多大厂的高管进入了初创企业也种种水土不服。</p>
<p>遗憾的是，很多优秀的企业都会将自己曾经的成功经验教条般地搬到新的阶段，造成战略方向的失误。这可能就是大部分企业最难的地方——让体验过成功的团队否定现状，这一定是很不愉快的感受。所以对未能跨过这一步的“成功过”的企业，只能对他们的曾经表示“respect”，然后摇一摇头，用前同事的话来美化一下这种失败——<strong>“优秀是卓越最大的敌人”</strong>。</p>
<h2 id="结语">结语</h2>
<p>战略是一门科学，而不是什么形而上的东西。所以战略不是高管或大厂的专属，而是值得每个人去思考和掌握的方法。</p>
<p>作为一门学科，给出“战略”的定义，正是开启科学发展方向的基础——有了定义，我们才知道战略不能空谈，<strong>必须要在看清自身所处的市场、自身的市场份额、自身的投入情况的条件下，去考虑各个业务方向的投入产出比</strong>。</p>
<p>这句话，也是今天想分享给大家的道理。</p>
<p>至于怎样制定战略？我还有一个关于“<strong>竞争策略</strong>”的研究，相对深入地分析了这个问题，以后有机会可以继续庖丁解牛。</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>黑暗森林法则和营销</title>
      <link>https://blog.uglyboy.cn/posts/2021-07-01/</link>
      <pubDate>Thu, 01 Jul 2021 08:00:00 +0800</pubDate>
      
      <guid>https://blog.uglyboy.cn/posts/2021-07-01/</guid>
      <description>宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须</description>
      <content:encoded><![CDATA[<blockquote>
<p>宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都小心翼翼……他必须小心，因为林中到处都有与他一样潜行的猎人。如果他发现了别的生命，不管是不是猎人，不管是天使还是魔鬼，不管是娇嫩的婴儿还是步履蹒跚的老人，能做的只有一件事：开枪消灭之！</p>
</blockquote>
<p>在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭。这就是宇宙文明的图景，这就是对费米悖论的解释。<!-- raw HTML omitted --></p>
<blockquote>
<p>——刘慈欣 《黑暗森林》</p>
</blockquote>
<h2 id="营销社会学">营销社会学</h2>
<p>《三体》是大部分技术人都喜欢的作品，至少也是技术人彰显自己文艺的重要共情素材——它用一种很理科的方式推演了一个理论：<strong>宇宙社会学</strong>，并以此延伸出了一个荡气回肠的故事。对于技术人（理科生）而言，这种快感大概也只有《生活大爆炸（The Big Bang Theory）》可以相提并论了。</p>
<blockquote>
<p>宇宙社会学基本公理：</p>
<ol>
<li>生存是文明的第一需求；</li>
<li>文明不断增长和扩张，但宇宙中物质总量基本保持不变；</li>
</ol>
</blockquote>
<p>从这两条基本的假设出发，加上 &ldquo;<strong>猜疑链</strong>&rdquo; 和 &ldquo;<strong>技术爆炸</strong>&rdquo; 的概念，就可以得到黑暗森林法则（即本文引言），进而构建出罗辑这样的执剑人角色，让明明处于绝对劣势的地球得以和三体星人平等共生 62 年。</p>
<p>有意思的是，出现在科幻小说中的宇宙社会学基本公理，正是从现实生活中归纳和升华出来的假设，放到熟悉的领域中，也依然有效：</p>
<blockquote>
<p>企业社会学基本公理：</p>
<ol>
<li>发展是企业的第一需求；</li>
<li>企业不断尝试增长和扩张，但全社会的总需求在一定时期内是基本保持不变的，或者说全社会可支配收入的总额是基本保持不变的；</li>
</ol>
</blockquote>
<p>直接的结论是：<a href="%E4%BA%A7%E8%83%BD%E4%B8%8D%E6%98%AF%E7%93%B6%E9%A2%88%EF%BC%8C%E4%BE%9B%E7%BB%99%E5%A4%A7%E4%BA%8E%E9%9C%80%E6%B1%82%E6%97%B6%EF%BC%8C%E8%BF%99%E6%98%AF%E5%BD%93%E4%B8%8B%E5%A4%A7%E9%83%A8%E5%88%86%E8%A1%8C%E4%B8%9A%E7%9A%84%E7%8E%B0%E7%8A%B6%E3%80%82" title="严谨地说，这种情况发生在">企业的发展是以抢占其他企业的市场份额实现的</a></p>
<p>而营销，是帮助企业抢占其他企业份额的具体手段的总称，所以当行业发展到一定阶段，营销就会成为企业间最激烈竞争的战场。</p>
<p>营销是如此的重要，以至于企业生产了一个产品后，愿意将其中的大部分利润让利给营销者，帮助自己抢占更大的市场份额。从而出现了大量的形形色色的营销型企业，包括：品牌商、代理商、渠道商、电商平台、直播带货、广告行业、门店销售等等。</p>
<p>而这些营销型的企业也依然适用上面的企业社会学基本公理，这里将之单独提炼成<strong>营销社会学基本公理</strong>：</p>
<blockquote>
<p>营销社会学基本公理:</p>
<ol>
<li>赚取更多的营销费用是营销型企业的第一需求；</li>
<li>在一定时期内，社会的总营销费用是保持不变的；</li>
</ol>
</blockquote>
<p>有了营销社会学的基本公理，很容易就能理解这样的推论：<strong>所有营销型企业之间都是互相竞争和取代的关系；营销型企业之间比较的是（一定时期内）帮助生产型企业抢占更大市场份额的能力</strong>。</p>
<h2 id="最好的时代--最坏的时代">最好的时代  最坏的时代</h2>
<p>通常技术的发展都是渐进式的，甚至哪怕是《三体》中在一个比较长的时间周期里，可能出现的“<strong>技术爆炸</strong>”，描述的也是技术线性推进过程中，在时间轴上的爆发。</p>
<p>或者说，技术的发展大体上都是有脉络的，是旧的技术不断积累和沉淀，为新的技术打下的良好的地基。</p>
<blockquote>
<p>关于技术如何演进这件事，推荐一本书——来自圣塔菲研究所的布莱恩·阿瑟撰写的《技术的本质》</p>
</blockquote>
<p>技术的渐进式发展，直接影响生产型企业也呈现渐进式发展——生产效率总是以一种相对稳定的方式进行提升，所以大多数情况下，同一时期内一个行业里的生产效率总是相近的，生产型企业之间的能力差异更多的是在资本层面（规模型经济）、管理层面，或者营销层面。</p>
<p>不过，这个规律放到营销这件事情上则大为不同了。</p>
<p>是的，营销的发展并不是营销手段的渐进提升，而是近乎完全不同的营销手段之间的更替的过程，这里把这种趋势称为“<strong>突变演进</strong>”。</p>
<p>例如，营销的重心从最早的报纸广告，快速演进到电视广告、综艺植入，再到互联网广告、新媒体营销；在渠道的营销层面，则是从层层代理商，到大型连锁超市，再到依赖电商平台，以及最近两年正大火的直播电商。</p>
<p>虽然营销领域在“突变演进”下也有一定的积累沉淀，而且在“突变”的间隔里，营销型企业也是以经验的积累努力提升营销效率，但纵观全局，起主导作用的营销手段依然都是“突变演进”的方式进化的——新的营销方式总能对旧有的营销方式形成绝对的优势。</p>
<p>幸或不幸，媒体行业、广告行业以及大部分的互联网企业，都属于营销型企业（赚的是生产型企业的营销费用）。所以这些行业里“眼见他起高楼，眼见他宴宾客，眼见他楼塌了”，都是时代的眼泪。纸媒转型成了新媒体，传统广告让位于互联网广告；互联网领域，谷歌百度一时风头正盛，到了移动互联网时代又让位给了 Facebook、微信；亚马逊和淘宝看似屹立不倒，抢的是线下传统渠道的营销费用；而短视频的崛起，让曾经的任何巨头都不敢轻视……这样的故事仍在这些领域中续写着。</p>
<p>这是最好的时代，也是最坏的时代。</p>
<p>营销的“突变演进”，带来了传统行业突变的可能性。一个企业（生产型）被另一个企业打败，可能不是因为它的技术能力不足，而是没能跟进到最新最“先进”的营销手段，导致不小心在历史舞台上落幕。</p>
<blockquote>
<p>我自己非常有感触的一件事情：我心中洽洽瓜子一直是第一名的休闲零嘴，也是大学期间常年的必备零食。但最近几年，要么是买了各路的网红零食，什么三只松鼠、良品铺子，要么是逛了周边的网红蛋糕店，买脏脏包、奶茶，甚至在网易严选之类的平台上屯糕点零食啥的，竟然都没机会想起来买一袋洽洽瓜子。</p>
</blockquote>
<p>打败恰恰瓜子的未必是更好吃的瓜子，或者更好吃的零食，只不过在流淌的时间里，不经意间被人遗忘，再回想起来时，已经成为了情怀。就好像儿时的北冰洋、熊猫雪糕、大大泡泡糖。</p>
<p>在这样的时代里，我们可以亲眼见证：两岁的完美日记打败了 112 年的巴黎欧莱雅和 73 年的雅诗兰黛；五岁的元气森林超过了不可一世的可口可乐和百事可乐；比旺旺年轻十几岁的三只松鼠线上市场占有率为 11.2%，而旺旺则不足 1%……</p>
<h2 id="营销的黑暗森林法则">营销的黑暗森林法则</h2>
<p>营销型企业唯一的价值在于其营销水平能帮助客户（生产型企业）比其他的营销者抢占到更多的市场份额，不论手段。</p>
<p>曾经的各地晚报上的广告段子再火，也挡不住纸媒的衰落；一个广告代理商哪怕把百度凤巢、淘宝直通车等优化到极点，但直播带货兴起之时，企业还是会把营销费用都网红，转向直播带货，减少广告平台的投入。</p>
<p>那直播带货会是终局吗？我们可以笑一笑，五年之后再来看这个话题。</p>
<p>营销世界的黑暗森林法则就是：<strong>没有“正确”的营销，只有更好的营销</strong>。</p>
<p>曾几何时，当大家拿着营销学的红宝书——科特勒的《营销管理》，学习宝洁的品牌战略，搭建和完善线下的供应商体系时，悄然崛起的并不是另一个宝洁，而是跟着电商平台一起成长起来的阿芙精油、韩都衣舍、三只松鼠、御泥坊、芳草集。</p>
<p>它们没有做曾经认为最正确的营销：打传统媒体广告、树立传统的品牌战略、构建完整的供应商体系，但相比传统营销，线上营销玩法是更好的营销，所以它们最终获得了更多的市场份额。</p>
<p>近些年，线上营销成为了主流，哪怕是传统的品牌商也都纷纷转型线上，都知道要新媒体营销， <a href="%EF%BC%88%E5%BE%AE%E4%BF%A1%E3%80%81%E5%BE%AE%E5%8D%9A%E3%80%81%E5%BE%AE%E6%B7%98%EF%BC%8C%E4%BB%A5%E5%8F%8A%E6%8A%96%E9%9F%B3">三微一抖</a> 。 &quot; 三微一抖是指 &ldquo;)，烧车钻展等等，但反倒是热闹的搞起线下营销的喜茶、瑞幸、泡泡玛特们脱颖而出。</p>
<p>甚至还有像拼多多这种，硬生生从不能做电商的社交流量上，搞出了一套“社交电商”的玩法，成为国内电商的第三极；vivo 和 oppo 则重金砸各类网络热门综艺节目，以异军突起之姿，顺利替代了“中华酷联”，已经将苹果挤出中国手机市场的前三；而已经有些由盛转衰的海底捞，则是靠着极致的服务成为餐饮行业的龙头。</p>
<p>这里似乎总在发生着“不讲武德”“乱拳打死老师傅”的故事——品牌们不停地学习新的营销玩法，但还是无可避免地被更新的一拨人挥舞着你没见过也看不懂的武器伤害着。</p>
<p>营销世界（营销型企业间的竞争）里，大厂们也都在焦虑着：因为短视频，腾讯在焦虑；因为拼多多，淘宝在焦虑；百度倒是可以佛了，正在焦虑的已经换成了今日头条和抖音。年轻的美团和拼多多也在焦虑，他们都急火火地进军了社区团购业务——他们是看明白了社区团购将是下一代的营销？未必。但是万一是呢？</p>
<p>年长者不得不杞人忧天，年轻者则做着异想天开一夜成名的梦。所以在营销的世界里，和《三体》中一片寂静的黑暗森林不同，这里的每个参与者都是一个发光体，并试图闪耀出更耀眼的光芒，沟通一片异常耀眼的黑暗森林，而且还将继续闪耀下去。</p>
<p>最后，也送给每个想在这片森林里闪耀的读者朋友一个问题：<strong>你正在为之奋斗的，是“正确”的营销，还是“更好”的营销</strong>？</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
