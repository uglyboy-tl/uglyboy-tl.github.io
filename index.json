[{"content":"大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\\Sigma,\\delta,q_0,F$，其中\n$Q$ 是一个有穷集合，称为状态集。 $\\Sigma$ 是一个有穷集合，称为字母表。 $\\delta:Q\\times\\Sigma_\\varepsilon\\rightarrow \\mathcal{P}(Q)$ 是转移函数。 $q_0\\in Q$ 是起始状态。 $F \\subseteq Q$ 是接受状态集。 大模型是 NFA 的证明 令 $q_0 =\\varepsilon$ 为初始状态，大语言模型的预测函数记为 $$ \\phi:{s_0s_1s_2\u0026hellip;s_{n-1}}\\rightarrow s_n,s_i \\in \\Sigma $$ 取 $\\delta$ 为： $$ \\begin{equation} \\delta(q,\\sigma) = \\left { \\begin{array}{ll} q \\circ\\sigma \u0026amp; \\sigma \\neq \\varepsilon\\ \\phi(q) \u0026amp; \\sigma = \\varepsilon \\end{array} \\right. \\end{equation} $$ 也就是将 $Q$ 设置为已经拥有的上文，连续预测下一个字符（若当前是输入过程，则只需要简单的叠加到状态集，不需要预测的过程）。这描述了大语言模型下的“Next Token Prediction” 范式。也就是说这个范式下的一切模型（无论是 Transformer 还是 其他的什么算子进行这种模式的预测），都跳不出这个基本的范式。\n即当前的大模型无论如何提升自己的能力，其计算能力也不过是一个有穷自动机。\n也就是说，类似于 ${0^n#1^n}$ 这个模式是无法被有穷自动机学习和预测出来的。换句话说，大模型的智能在这个例子上直接会被锁死，注定达不成所谓的**“AGI”**。\n以这个例子泛化来说，我们仅通过构造正负样本和机器学习做概率预测的方式，永远也无法对上面的模式做完美的判定。这个结论正是上面的推理想表达的意思。\n这件事可以拿 ChatGPT 来测试，对于 0#1，00#11，000#111，\u0026hellip;，这个序列，让 ChatGPT 续写，它可以继续写下去且不出错（但这只是假象），而且也会明确的说出这个序列是 ${0^n#1^n}$ 这个模式的产物。但当你要求它输出 n=100 时的输出，或者你拿 n=100 时的输入让 ChatGPT 判定时，它就会出错了。\n直接得到的重要启示是：\n除了大模型，我们还需要新的范式来解决 AGI 问题。仅靠提升模型规模，注定有很多事情做不到。\n额外的说明 有穷自动机是做不出基础四则运算的计算器 1 的。这也意味着大模型的推理能力是不存在的。\n我们认为的推理能力，不过是在有限状态空间下的穷举，例如上文中的 ${0^n#1^n}$ 这个例子。更大的模型可以通过训练模拟出更长的匹配，但是从“压缩比”的角度看，终究是没有能掌握这个规律，而是通过空间换时间的方式将更多的答案在训练的过程中记住。\n所以可能又回到了最初的问题——大模型是不是必须要足够大？继续增加大模型的规模还可以进一步提升泛化性，在类似这样的原本有穷自动机解决不了的问题上缓存更多的答案，“假装”大模型是可以解决它的。但这不是我们想要的答案。\nAgent + LLM 可以成为完备图灵机 While 循环的图灵完备性 编程语言 WHILE 语义 (Semnatik):\n一个 while 程序 $P$ ,通过传递 $k$ 个参数,返回一个结果, 即 $f:\\mathbb{N}^k\\rightarrow\\mathbb{N}$ 其他未定义的参数可以在程序里被调用,但是必须设定为 $0$ WHILE 程序的结果在结束结束后通过 $x_0$ 传达 对于一个 WHILE 程序,有三种运行模式: 变量赋值: $x_i=x_j+c,c\\in{0,1,−1}$ $P_1$;$P_2$ ( $P_1$,$P_2$ 是两个任意程序程序),先运行 $P_1$ ,再运行 $P_2$ WHILE $x_i \\neq 0$ DO $P$ END 意思是, $P$ 程序将一直被运行,直到 $x_i$ 等于 0 定理：编程语言 WHILE 是图灵完备的\n证明: 我们将受限 RAM(Registermaschine)(只有 LOAD,STORE,CLOAD,CADD,CSUB 功能的 RAM) 中的每一步都用 WHILE 程序来代替计算 2，由于受限 RAM 是图灵完备的,所以 WHILE 也是图灵完备的。\n需要注意的是，循环中 ${x_i}$ 的个数对其是否是图灵完备的有影响。具体来说，任意图灵机可以被拥有 $8$ 个变量的 WHILE 程序模拟计算。\n这里的大部分变量其实是用来操控 RAM 或者用来操控图灵机的。真实使用时，不需要这么多的掣肘。\nAgent 的基本范式 Agent 的基本范式恰好就是一个 While 程序，其 Python 描述如下：\n@dataclass class ReAct(ABC): thought: Optional[str] = None action: Optional[str] = None def __post_init__(self): self.obs = self.run() # 获取执行 Action 的结果 @abstractmethod def run(self) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 执行Action \u0026#34;\u0026#34;\u0026#34; pass @classmethod @abstractmethod def parse(cls, text: str) -\u0026gt; \u0026#34;ReAct\u0026#34;: \u0026#34;\u0026#34;\u0026#34; 从大模型返回的文本解析成 ReAct 的实例 \u0026#34;\u0026#34;\u0026#34; pass @property @abstractmethod def done(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; 终止条件 \u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def __str__(self) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 从 ReAct 中抽取信息形成新的 Prompt \u0026#34;\u0026#34;\u0026#34; pass act = ReAct() acts = [] while not act.done: acts.append(act) prompt = get_prompt(acts) act: ReAct = ReAct.parse(llm.call(prompt)) # 调用大模型，并将 response 解析成 ReAct 的实例 其中，存储和变量有两种选择：可以保存在函数 get_prompt 中（这意味着更多的人工控制设定），也可以保存在 ReAct 中（这意味着让大模型在上下文中自行决定保存哪些信息）。\n所以，Agent 的基本范式是图灵完备的。\n典型的几个 Agent 流程：\nReAct 获得反思推理能力 BabyAGI 基础的计划任务 Agent Reflexion 长期记忆和短期记忆（短期记忆就符合上述流程） AutoGPT 第一个全能 Agent 都可以转化到上述的范式中，进而获得更强大的计算能力（通用图灵机）。\nAgent 的重要意义其实是帮助 LLM 获得图灵完备性。当然，现在的 Agent 所缺乏的是自适应能力，还强依赖于 Prompt Engineering，不能自适应，不能进化，也没有利用上足够多的人类知识。\n突破的方向——可训练的 Agent 如果想获得更强的计算能力，需要提升的不仅仅是 LLM，而是结合了 Agent 后的整体系统。所以微调（fine tuning）和对齐（Alignment）更应该在整合了一个可学习的 Agent 之后进行。\n另外，基础的预训练模型或许并不需要特别的大（当然，越大性能越好的结论不变，但与其记更多的数据不如记更多的规律），而需要把更多的训练工作后置到集成了 Agent 之后进行，这样才有可能将有穷自动机无法识别的模式学习出来。\nAgent 的 While 程序模式，其实也恰好符合一个强化学习的学习过程，这里确实是可以做很多工作的。\n这是通往 AGI 之路吗 到今天为止，其实我们也没有一个关于智能的合理定义。\n学会了人的技能就算是智能了吗？会不会千百万年后的未来人回头看，会觉得人类太傻，并不具有智能呢？所以大模型学习人这件事是不是就是最好的选择？\n但至少今天人能够完成的一切，都没有可以超出图灵机范式的计算能力，所以图灵机的计算能力可以当作今天人类的极限。\nAGI 可以定义为:\n无需人类的介入，实现任意的图灵机能力。\n如果以这个定义来看，那么当下的 Agent + LLM 在理论上已经可以到达人类能够触达的一切天空了。\n有穷计算机无法模拟括号的匹配和乘除法的运算优先级。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n证明细节请看：·while循环 ，源自 Unentscheidbarkeit des Halteproblems: WHILE-Programm, Vorlesung 10 in BUK von RWTH Aachen\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.uglyboy.cn/posts/2023-11-02/","summary":"大模型是有穷自动机 非确定型有穷自动机（NFA）的定义 非确定型有穷自动机是一个 5 元数组 $Q,\\Sigma,\\delta,q_0,F$，其中 $Q$ 是","title":"大语言模型的计算能力"},{"content":"算数编码才是压缩的本质 一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 Compression for AGI - Jack Rae | Stanford MLSys #76\n里面核心模式只有一个：\n假定我有一个程序 f，我将 f 的代码传输给另一端； 我有一个序列需要传输，我通过 f 对逐个字符出现的概率进行了预测； 我根据算数编码，将结果编码后，传输给了另一端； 最后传输的信息量最小。 这不过是算数编码的定义好不好！！！ 哪里有什么神奇的地方。。。\n如果非说细节，也不过就是说明了为什么不用传输参数，将大模型的训练跟编码合到了一起而已。这完全证明不了大模型为什么有效果，以及为什么更大的模型效果更好。说出来的道理仅仅是：概率预测得越准，使用算数编码的压缩率越高。这件事结合算数编码的定义，不就是显然的问题吗？\n而且它原始的流程中，也没有能体现出“Next Token Prediction”的优越性和必要性。\n如果序列很小，那么压缩效率的核心是 f 的代码量。此时使用 lambda:x=x 达到的效果最好。 如果序列很大，那么传参也不会是压缩算法优劣的核心差别。那么其他模式训练出来的能对文本做良好概率预测的模型都可以达到好的压缩效果。 如果序列中等，我们需要的是是否存在一个方法，一次传输了多个算数编码和多个残差，能否通过这些信息还原出初始编码？针对这个问题，我们单独开一章来分析 是否只能用 NTP 做压缩？ 由自然归纳法，如果一次传输两个编码和两个残差，能还原出原始信息，那么，一次传输 $n$ 个算数编码和 $n$ 个残差就一定可以还原出原始编码。\n假设我们使用的算法的过程是先用除第一个字符以外的所有字符来预测第一个字符的概率，同时梯度下降；然后再用除第二个字符以外的其他字符预测第二个字符的概率，同时梯度下降。这样可以得到两个算数编码和两个残差，应该如何用这些信息还原初始的字符呢？\n方法和不确定型自动机的原理类似，或者用更土的办法来理解算法：\n我们用词表中的所有字符，重试这个过程，看哪个字符可以匹配上。虽然计算效率相比原版的 $\\mathcal{O}(1)$，这个方法的复杂度是 $\\mathcal{O}(n^2)$，但至少从压缩率的角度来看，我们对算法的要求没有计算速度方面的考量，更不用提这个算法一定是可以被优化的。\n以此推广，也就是对于任何模式的文本预测算法，都可以用同样的方法进行信息解压缩。于是不同方法之间在压缩率方面的差距还是会回归到对概率预测的精度上。甚至理论上看，使用了更多上下文的算法，应当可以比只做 \u0026ldquo;Next Token Prediction\u0026rdquo; 的算法精度更高。\n其他的无效解读 至于残差究竟是不是用信息熵，其实对这个压缩算法没有什么核心的影响，无论哪种残差该反向传播依旧按原本的方式传播，无所谓其物理意义。因为所有的意义都只体现在传递的残差能否还原原来的编码。残差能对应上什么物理意义的各种解释其实对压缩率和计算都没有帮助。\n结论 所以那个演讲其实不过是个披着数学魔术的神奇表演，本质上不过是说：大模型谁的性能好，谁就是更好的大模型——典型的废话文学新版本了。\n","permalink":"https://blog.uglyboy.cn/posts/2023-10-25/","summary":"算数编码才是压缩的本质 一直以来，大家对于大模型的理解都接受了“压缩即是智慧”这个思想，这个想法源自 Compression for AGI - Jack Rae | Stanford MLSys #76 里面核心模式只有一个：","title":"所谓“压缩即是智慧”毫无意义"},{"content":"基本安装 搭建站点 hugo new site \u0026lt;name of site\u0026gt; -f yml 初始化 Git 仓库 git init git branch -m main # 兼容 Github 的设置 安装主题 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 若是已经安装过主题的，需要下面的命令激活\ngit submodule update --init --recursive 本地调试 hugo server 添加新文章 hugo new posts/my-first-post.md 配置 配置 config.yml baseURL: \u0026#34;https://examplesite.com/\u0026#34; languageCode: zh-cn title: ExampleSite theme: PaperMod timeZone: Asia/Shanghai enableInlineShortcodes: true enableGitInfo: true enableRobotsTXT: true enableEmoji: true hasCJKLanguage: true outputs: home: [HTML, RSS, JSON] Params: title: ExampleSite description: \u0026#34;ExampleSite description\u0026#34; author: xxx homeInfoParams: Title: Hi there wave Content: Can be Info, links, about... socialIcons: # optional - name: rss url: /index.xml ShowFullTextinRSS: true ShowReadingTime: true ShowCodeCopyButtons: true DateFormat: \u0026#34;2006-01-02\u0026#34; # 日期格式化 menu: main: - identifier: home name: 主页 url: / weight: 10 - identifier: search name: 搜索 url: /search weight: 20 - identifier: tags name: 标签 url: /tags weight: 30 - identifier: archives name: 时间轴 url: /archives weight: 40 配置 content/archives.md --- title: \u0026#34;时间轴\u0026#34; layout: \u0026#34;archives\u0026#34; summary: archives --- 配置 content/search.md --- title: \u0026#34;搜索\u0026#34; # in any language you want layout: \u0026#34;search\u0026#34; # is necessary summary: \u0026#34;search\u0026#34; placeholder: \u0026#34;Typing something...\u0026#34; --- 增加 Latex 数学公式的支持 在 layouts/partials 路径下新建文件 extend_head.html：\n{{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} 和 math.html 文件：\n\u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [ [\u0026#34;$\u0026#34;, \u0026#34;$\u0026#34;], [\u0026#34;\\\\(\u0026#34;, \u0026#34;\\\\)\u0026#34;], ], displayMath: [ [\u0026#34;$$\u0026#34;, \u0026#34;$$\u0026#34;], [\u0026#34;\\\\[\u0026#34;, \u0026#34;\\\\]\u0026#34;], ], processEscapes: true, processEnvironments: true, }, options: { skipHtmlTags: [\u0026#34;script\u0026#34;, \u0026#34;noscript\u0026#34;, \u0026#34;style\u0026#34;, \u0026#34;textarea\u0026#34;, \u0026#34;pre\u0026#34;], }, }; window.addEventListener(\u0026#34;load\u0026#34;, (event) =\u0026gt; { document.querySelectorAll(\u0026#34;mjx-container\u0026#34;).forEach(function (x) { x.parentElement.classList += \u0026#34;has-jax\u0026#34;; }); }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js\u0026#34; \u0026gt;\u0026lt;/script\u0026gt; 利用 Github Actions 自动发布 编写 Github Actions 脚本 在 .github/workflows 下新建文件 build.yml：\n# This is a basic workflow to help you get started with Actions name: Auto Deploy Hugo # Controls when the workflow will run on: # Triggers the workflow on push or pull request events but only for the main branch push: branches: [ main ] pull_request: # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \u0026#34;build\u0026#34; build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: latest - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/main\u0026#39; }} with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} # secret 中设置好私钥 external_repository: your-repo/your-repo.github.io # Page 仓库 publish_branch: main # Page 仓库的分支 publish_dir: ./public # 静态网页路径 commit_message: ${{ github.event.head_commit.message }} 记得在 Page 仓库的设置中开启 Github Pages，选择 main 分支，用你的仓库名替换 your-repo/your-repo.github.io。\n生成私钥 ssh-keygen -t rsa -b 4096 -C \u0026#34;$(git config user.email)\u0026#34; -f gh-pages -N \u0026#34;\u0026#34; 你将得到两个文件：\ngh-pages.pub 是 Public Key gh-pages 是 Private Key 在 Github 中设置信息 在本项目目录下设置 Sectets 的 ACTIONS_DEPLOY_KEY 信息，填入 Private Key 在 Pages 项目下设置 Deploy Keys，填入 Public Key，记得选中 Allow write access 添加 public key Success 添加 private key Success ","permalink":"https://blog.uglyboy.cn/posts/2023-10-18/","summary":"基本安装 搭建站点 hugo new site \u0026lt;name of site\u0026gt; -f yml 初始化 Git 仓库 git init git branch -m main # 兼容 Github 的设置 安装主题 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 若是已经安装过主题的，需要下面的命令激活 git submodule update --init --recursive","title":"Hugo 搭建流程"},{"content":"Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计：\n模型下的理想真实世界的概率分布：$p(x|\\theta)$\n我们不知道真实世界的分布，所以我们要用样本估计似然函数 $L(\\theta|x)$\n现在 $x$ 已知，$\\theta$ 未知，若对于两个参数 $\\theta_1$ 和 $\\theta_2$ 有\n$$ L(\\theta_1|x) = p(x|\\theta_1) \u0026gt; p(x|\\theta_2) = L(\\theta_2|x) $$\n那么意味着 $\\theta=\\theta_1$ 时，随机变量 $\\theta_1$ 生成 $x$ 的概率大于当参数 $\\theta=\\theta_2$ 时。这也正是似然的意义所在，若观测数据为 $x$，那么 $\\theta_1$ 是比 $\\theta_2$ 更有可能为分布函数的参数。\n在给定观测数据集 $X={x_n},n \\in \\mathbb{N}$ 时，真实世界最有可能的概率分布对应的参数 $\\hat\\theta$ 应该满足：\n$$ L(\\hat\\theta|x) = p(x|\\hat\\theta) \u0026gt; p(x|\\theta) = L(\\theta|x), \\theta \\in \\mathbb{\\Theta} 且 \\theta \\ne \\hat\\theta $$\n即：\n$$ \\hat\\theta = \\arg\\max\\limits_\\theta L(\\theta|x) $$\n求解最大似然函数：\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\theta} L(\\theta|x) = 0 $$\n对这个方程数值求解的过程，对应的就是绝大部分机器学习算法中的梯度下降过程。\n在测试集上评估的结果，我们预想的误差应当包含两部分：\n似然函数 $L(\\theta|x)$ 对真实世界概率分布描述能力不足，带来的误差； 通过 $X$ 估计 $\\theta$ 时，样本本身的误差； 若假定我们可以通过梯度下降收敛（即上面最大似然函数的导数在 0 的一个很小的临域中），那么至少就是我们相信在观测数据集 $X$ 上，模型是正确的，那么评估的误差就更加明确的指向 $X$ 本身带来的误差。\nFisher 信息量 为了求解最大似然估计，我们常用的数值手段是：\n假定观测数据集 $X$ 的真实世界概率对应的概率密度函数是 $f(x_i;\\theta)$，定义似然函数：\n$$ L(X;\\theta) = \\prod \\limits^{n}_{i=1} f(x_i;\\theta) $$\n求解时，先对 $L(X|\\theta)$ 取对数，再求导，这个函数定义为 Score function：\n$$ S(X;\\theta) = \\sum \\limits^n_{i=1} \\frac{\\partial \\ln f(x_i;\\theta)}{\\partial\\theta} $$\n则 Fisher 信息量的定义就是这个 Score function 的二阶矩（second moment）\n$$ I(\\theta) = E[S(X;\\theta)^2] $$\nFisher 信息量最重要的意义是：通过中心极限定理，弱大数定律，依概率一致收敛，以及 Slutsky 定理，可以证明 MLE 的渐进分布是正态分布 1，即：\n$\\hat \\theta \\stackrel{P}{\\longrightarrow} \\theta_0$，其中 $\\theta_0$ 是参数的真实值； $\\sqrt{n}(\\hat\\theta - \\theta_0) \\stackrel{L}{\\longrightarrow} N(0,I^{-1}(\\theta))$ ; 数据量与误差的关系 花了大量篇幅描述了 最大似然 和 Fisher 信息量后，最终真正值得我们关注的结论却异常的简单：\n$$ L(D) \\propto D^{-0.5} $$\n这个结论同计算均值时，数据样本带来的误差是完全一样的。\n真实的机器学习条件下，我们的样本量的质量并不均匀，所以往往会优先使用更好的样本（小样本集不是大样本集的随机采样，而是精选），会导致观测数据集 $X$ 不能满足概率同分布，所以带来的结果是上述幂律关系中，实际的幂律值会小于 $0.5$。\n理论上来说，如果我们能做到样本集随机采样，那样这个幂律就会更加接近 $0.5$，而如果样本集不能随机采样，某种意义上说，能否保持这种幂律关系是值得怀疑的。所以对于 OpenAI 和 Google 的 Scaling Law 的论文，在样本量同 Loss 的关系上，Google 的结果是更可信的。\n哪怕依旧能维持幂律关系（维持幂律关系的数学基础是不存在的。。。），具体的数值也只能通过实际拟合来估计。因为这件事不是通用规律，只跟具体的训练数据集的分布有关，跟模型无关（前提条件是模型能在大数据下收敛，即满足大数定律、中心极限定律，并且模型可以拟合真实分布）。\nCompute 和 Loss 的关系 控制论和 PID 算法 梯度下降法的数值计算过程，某种视角下可以理解成就是控制论下的控制算法——我如何根据真实信息来控制我的预期值离目标值更近。\n直观而好用的方法就是 PID 算法：\n$$ u(t) = K_pe(t) + K_i\\int^t_0 e(\\tau)\\mathrm{d}\\tau + K_d\\frac{\\mathrm{d}e(t)}{\\mathrm{d}t} $$\n当然，我们的梯度下降法原没有 PID 算法如此之精密，实际流程大概率只使用了 P 的部分，也就是对误差做补偿。在深度学习中，被称为反向传播。\n单参数计算量与误差的关系 单目标的 PID（只省 P 过程了）算法，误差与计算量（迭代次数）之间的关系：\n$$ L(C) \\propto K_p^{C}=e^{\\lambda C} $$\n即，误差同计算量之间的关系是指数关系，不是幂律关系。\n这一点在 Sorscher et al. (2022) 中有所体现，它的结论是：至少对于某些任务，损失可以随着数据集 2 大小呈指数级增长，而不是作为幂律。\n总计算量与误差的关系 不同于优化问题中，我们会通过反复迭代的方式增加计算量，深度学习的计算量基本上是同模型规模和数据量正相关的。反过来意味着对单参数的优化迭代很少的固定步数就可以收敛，所以在通常数据量规模下，可以将单参数计算量带来的优化效果视作常数（都能优化到收敛）。\n单参数计算量带来的优化效果视为常数（不会随计算量、节点数、数据量变化而变化），意味着计算本身同误差之间没有直接关联，总计算量与误差之间的关联体现的是数据量与误差的关系和节点数（结构）与误差的关系。\n总计算量与数据量成正比，而数据量同优化效果之间的关联我们已经在前文完成了论述。下一步我们将分析节点数和误差之间的关系，或者其实更加精确的说，应当是在单参数误差不变的条件下，节点数的变化与总计算量之间的关系，是这个关系蕴含了总计算量与误差之间的关联。\nCompute 和 Parameters 的关系 分形维度 具有自相似性的结构就是分形。而我们的深度学习计算就是典型的分形结构——当模型规模扩大时，主流的扩大的方式就是增加层数 3，这带来的就是自相似性。\n而自相似性带来的重要性质就是，系统会具有分形维度，分形维度会使得系统规模扩大时，对应的全局属性并不是等比增加，而是幂律增加，幂律的指数就是其分形维度。\n生物学中有重要的 $\\frac{3}{4}$ 定律——生物随着重量的变大（原子数量的规模扩大），其相关的很多生物学特征，例如新陈代谢能力、血管长度、心跳、呼吸等等，并不与重量成正比，而是按照 $\\frac{3}{4}$ 的幂律进行增长。 一个直观的理解，随着生物体长度增长，其体重会以幂律 $3$ 进行提升，而腿部的横截面则是幂律 $2$ 增长。所以生物的规模变大，就会带来腿部承受的压力不断变大，所以老鼠体型的动物的腿都很细，但大象规模的动物，腿都很粗；蚂蚁可以举起自身体重百倍的物品，但人只能举起和体重相仿的物品。这些都是因为规模变化带来的非线性，要求生物的动力学模型必须发生变化，而不能与小规模时一样。 类似的，在城市规模同城市中加油站、小超市、医院之类的城市核心建设之间，也存在着幂律增长的关系——相关幂律大约是 $0.85$。\n对应的，深度学习模型中，在保证单参数误差不变的条件下，Parameters 规模的增加所需要的 Compute 计算量的增加不是等比的，而是幂律的，而且这个幂律应当是小于 $1$ 的。\n换句话说，计算量同损失之间的关系是伴生关系——计算量本身同损失是没有直接关联的。带来损失变化的根本原因不是计算不足，而是模型表达能力以及数据本身蕴含的信息带来的。\n但因为这里的结论中，计算量与参数数量也是幂律关系，由前文，数据同损失也是幂律关系，如果参数数量同损失同样是幂律关系的化，那么计算量与损失也可以用幂律关系来表示。\nParameters 与 Loss 之间的关系 这里要分析的是参数量增加为何能带来 Loss 的降低。这是因为 Parameters 的增加，可以提升模型的表达能力，可以更好的拟合目标函数。也就是说，一个模型距离真值的误差（Loss），除了因为 Dataset 自身的误差外，还有一部分是模型距离 Dataset 所描述的最大似然函数的误差。\n这部分要是详尽分析起来会很复杂，幸好已经有一些这方面的研究：Sharma et al. (2020) 和 Bahri et al. (2021) 都对这个问题进行了很好的分析，其结果也有对应的实验支撑。\n文章假定深度模型将数据映射到一个 $d$ 维数据流形上，增加的模型参数（无限数据的条件下）都会被模型用来将数据流形分割成更小的组件，然后模型将在数据流形的每个分量上进行独立的预测，以优化训练损失。\n这样自然的，如果我们想让子区域的大小缩小 $2$ 倍，就需要增加 $2^d$ 倍的数据量或模型参数。进而就是直观的结论：\n$$ L(P) \\propto P^{-\\frac{1}{d}} $$\n即 Loss 与 参数量之间是幂律关系，其幂律值小于 $1$（因为有 $d\u0026gt;1$）。\n总结 至此，关于 Scaling Law 的数学含义就已经基本都解释清楚了。\n更重要的问题是，有了相关的理论支撑后，我们能做什么？哪些事情做不了。\n基于多份数据融合的实验结果预测 这件事是不可行的。\n一切机器学习的基础都是最大似然估计，而最大似然估计的基础假设就是独立同分布。两组分布不同的数据融合，一定会破坏原有的分布，至于不同比例下融合后形成怎样的分布，具有怎样的特性，在两份数据的分布都已知的条件下，是可以计算的。但对于我们自己的机器学习任务，原本就是要去学习数据的分布，这就决定了，不可能在不了解数据分布的条件下，估计融合后的数据分布。\n类似的，多分不同分布的数据集怎么融合能更贴近测试集也是不可知的，只能试出来。由于测试集也不是真值，甚至测试集对真实世界的表达很可能还不如训练集，所以针对测试集做针对性调优是不值得的。\n这部分的定量分析，其实可以借鉴 OpenAI 关于 Scaling Laws 的经典文章 Kaplan et al. (2020) 中尝试的方法：\n迁移学习与测试效果的提升： 当我们在与训练集分布不同的文本上评估模型时，其结果与训练验证集上的结果强烈相关，损失函数中有一个大致恒定的偏移量。换句话说，转移到不同的分布会带来一定的固定惩罚，但除此之外，其提升程度大致与训练集上的表现一致。\n可以用类似这样的方法，通过多份不同分布的测试集效果打分情况，评估模型表现。\n当然，实操方面其实也不复杂，就是多看几个测试集的结果，记录下来。如果模型优化后，在各个测试集上的提升是基本一致的，那就说明这次改进不是因为数据分布变化带来的，而是因为模型能力带来的。\n判断最优的参数和模型数据量配比 这件事不是特别值得做。因为我们当前模型的优质数据不够多。所以提供的数据质量是不稳定的。小模型上得到的预测数据值，在大模型上操作时，肯定不能按预测量来操作，而是还需要进一步增加数据量。但是具体增加多少，因为我们对数据质量无法在训练前得到评估，所以是不可预测的。\n这件事值得做的条件是：我们已经用一份数据训练了一个很大的模型，然后我们可以通过抽样的方法构建小模型，用大模型预测小模型需要多少数据量，这件事是可行的。\n当然，如果只是一个预估值做参考，这件事倒是可以做一下。\n注：这件事值得做的数学理论基础是：我们需要找到样本的精度和模型训练的流型精度一致的对应比例。这件事的前提条件是：模型得到充分训练，且 Loss 与 样本、模型精度是同一个数量级（Loss 就是当前的精度）。如果这个精度不一致，loss 会被更大的精度所制约。带来的影响是会增加一定的无效计算量。\n理论上，这件事更应该用适合的停机算法来避免冗余的计算，而不是需要精准的预估精度。\n尝试用更小的模型达到更优的效果 这件事价值不是特别大。\n不需要知道具体的比例，我们也知道，哪怕对于小模型，喂更多的数据可以达到更好的效果。 小模型的表达能力是有限的，所以也不是喂更多的数据就一定可以提升效果。 于是哪怕做出了预估，也需要加好多限制条件，而实际应用场景也不多。\n其他？ 昨天看完后，原本想说 Scaling Law 是个显然的结果，其规律并不蕴含更深层次的信息。但后来仔细想了想，可能还是有很多细节值得仔细的表述一下，以免遗漏什么可能性，所以写了这个文档。\n总得来说，我对于 Scaling Law 并没有想到更深的应用场景，它所能表达的大概也只是：更多的数据、更大的模型（更多的模型参数）可以更好的拟合真实的概率分布。这件事对于机器学习来说，是自然的结论。这个规律几乎不涉及具体的模型形式——几乎只要是机器学习都符合这个规律。\n所以从第一性原理角度出发，它算是一个数学上给出定性的存在性定理：我们的机器学习是可以不断优化的。但它不蕴含如何能更好地做优化的信息。\n这个定理的前置条件和证明过程这里就不赘述了，需要的话自己查一下。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n大部分的训练，增加迭代次数的方式都伴随着提供更多的训练样本，若模型距离收敛所需要的迭代次数比较多，例如如果 学习律（本质上就是 PID 中的 $K_p$）比较小，模型距离理论上限比较远，这时误差项主要不是来源于数据自身的误差，而是来自梯度下降逼近的误差，那么这个指数关系就会比较显著，对应的表象就是误差同数据量之间是指数关系。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n还可以增加每层的神经元数量（宽度），这种增加模式就不属于分层。类似于 Scaling Law 的规律，这种扩大的方式（形状变化）对于结果的影响不显著。当然，这件事是值得做实验，试一试少层数多神经元和多层数少神经元（参数总数一致）训练的结果是否一致。盲猜会有显著性能差异。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.uglyboy.cn/posts/2023-10-10/","summary":"Dataset Size 和 Loss 的关系 最大似然估计（MLE） 一切机器学习的本质都是最大似然估计： 模型下的理想真实世界的概率分布：$p(x|\\theta)$ 我们不知道","title":"Scaling Law 的数学解读"}]